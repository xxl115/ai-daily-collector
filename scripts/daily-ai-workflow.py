#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Daily Collector - æ¯æ—¥çƒ­ç‚¹é‡‡é›†å·¥ä½œæµ

åŠŸèƒ½:
1. é‡‡é›†å¤šå¹³å° AI çƒ­ç‚¹èµ„è®¯
2. å…³é”®è¯è¿‡æ»¤å’Œæ™ºèƒ½æ’åº
3. ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š
4. å¤šæ¸ é“æ¨é€

æ”¯æŒçš„æ•°æ®æº:
- RSS è®¢é˜… (è‹±æ–‡ AI èµ„è®¯)
- GitHub Trending
- Hacker News
- Product Hunt
- NewsNow (ä¸­æ–‡çƒ­ç‚¹)
- V2EX çƒ­é—¨
- Reddit çƒ­é—¨
"""

import argparse
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from config.settings import (
    DEFAULT_CONFIG,
)
from utils.logger import setup_logger, get_logger
from utils.rss import RSSGenerator
from utils.notification import notification_manager
from utils.filter import keyword_filter, sort_by_hotness
from utils.cache import cache
from utils.rate_limit import limiter
from utils.errors import retry, FallbackManager, fallback_return_empty

# å¯¼å…¥æŠ“å–å™¨ï¼ˆä½¿ç”¨ fetchers æ¨¡å—ï¼‰
from fetchers import (
    fetch_by_config,           # ç»Ÿä¸€è°ƒåº¦æ¥å£
    fetch_newsnow_hotspots,
    fetch_v2ex_hotspots,
    fetch_reddit_hotspots,
    fetch_tech_media_hotspots,  # æ›¿ä»£åŸæ¥çš„ RSS é‡‡é›†
    fetch_ai_blog_hotspots,     # AI åšå®¢
)

# GitHub Trending éœ€è¦å•ç‹¬å®ç°æˆ–ä½¿ç”¨ç¬¬ä¸‰æ–¹æœåŠ¡
# æš‚æ—¶æ³¨é‡Šæ‰ï¼Œåç»­å¯ä»¥æ·»åŠ 
# from collectors.github import fetch_github_trending
# from collectors.hackernews import fetch_hacker_news
# from collectors.producthunt import fetch_product_hunt


def setup_logging(verbose: bool = False):
    """è®¾ç½®æ—¥å¿—"""
    level = logging.DEBUG if verbose else logging.INFO
    setup_logger(
        name="ai-daily",
        level=level,
        log_file=DEFAULT_CONFIG["log_file"],
        max_bytes=DEFAULT_CONFIG["log_max_bytes"],
        backup_count=DEFAULT_CONFIG["log_backup_count"],
    )


def collect_all_sources(config: Dict) -> Dict[str, List[Dict]]:
    """
    é‡‡é›†æ‰€æœ‰æ•°æ®æºï¼ˆä» sources.yaml è¯»å–é…ç½®ï¼‰

    Returns:
        é‡‡é›†ç»“æœå­—å…¸
    """
    import yaml
    from pathlib import Path

    logger = get_logger(__name__)
    results = {
        "tech_media": [],    # ç§‘æŠ€åª’ä½“ï¼ˆåŒ…æ‹¬ä¸­æ–‡ï¼‰
        "ai_blogs": [],      # AI å®˜æ–¹åšå®¢
        "newsnow": [],       # NewsNow ä¸­æ–‡çƒ­ç‚¹
        "v2ex": [],          # V2EX
        "reddit": [],        # Reddit
    }

    logger.info("=" * 60)
    logger.info("å¼€å§‹é‡‡é›† AI çƒ­ç‚¹èµ„è®¯")
    logger.info("=" * 60)

    # åŠ è½½ sources.yaml é…ç½®
    config_path = Path(__file__).parent.parent / "config" / "sources.yaml"
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            sources_config = yaml.safe_load(f)
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return results

    # éå†é…ç½®ä¸­çš„æ¯ä¸ªæ•°æ®æº
    for source in sources_config.get('sources', []):
        if not source.get('enabled', False):
            continue

        source_name = source['name']
        source_type = source.get('type', '')

        logger.info(f"\nğŸ“¡ é‡‡é›†: {source_name}")

        try:
            # ä½¿ç”¨ç»Ÿä¸€è°ƒåº¦æ¥å£
            items = fetch_by_config(source)

            if items:
                # æ ¹æ®ç±»å‹å½’ç±»
                if source_type == 'tech_media':
                    results['tech_media'].extend(items)
                elif source_type == 'ai_blogs':
                    results['ai_blogs'].extend(items)
                elif source_type == 'newsnow':
                    results['newsnow'].extend(items)
                elif source_type == 'v2ex':
                    results['v2ex'].extend(items)
                elif source_type == 'reddit':
                    results['reddit'].extend(items)
                else:
                    # æœªçŸ¥ç±»å‹ï¼Œæ”¾å…¥ tech_media
                    results['tech_media'].extend(items)

                logger.info(f"   âœ… {source_name}: {len(items)} æ¡")
            else:
                logger.info(f"   âš ï¸ {source_name}: æ— æ•°æ®")

        except Exception as e:
            logger.error(f"   âŒ {source_name}: {e}")

    # ç»Ÿè®¡
    total = sum(len(v) for v in results.values())
    logger.info("\n" + "=" * 60)
    logger.info(f"é‡‡é›†å®Œæˆ! æ€»è®¡: {total} æ¡")
    logger.info("=" * 60)

    return results


def filter_and_process(articles: List[Dict], config: Dict) -> List[Dict]:
    """
    è¿‡æ»¤å’Œå¤„ç†æ–‡ç« 
    
    Args:
        articles: åŸå§‹æ–‡ç« åˆ—è¡¨
        config: é…ç½®
    
    Returns:
        å¤„ç†åçš„æ–‡ç« åˆ—è¡¨
    """
    logger = get_logger(__name__)
    
    # 1. å…³é”®è¯è¿‡æ»¤
    if config.get("enable_filter", True):
        logger.info("\nğŸ” åº”ç”¨å…³é”®è¯è¿‡æ»¤...")
        matched, filtered = keyword_filter.filter_articles(
            articles,
            title_field="title",
        )
        logger.info(f"   åŒ¹é…: {len(matched)}, è¿‡æ»¤: {len(filtered)}")
        articles = matched
    
    # 2. å»é‡ (åŸºäº URL)
    seen_urls = set()
    unique_articles = []
    for article in articles:
        url = article.get("url", "")
        if url and url not in seen_urls:
            seen_urls.add(url)
            unique_articles.append(article)
    logger.info(f"\nğŸ”— å»é‡: {len(unique_articles)} æ¡")
    
    # 3. æŒ‰çƒ­åº¦æ’åº
    if config.get("enable_sorting", True):
        logger.info("\nğŸ“Š æŒ‰çƒ­åº¦æ’åº...")
        unique_articles = sort_by_hotness(
            unique_articles,
            rank_field="rank",
            count_field="count",
            rank_weight=0.6,
            frequency_weight=0.3,
            hotness_weight=0.1,
        )
    
    # é™åˆ¶æ•°é‡
    limit = config.get("max_articles", 50)
    return unique_articles[:limit]


def generate_report(
    articles: List[Dict],
    config: Dict,
    output_dir: Path,
) -> Path:
    """
    ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š
    
    Returns:
        æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    logger = get_logger(__name__)
    
    beijing_tz = __import__("pytz").timezone("Asia/Shanghai")
    now = datetime.now(beijing_tz)
    date_str = now.strftime("%Y-%m-%d")
    
    # æŒ‰æ¥æºåˆ†ç»„
    sources = {}
    for article in articles:
        source = article.get("source", "Unknown")
        if source not in sources:
            sources[source] = []
        sources[source].append(article)
    
    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    report_content = f"""# AI Daily - {date_str}

> ç”Ÿæˆæ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')} (åŒ—äº¬æ—¶é—´)
> æ€»è®¡: {len(articles)} æ¡

## ç»Ÿè®¡

| æ¥æº | æ•°é‡ |
|------|------|
"""
    for source, items in sorted(sources.items(), key=lambda x: -len(x[1])):
        report_content += f"| {source} | {len(items)} |\n"
    
    report_content += "\n## çƒ­ç‚¹æ’è¡Œ\n\n"
    
    for i, article in enumerate(articles[:30], 1):
        title = article.get("title", "æ— æ ‡é¢˜")
        url = article.get("url", "")
        source = article.get("source", "")
        rank = article.get("rank", 0)
        hot_score = article.get("hot_score", 0)
        
        if url:
            report_content += f"{i}. **[{title}]({url})**\n"
        else:
            report_content += f"{i}. **{title}**\n"
        
        report_content += f"   - æ¥æº: {source}"
        if rank:
            report_content += f" | æ’å: #{rank}"
        if hot_score:
            report_content += f" | çƒ­åº¦: {hot_score}"
        report_content += "\n\n"
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = output_dir / f"ai-hotspot-{date_str}.md"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report_content)
    
    logger.info(f"\nğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    return report_file


def generate_rss_feed(
    articles: List[Dict],
    config: Dict,
    output_dir: Path,
) -> Path:
    """
    ç”Ÿæˆ RSS Feed
    
    Returns:
        RSS æ–‡ä»¶è·¯å¾„
    """
    beijing_tz = __import__("pytz").timezone("Asia/Shanghai")
    now = datetime.now(beijing_tz)
    date_str = now.strftime("%Y-%m-%d")
    
    rss = RSSGenerator(
        title="AI Daily - äººå·¥æ™ºèƒ½çƒ­ç‚¹èµ„è®¯",
        link="https://github.com/xxl115/ai-daily-collector",
        description="æ¯æ—¥ AI çƒ­ç‚¹èµ„è®¯èšåˆï¼ŒåŒ…æ‹¬ RSS è®¢é˜…ã€GitHub Trendingã€Hacker Newsã€NewsNowã€V2EXã€Reddit ç­‰å¤šå¹³å°å†…å®¹",
        language="zh-CN",
    )
    
    for article in articles[:30]:
        rss.add_item(
            title=article.get("title", "æ— æ ‡é¢˜"),
            link=article.get("url", ""),
            description=article.get("summary", "")[:500],
            pub_date=now.strftime("%a, %d %b %Y %H:%M:%S GMT"),
            guid=article.get("url", ""),
            category=article.get("source", ""),
        )
    
    rss_content = rss.generate()
    
    # ä¿å­˜ RSS
    rss_file = output_dir / f"rss/{date_str}.xml"
    rss_file.parent.mkdir(parents=True, exist_ok=True)
    with open(rss_file, "w", encoding="utf-8") as f:
        f.write(rss_content)
    
    return rss_file


def send_notifications(
    articles: List[Dict],
    config: Dict,
    report_file: Path,
):
    """
    å‘é€æ¨é€é€šçŸ¥
    """
    logger = get_logger(__name__)
    
    if not config.get("enable_notification", True):
        logger.info("\nğŸ”• æ¨é€å·²ç¦ç”¨")
        return
    
    # è·å–é…ç½®çŠ¶æ€
    status = notification_manager.get_config_status()
    configured_platforms = [
        p for p, s in status.items() if s.get("configured")
    ]
    
    if not configured_platforms:
        logger.info("\nğŸ”• æœªé…ç½®ä»»ä½•æ¨é€æ¸ é“")
        return
    
    beijing_tz = __import__("pytz").timezone("Asia/Shanghai")
    now = datetime.now(beijing_tz)
    
    # æ„å»ºæ¨é€å†…å®¹
    title = f"AI Daily - {now.strftime('%m/%d')} çƒ­ç‚¹ ({len(articles)} æ¡)"
    
    content = f"## {title}\n\n"
    for i, article in enumerate(articles[:10], 1):
        title_text = article.get("title", "")[:50]
        source = article.get("source", "")
        url = article.get("url", "")
        
        if url:
            content += f"{i}. [{title_text}]({url}) ({source})\n"
        else:
            content += f"{i}. {title_text} ({source})\n"
    
    content += f"\n... å…± {len(articles)} æ¡"
    
    # å‘é€åˆ°å„å¹³å°
    logger.info(f"\nğŸ“± æ¨é€åˆ°: {', '.join(configured_platforms)}")
    
    results = notification_manager.send_to_all(
        title=title,
        content=content,
    )
    
    for platform, success in results.items():
        if success:
            logger.info(f"   âœ… {platform}")
        else:
            logger.info(f"   âŒ {platform}")


def run_workflow(args: argparse.Namespace):
    """è¿è¡Œå®Œæ•´å·¥ä½œæµ"""
    # åŠ è½½é…ç½®
    config = DEFAULT_CONFIG.copy()
    config.update({
        "enable_rss": not args.skip_rss,
        "enable_github": not args.skip_github,
        "enable_hackernews": not args.skip_hn,
        "enable_producthunt": not args.skip_ph,
        "enable_newsnow": not args.skip_newsnow,
        "enable_v2ex": not args.skip_v2ex,
        "enable_reddit": not args.skip_reddit,
        "enable_filter": not args.no_filter,
        "enable_sorting": not args.no_sort,
        "enable_notification": not args.no_notify,
    })
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.verbose)
    logger = get_logger(__name__)
    
    try:
        # 1. é‡‡é›†æ•°æ®
        results = collect_all_sources(config)
        
        # åˆå¹¶æ‰€æœ‰æ–‡ç« 
        all_articles = []
        for articles in results.values():
            all_articles.extend(articles)
        
        logger.info(f"\nğŸ“Š é‡‡é›†æ€»è®¡: {len(all_articles)} æ¡")
        
        # 2. è¿‡æ»¤å’Œå¤„ç†
        articles = filter_and_process(all_articles, config)
        
        # 3. è¾“å‡ºç›®å½•
        output_dir = Path(config["output_dir"])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        report_file = generate_report(articles, config, output_dir)
        
        # 5. ç”Ÿæˆ RSS
        rss_file = generate_rss_feed(articles, config, output_dir)
        
        # 6. å‘é€æ¨é€
        send_notifications(articles, config, report_file)
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ… å·¥ä½œæµå®Œæˆ!")
        logger.info("=" * 60)
        
    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        logger.exception(f"å·¥ä½œæµå¤±è´¥: {e}")
        sys.exit(1)


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        description="AI Daily Collector - æ¯æ—¥ AI çƒ­ç‚¹é‡‡é›†å·¥ä½œæµ"
    )
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="è¯¦ç»†æ—¥å¿—æ¨¡å¼",
    )
    
    # è·³è¿‡é€‰é¡¹
    skip_group = parser.add_argument_group("è·³è¿‡é‡‡é›†")
    skip_group.add_argument(
        "--skip-rss",
        action="store_true",
        help="è·³è¿‡ RSS é‡‡é›†",
    )
    skip_group.add_argument(
        "--skip-github",
        action="store_true",
        help="è·³è¿‡ GitHub Trending é‡‡é›†",
    )
    skip_group.add_argument(
        "--skip-hn",
        action="store_true",
        help="è·³è¿‡ Hacker News é‡‡é›†",
    )
    skip_group.add_argument(
        "--skip-ph",
        action="store_true",
        help="è·³è¿‡ Product Hunt é‡‡é›†",
    )
    skip_group.add_argument(
        "--skip-newsnow",
        action="store_true",
        help="è·³è¿‡ NewsNow é‡‡é›†",
    )
    skip_group.add_argument(
        "--skip-v2ex",
        action="store_true",
        help="è·³è¿‡ V2EX é‡‡é›†",
    )
    skip_group.add_argument(
        "--skip-reddit",
        action="store_true",
        help="è·³è¿‡ Reddit é‡‡é›†",
    )
    
    # å¤„ç†é€‰é¡¹
    process_group = parser.add_argument_group("å¤„ç†é€‰é¡¹")
    process_group.add_argument(
        "--no-filter",
        action="store_true",
        help="è·³è¿‡å…³é”®è¯è¿‡æ»¤",
    )
    process_group.add_argument(
        "--no-sort",
        action="store_true",
        help="è·³è¿‡æ’åº",
    )
    process_group.add_argument(
        "--no-notify",
        action="store_true",
        help="è·³è¿‡æ¨é€",
    )
    
    args = parser.parse_args()
    run_workflow(args)


if __name__ == "__main__":
    main()
