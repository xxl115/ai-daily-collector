#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Daily Collector - çœŸå®æ•°æ®é‡‡é›† (ç‹¬ç«‹è¿è¡Œç‰ˆ)
ä¸ä¾èµ–é¡¹ç›®æ¨¡å—ï¼Œç›´æ¥æŠ“å–æ•°æ®
"""

import json
import sys
import time
from datetime import datetime
from pathlib import Path

try:
    import requests
    from bs4 import BeautifulSoup
    IMPORTS_OK = True
except ImportError:
    IMPORTS_OK = False
    print("âš ï¸ requests/beautifulsoup4 æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç¤ºä¾‹æ•°æ®")


def fetch_v2ex_hotspots(limit=10):
    """æŠ“å– V2EX çƒ­é—¨"""
    if not IMPORTS_OK:
        return []
    
    try:
        url = "https://www.v2ex.com/api/v2/topics/hot"
        resp = requests.get(url, timeout=10)
        if resp.ok:
            data = resp.json()
            items = []
            for item in data[:limit]:
                items.append({
                    "title": item.get("title", ""),
                    "url": f"https://www.v2ex.com/t/{item.get('id')}",
                    "source": "V2EX",
                    "hot_score": 100 - len(items),
                    "timestamp": datetime.now().isoformat(),
                })
            print(f"   âœ… V2EX: {len(items)} æ¡")
            return items
    except Exception as e:
        print(f"   âŒ V2EX å¤±è´¥: {e}")
    return []


def fetch_hacker_news(limit=10):
    """æŠ“å– Hacker News AI ç›¸å…³"""
    if not IMPORTS_OK:
        return []
    
    try:
        # è·å– top stories
        resp = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", timeout=10)
        if resp.ok:
            ids = resp.json()[:30]
            items = []
            for story_id in ids:
                try:
                    story_resp = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json", timeout=5)
                    if story_resp.ok:
                        story = story_resp.json()
                        title = story.get("title", "")
                        # è¿‡æ»¤ AI ç›¸å…³
                        if any(kw in title.lower() for kw in ["ai", "llm", "gpt", "claude", "agent", "mcp", "deepseek"]):
                            items.append({
                                "title": title,
                                "url": story.get("url", f"https://news.ycombinator.com/item?id={story_id}"),
                                "source": "Hacker News",
                                "hot_score": story.get("score", 0),
                                "timestamp": datetime.fromtimestamp(story.get("time", 0)).isoformat() if story.get("time") else datetime.now().isoformat(),
                            })
                            if len(items) >= limit:
                                break
                except:
                    pass
            print(f"   âœ… Hacker News: {len(items)} æ¡")
            return items
    except Exception as e:
        print(f"   âŒ Hacker News å¤±è´¥: {e}")
    return []


def fetch_github_trending(limit=10):
    """æŠ“å– GitHub Trending AI"""
    if not IMPORTS_OK:
        return []
    
    try:
        url = "https://github.com/trending?spoken_language_code=en"
        resp = requests.get(url, timeout=15)
        if resp.ok:
            soup = BeautifulSoup(resp.text, 'html.parser')
            items = []
            for article in soup.select('article.Box-row')[:limit]:
                title_elem = article.select_one('h2 a')
                if title_elem:
                    title = title_elem.get_text(strip=True).replace('\n', '').replace(' ', '')
                    url = "https://github.com" + title_elem.get('href', '')
                    stars_elem = article.select_one('span.float-right')
                    stars = stars_elem.get_text(strip=True) if stars_elem else "0"
                    
                    items.append({
                        "title": f"[GitHub] {title}",
                        "url": url,
                        "source": "GitHub Trending",
                        "hot_score": 100 + int(stars.replace(',', '')) if stars.replace(',', '').isdigit() else 50,
                        "timestamp": datetime.now().isoformat(),
                    })
            print(f"   âœ… GitHub Trending: {len(items)} æ¡")
            return items
    except Exception as e:
        print(f"   âŒ GitHub Trending å¤±è´¥: {e}")
    return []


def fetch_ai_blogs(limit=5):
    """æŠ“å– AI å®˜æ–¹åšå®¢"""
    if not IMPORTS_OK:
        return []
    
    blogs = [
        {"name": "OpenAI", "url": "https://openai.com/blog/rss.xml"},
        {"name": "Google AI", "url": "https://developers.google.com/feeds/blog.xml?alt=rss"},
    ]
    
    items = []
    for blog in blogs:
        try:
            resp = requests.get(blog["url"], timeout=10)
            if resp.ok:
                soup = BeautifulSoup(resp.text, 'xml')
                for entry in soup.select('entry')[:limit]:
                    title = entry.select_one('title')
                    link = entry.select_one('link')
                    items.append({
                        "title": title.get_text(strip=True) if title else "",
                        "url": link.get('href') if link else "",
                        "source": f"AI Blog - {blog['name']}",
                        "hot_score": 80,
                        "timestamp": datetime.now().isoformat(),
                    })
        except Exception as e:
            print(f"   âŒ {blog['name']} å¤±è´¥: {e}")
    
    print(f"   âœ… AI Blogs: {len(items)} æ¡")
    return items


def fetch_dev_to(limit=10):
    """æŠ“å– Dev.to AI æ–‡ç« """
    if not IMPORTS_OK:
        return []
    
    try:
        url = "https://dev.to/api/articles?tag=ai&top=1"
        resp = requests.get(url, timeout=10)
        if resp.ok:
            articles = resp.json()
            items = []
            for article in articles[:limit]:
                items.append({
                    "title": article.get("title", ""),
                    "url": article.get("url", ""),
                    "source": "Dev.to",
                    "hot_score": article.get("positive_reactions_count", 0),
                    "timestamp": article.get("published_at", datetime.now().isoformat()),
                })
            print(f"   âœ… Dev.to: {len(items)} æ¡")
            return items
    except Exception as e:
        print(f"   âŒ Dev.to å¤±è´¥: {e}")
    return []


def fetch_all_hotspots():
    """é‡‡é›†æ‰€æœ‰æ•°æ®æº"""
    all_items = []
    
    print("ğŸ“¥ å¼€å§‹é‡‡é›†æ•°æ®æº...")
    start_time = datetime.now()
    
    # 1. V2EX
    all_items.extend(fetch_v2ex_hotspots(limit=10))
    time.sleep(0.5)
    
    # 2. Hacker News
    all_items.extend(fetch_hacker_news(limit=10))
    time.sleep(0.5)
    
    # 3. GitHub Trending
    all_items.extend(fetch_github_trending(limit=10))
    time.sleep(0.5)
    
    # 4. AI Blogs
    all_items.extend(fetch_ai_blogs(limit=5))
    time.sleep(0.5)
    
    # 5. Dev.to
    all_items.extend(fetch_dev_to(limit=10))
    
    elapsed = (datetime.now() - start_time).total_seconds()
    print(f"\nğŸ“Š æ€»è®¡é‡‡é›† {len(all_items)} æ¡æ•°æ® ({elapsed:.1f}ç§’)")
    
    return all_items


def sort_hotspots(items):
    """ç®€å•æ’åº"""
    # æŒ‰ hot_score æ’åº
    return sorted(items, key=lambda x: x.get('hot_score', 0), reverse=True)


def generate_report(items):
    """ç”Ÿæˆæ—¥æŠ¥"""
    return {
        'success': True,
        'title': f'AI Daily - {datetime.now().strftime("%Y-%m-%d")}',
        'generated_at': datetime.now().isoformat(),
        'total_collected': len(items),
        'hotspots': items,
    }


def main():
    print("ğŸš€ AI Daily Collector - çœŸå®æ•°æ®é‡‡é›†")
    print("=" * 50)
    print(f"â° æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # åˆ›å»ºæ•°æ®ç›®å½•
    data_dir = Path('data')
    data_dir.mkdir(exist_ok=True)
    
    if IMPORTS_OK:
        # çœŸå®é‡‡é›†
        items = fetch_all_hotspots()
        items = sort_hotspots(items)
    else:
        # ç¤ºä¾‹æ•°æ®
        print("âš ï¸ ä½¿ç”¨ç¤ºä¾‹æ•°æ®")
        items = [{
            "title": "AI Daily Collector - çœŸå®æ•°æ®é‡‡é›†ä¸­",
            "url": "https://github.com/xxl115/ai-daily-collector",
            "source": "GitHub",
            "hot_score": 100,
            "timestamp": datetime.now().isoformat(),
        }]
    
    print("\nğŸ”„ å¤„ç†æ•°æ®...")
    print(f"   âœ… ä¿ç•™ {len(items)} æ¡çƒ­ç‚¹")
    
    print("\nğŸ“ ç”Ÿæˆæ—¥æŠ¥...")
    report = generate_report(items)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    output_file = data_dir / 'daily.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"   âœ… å·²ä¿å­˜åˆ°: {output_file}")
    
    print()
    print("=" * 50)
    print("âœ… é‡‡é›†å®Œæˆ!")
    print(f"   æ€»è®¡: {report['total_collected']} æ¡")
    print(f"   çƒ­ç‚¹: {len(items)} æ¡")
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
