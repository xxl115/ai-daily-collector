#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Daily Collector - ç®€åŒ–çš„é‡‡é›†è„šæœ¬ (ç”¨äº GitHub Actions)

åŠŸèƒ½:
1. é‡‡é›†å¤šå¹³å° AI çƒ­ç‚¹èµ„è®¯
2. ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š
3. ä¿å­˜åˆ° data/daily.json
"""

import json
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from fetchers import (
        fetch_v2ex_hotspots,
        fetch_reddit_hotspots,
        fetch_ai_blogs,
        fetch_tech_media,
    )
    from utils.filter import keyword_filter, sort_by_hotness
    FETCHERS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ æŠ“å–å™¨å¯¼å…¥å¤±è´¥: {e}")
    FETCHERS_AVAILABLE = False


def fetch_all_hotspots(max_per_source: int = 10) -> list:
    """é‡‡é›†æ‰€æœ‰æ•°æ®æº"""
    all_items = []
    
    print("ğŸ“¥ å¼€å§‹é‡‡é›†æ•°æ®æº...")
    start_time = datetime.now()
    
    # 1. V2EX
    try:
        if FETCHERS_AVAILABLE:
            v2ex = fetch_v2ex_hotspots(limit=max_per_source)
            print(f"   âœ… V2EX: {len(v2ex)} æ¡")
            all_items.extend(v2ex)
        else:
            print("   â­ï¸ V2EX (æŠ“å–å™¨ä¸å¯ç”¨)")
    except Exception as e:
        print(f"   âŒ V2EX å¤±è´¥: {e}")
    
    # 2. Reddit
    try:
        if FETCHERS_AVAILABLE:
            reddit = fetch_reddit_hotspots(limit=max_per_source)
            print(f"   âœ… Reddit: {len(reddit)} æ¡")
            all_items.extend(reddit)
        else:
            print("   â­å–å™¨ä¸å¯ç”¨ï¸ Reddit (æŠ“)")
    except Exception as e:
        print(f"   âŒ Reddit å¤±è´¥: {e}")
    
    # 3. AI åšå®¢
    try:
        if FETCHERS_AVAILABLE:
            blogs = fetch_ai_blogs(limit=max_per_source)
            print(f"   âœ… AI åšå®¢: {sum(len(v) for v in blogs.values())} æ¡")
            for source, items in blogs.items():
                for item in items:
                    item['source'] = source
                    all_items.append(item)
        else:
            print("   â­ï¸ AI åšå®¢ (æŠ“å–å™¨ä¸å¯ç”¨)")
    except Exception as e:
        print(f"   âŒ AI åšå®¢ å¤±è´¥: {e}")
    
    # 4. ç§‘æŠ€åª’ä½“
    try:
        if FETCHERS_AVAILABLE:
            media = fetch_tech_media(limit=max_per_source)
            print(f"   âœ… ç§‘æŠ€åª’ä½“: {sum(len(v) for v in media.values())} æ¡")
            for source, items in media.items():
                for item in items:
                    item['source'] = source
                    all_items.append(item)
        else:
            print("   â­ï¸ ç§‘æŠ€åª’ä½“ (æŠ“å–å™¨ä¸å¯ç”¨)")
    except Exception as e:
        print(f"   âŒ ç§‘æŠ€åª’ä½“ å¤±è´¥: {e}")
    
    elapsed = (datetime.now() - start_time).total_seconds()
    print(f"\nğŸ“Š æ€»è®¡é‡‡é›† {len(all_items)} æ¡æ•°æ® ({elapsed:.1f}ç§’)")
    
    return all_items


def process_hotspots(items: list, limit: int = 30) -> list:
    """å¤„ç†å’Œæ’åºçƒ­ç‚¹"""
    if not items:
        return []
    
    # å»é‡
    seen = set()
    unique_items = []
    for item in items:
        url = item.get('url', '')
        if url and url not in seen:
            seen.add(url)
            unique_items.append(item)
    
    # è¿‡æ»¤æ— æ•ˆé¡¹ç›®
    valid_items = [item for item in unique_items if item.get('title')]
    
    # æ’åº
    sorted_items = sort_by_hotness(valid_items)
    
    return sorted_items[:limit]


def generate_report(items: list) -> dict:
    """ç”Ÿæˆæ—¥æŠ¥"""
    return {
        'success': True,
        'title': f'AI Daily - {datetime.now().strftime("%Y-%m-%d")}',
        'generated_at': datetime.now().isoformat(),
        'total_collected': len(items),
        'hotspots': items,
    }


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ AI Daily Collector - çœŸå®æ•°æ®é‡‡é›†")
    print("=" * 50)
    print(f"â° æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # åˆ›å»ºæ•°æ®ç›®å½•
    data_dir = project_root / 'data'
    data_dir.mkdir(exist_ok=True)
    
    # é‡‡é›†æ•°æ®
    items = fetch_all_hotspots(max_per_source=10)
    
    # å¤„ç†æ•°æ®
    print("\nğŸ”„ å¤„ç†æ•°æ®...")
    hotspots = process_hotspots(items, limit=30)
    print(f"   âœ… ä¿ç•™ {len(hotspots)} æ¡çƒ­ç‚¹")
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“ ç”Ÿæˆæ—¥æŠ¥...")
    report = generate_report(hotspots)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    output_file = data_dir / 'daily.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"   âœ… å·²ä¿å­˜åˆ°: {output_file}")
    
    print()
    print("=" * 50)
    print("âœ… é‡‡é›†å®Œæˆ!")
    print(f"   æ€»è®¡: {report['total_collected']} æ¡")
    print(f"   çƒ­ç‚¹: {len(hotspots)} æ¡")
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
