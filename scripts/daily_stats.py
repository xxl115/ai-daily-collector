# -*- coding: utf-8 -*-
"""
æ—¥æŠ¥ç»Ÿè®¡æ¨¡å—

åŠŸèƒ½:
- è‡ªåŠ¨ç”Ÿæˆæ—¥æŠ¥ç»Ÿè®¡
- æ•°æ®æºåˆ†å¸ƒåˆ†æ
- å…³é”®è¯æå–å’Œè¶‹åŠ¿
- å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆ
"""

import re
from collections import Counter
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import yaml


class DailyStatsAnalyzer:
    """æ—¥æŠ¥ç»Ÿè®¡åˆ†æå™¨"""
    
    def __init__(self, daily_dir: str = "ai/daily"):
        """
        åˆå§‹åŒ–
        
        Args:
            daily_dir: æ—¥æŠ¥ç›®å½•
        """
        self.daily_dir = Path(daily_dir)
        self.cache_dir = Path(".cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def load_daily_report(self, date: str = None) -> Optional[Dict]:
        """
        åŠ è½½æ—¥æŠ¥
        
        Args:
            date: æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DDï¼Œé»˜è®¤ä»Šå¤©
        
        Returns:
            æ—¥æŠ¥æ•°æ®
        """
        date = date or datetime.now().strftime("%Y-%m-%d")
        file_path = self.daily_dir / f"ai-hotspot-{date}.md"
        
        if not file_path.exists():
            return None
        
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        
        return {
            "date": date,
            "content": content,
            "file_path": str(file_path),
        }
    
    def parse_sections(self, content: str) -> List[Dict]:
        """
        è§£ææ—¥æŠ¥ sections
        
        Args:
            content: æ—¥æŠ¥å†…å®¹
        
        Returns:
            Section åˆ—è¡¨
        """
        sections = []
        
        # åŒ¹é… ## å¼€å¤´çš„ç« èŠ‚
        pattern = r"##\s*(\d+ï¸âƒ£)?\s*(.+?)\n(.*?)(?=##\s|\Z)"
        matches = re.findall(pattern, content, re.DOTALL)
        
        for num, title, body in matches:
            # æå–æ–‡ç« åˆ—è¡¨
            articles = self._parse_articles(body)
            
            sections.append({
                "title": title.strip(),
                "articles": articles,
                "article_count": len(articles),
            })
        
        return sections
    
    def _parse_articles(self, body: str) -> List[Dict]:
        """è§£ææ–‡ç« åˆ—è¡¨"""
        articles = []
        
        # åŒ¹é… ### å¼€å¤´çš„æ–‡ç« 
        pattern = r"###\s+(.+?)\n.*?æ¥æº:\s*(\S+).*?æ€»ç»“:\s*(.+?)(?=###|---|\Z)"
        matches = re.findall(pattern, body, re.DOTALL)
        
        for title, source, summary in matches:
            # æå–é“¾æ¥
            link_match = re.search(r"\[é“¾æ¥\]\((.+?)\)", body)
            link = link_match.group(1) if link_match else ""
            
            articles.append({
                "title": title.strip()[:200],
                "source": source,
                "summary": summary.strip()[:500],
                "link": link,
            })
        
        return articles
    
    def extract_keywords(self, text: str, top_n: int = 20) -> List[Tuple[str, int]]:
        """
        æå–å…³é”®è¯
        
        Args:
            text: æ–‡æœ¬
            top_n: è¿”å›æ•°é‡
        
        Returns:
            å…³é”®è¯åˆ—è¡¨ (è¯, é¢‘ç‡)
        """
        # åœç”¨è¯
        stopwords = {
            "çš„", "æ˜¯", "åœ¨", "å’Œ", "ä¸", "æˆ–", "ç­‰", "äº†", "ä¸º", "äº",
            "the", "a", "an", "is", "are", "was", "were", "and", "or",
            "to", "of", "in", "on", "for", "with", "by", "from",
            "ai", "an", "this", "that", "it", "be", "as", "at",
        }
        
        # æå–è¯
        words = re.findall(r"[ä¸€-é¾¥]{2,}|[a-zA-Z]{3,}", text.lower())
        
        # è¿‡æ»¤åœç”¨è¯
        words = [w for w in words if w not in stopwords and len(w) < 20]
        
        # ç»Ÿè®¡
        counter = Counter(words)
        return counter.most_common(top_n)
    
    def analyze_source_distribution(self, sections: List[Dict]) -> Dict:
        """
        åˆ†ææ•°æ®æºåˆ†å¸ƒ
        
        Returns:
            æºåˆ†å¸ƒç»Ÿè®¡
        """
        sources = []
        
        for section in sections:
            for article in section["articles"]:
                sources.append(article["source"])
        
        counter = Counter(sources)
        total = sum(counter.values())
        
        return {
            "sources": [
                {
                    "name": source,
                    "count": count,
                    "percentage": round(count / total * 100, 1),
                }
                for source, count in counter.most_common()
            ],
            "total": total,
            "unique_sources": len(counter),
        }
    
    def analyze_trends(self, days: int = 7) -> Dict:
        """
        åˆ†æè¶‹åŠ¿ï¼ˆæœ€è¿‘ N å¤©ï¼‰
        
        Args:
            days: å¤©æ•°
        
        Returns:
            è¶‹åŠ¿æ•°æ®
        """
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        stats = []
        keywords = []
        
        for i in range(days):
            date = (start_date + timedelta(days=i)).strftime("%Y-%m-%d")
            report = self.load_daily_report(date)
            
            if report:
                sections = self.parse_sections(report["content"])
                
                # æ–‡ç« æ•°
                total_articles = sum(len(s["articles"]) for s in sections)
                stats.append({
                    "date": date,
                    "articles": total_articles,
                    "sections": len(sections),
                })
                
                # å…³é”®è¯
                daily_keywords = self.extract_keywords(report["content"], 10)
                for word, count in daily_keywords:
                    keywords.append((word, date, count))
        
        return {
            "period": f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}",
            "daily_stats": stats,
            "keyword_trends": self._aggregate_keyword_trends(keywords),
        }
    
    def _aggregate_keyword_trends(
        self,
        keywords: List[Tuple[str, str, int]],
    ) -> Dict[str, List[Dict]]:
        """èšåˆå…³é”®è¯è¶‹åŠ¿"""
        trends = {}
        
        for word, date, count in keywords:
            if word not in trends:
                trends[word] = []
            trends[word].append({
                "date": date,
                "count": count,
            })
        
        # è¿”å›å‡ºç°é¢‘ç‡é«˜çš„è¯
        frequent = sorted(trends.keys(), key=lambda x: sum(
            d["count"] for d in trends[x]
        ), reverse=True)[:20]
        
        return {
            word: trends[word] for word in frequent
        }
    
    def generate_summary(self, report: Dict) -> Dict:
        """
        ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
        
        Args:
            report: æ—¥æŠ¥æ•°æ®
        
        Returns:
            ç»Ÿè®¡æ‘˜è¦
        """
        sections = self.parse_sections(report["content"])
        
        # åŸºç¡€ç»Ÿè®¡
        total_articles = sum(len(s["articles"]) for s in sections)
        
        # å…³é”®è¯
        keywords = self.extract_keywords(report["content"], 20)
        
        # æ•°æ®æº
        source_dist = self.analyze_source_distribution(sections)
        
        # åˆ†ç±»ç»Ÿè®¡
        category_stats = [
            {
                "category": s["title"],
                "count": s["article_count"],
                "percentage": round(s["article_count"] / total_articles * 100, 1),
            }
            for s in sections
        ]
        
        return {
            "date": report["date"],
            "total_articles": total_articles,
            "total_sections": len(sections),
            "categories": category_stats,
            "sources": source_dist["sources"],
            "keywords": [{"word": w, "count": c} for w, c in keywords],
            "generated_at": datetime.now().isoformat(),
        }
    
    def generate_markdown_report(self, report: Dict) -> str:
        """
        ç”Ÿæˆ Markdown ç»Ÿè®¡æŠ¥å‘Š
        
        Args:
            report: æ—¥æŠ¥æ•°æ®
        
        Returns:
            Markdown æ ¼å¼ç»Ÿè®¡æŠ¥å‘Š
        """
        summary = self.generate_summary(report)
        
        lines = [
            "---",
            "title: AI æ—¥æŠ¥ç»Ÿè®¡",
            f"date: {summary['date']}",
            "tags: [ç»Ÿè®¡, æ—¥æŠ¥, åˆ†æ]",
            "---",
            "",
            f"# ğŸ“Š AI æ—¥æŠ¥ç»Ÿè®¡ {summary['date']}",
            "",
            "> è‡ªåŠ¨ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š",
            "",
            "---",
            "",
            "## 1ï¸âƒ£ åŸºç¡€ç»Ÿè®¡",
            "",
            f"- **æ–‡ç« æ€»æ•°**: {summary['total_articles']} ç¯‡",
            f"- **åˆ†ç±»æ•°**: {summary['total_sections']} ä¸ª",
            f"- **æ•°æ®æº**: {summary['sources'][0]['name']} ä¸ºä¸» ({summary['sources'][0]['percentage']}%)",
            f"- **ç‹¬ç«‹æ¥æº**: {summary['sources'][-1]['name']} ç­‰ {len(summary['sources'])} ä¸ª",
            "",
            "## 2ï¸âƒ£ åˆ†ç±»åˆ†å¸ƒ",
            "",
            "| åˆ†ç±» | æ–‡ç« æ•° | å æ¯” |",
            "|------|--------|------|",
        ]
        
        for cat in summary["categories"]:
            lines.append(f"| {cat['category']} | {cat['count']} | {cat['percentage']}% |")
        
        lines.extend([
            "",
            "## 3ï¸âƒ£ æ•°æ®æºåˆ†å¸ƒ",
            "",
            "| æ¥æº | æ•°é‡ | å æ¯” |",
            "|------|------|------|",
        ])
        
        for source in summary["sources"]:
            lines.append(f"| {source['name']} | {source['count']} | {source['percentage']}% |")
        
        lines.extend([
            "",
            "## 4ï¸âƒ£ çƒ­é—¨å…³é”®è¯",
            "",
            "```",
        ])
        
        for kw in summary["keywords"][:15]:
            lines.append(f"  {kw['word']}: {kw['count']}")
        
        lines.extend([
            "```",
            "",
            f"_ç»Ÿè®¡ç”Ÿæˆæ—¶é—´: {summary['generated_at']}_",
        ])
        
        return "\n".join(lines)
    
    def save_stats_report(self, date: str = None) -> str:
        """
        ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        
        Args:
            date: æ—¥æœŸ
        
        Returns:
            ä¿å­˜è·¯å¾„
        """
        report = self.load_daily_report(date)
        if not report:
            return None
        
        md_report = self.generate_markdown_report(report)
        
        stats_path = self.daily_dir / f"stats-{report['date']}.md"
        with open(stats_path, "w", encoding="utf-8") as f:
            f.write(md_report)
        
        return str(stats_path)


def analyze_daily_stats(date: str = None) -> Dict:
    """
    å¿«é€Ÿåˆ†ææ—¥æŠ¥ç»Ÿè®¡
    
    Usage:
        stats = analyze_daily_stats("2026-02-03")
        print(stats['total_articles'])
        print(stats['keywords'])
    """
    analyzer = DailyStatsAnalyzer()
    report = analyzer.load_daily_report(date)
    
    if not report:
        return {"error": f"æœªæ‰¾åˆ° {date} çš„æ—¥æŠ¥"}
    
    return analyzer.generate_summary(report)


def generate_stats_markdown(date: str = None) -> str:
    """
    ç”Ÿæˆ Markdown ç»Ÿè®¡æŠ¥å‘Š
    
    Usage:
        md = generate_stats_markdown("2026-02-03")
        print(md)
    """
    analyzer = DailyStatsAnalyzer()
    report = analyzer.load_daily_report(date)
    
    if not report:
        return f"# æœªæ‰¾åˆ° {date} çš„æ—¥æŠ¥"
    
    return analyzer.generate_markdown_report(report)


if __name__ == "__main__":
    # ç¤ºä¾‹
    print("=== AI æ—¥æŠ¥ç»Ÿè®¡ ===")
    
    # åˆ†æä»Šå¤©
    stats = analyze_daily_stats()
    if "error" in stats:
        print(stats["error"])
    else:
        print(f"æ—¥æœŸ: {stats['date']}")
        print(f"æ–‡ç« æ•°: {stats['total_articles']}")
        print(f"åˆ†ç±»: {stats['total_sections']}")
        print()
        print("åˆ†ç±»åˆ†å¸ƒ:")
        for cat in stats["categories"]:
            print(f"  {cat['category']}: {cat['count']} ({cat['percentage']}%)")
        print()
        print("æ•°æ®æº:")
        for source in stats["sources"][:5]:
            print(f"  {source['name']}: {source['count']} ({source['percentage']}%)")
        print()
        print("å…³é”®è¯:")
        for kw in stats["keywords"][:10]:
            print(f"  {kw['word']}: {kw['count']}")
