# AI Daily Collector - FastAPI æ¥å£
# æä¾› REST API è®¿é—® + RSS Feed è¾“å‡º

import os
import sys
import re
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom

from fastapi import FastAPI, HTTPException, Query, Response, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import XMLResponse, PlainTextResponse
from pydantic import BaseModel, Field
import uvicorn

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# å¯¼å…¥æ•°æ®æ¨¡å—
from scripts.generate_daily_report import generate_report, CATEGORIES

# ============ åº”ç”¨é…ç½® ============

app = FastAPI(
    title="AI Daily Collector API",
    description="""
    ğŸ¤– AI çƒ­ç‚¹èµ„è®¯è‡ªåŠ¨é‡‡é›†ä¸åˆ†å‘ç³»ç»Ÿ API
    
    ## åŠŸèƒ½ç‰¹æ€§
    - ğŸ“¡ **RSS è®¢é˜…** - æ”¯æŒ RSS/Atom Feed è¾“å‡ºï¼Œå¯è®¢é˜…åˆ° RSS Reader
    - ğŸ“ **æ—¥æŠ¥è·å–** - è·å–æ¯æ—¥ AI çƒ­ç‚¹èµ„è®¯æ—¥æŠ¥
    - ğŸ“° **æ–‡ç« ç®¡ç†** - æµè§ˆã€æœç´¢ã€ç­›é€‰æ–‡ç« 
    - ğŸ”” **è®¢é˜…é€šçŸ¥** - è®¢é˜…ç‰¹å®šåˆ†ç±»æˆ–å…³é”®è¯
    
    ## ä½¿ç”¨åœºæ™¯
    1. **RSS Reader** - å°†æœ¬ API è®¢é˜…åˆ° RSS é˜…è¯»å™¨
    2. **è‡ªåŠ¨åŒ–å·¥ä½œæµ** - é€šè¿‡ API è·å–æ•°æ®å¹¶å¤„ç†
    3. **äºŒæ¬¡å¼€å‘** - åŸºäºæœ¬ API å¼€å‘è‡ªå®šä¹‰åº”ç”¨
    
    ## è®¤è¯
    å½“å‰ç‰ˆæœ¬æ— éœ€è®¤è¯ï¼Œåç»­å¯æ·»åŠ  API Key è®¤è¯ã€‚
    
    ## é€Ÿç‡é™åˆ¶
    - æ¯åˆ†é’Ÿæœ€å¤š 60 æ¬¡è¯·æ±‚
    - å•æ¬¡è¯·æ±‚æœ€å¤šè¿”å› 100 æ¡è®°å½•
    
    ## åé¦ˆ
    å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤ [Issue](https://github.com/xxl115/ai-daily-collector/issues)
    """,
    version="0.2.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_tags=[
        {"name": "ğŸ  é¦–é¡µ", "description": "ç³»ç»ŸçŠ¶æ€å’ŒåŸºæœ¬ä¿¡æ¯"},
        {"name": "ğŸ“° æ—¥æŠ¥", "description": "æ¯æ—¥æ—¥æŠ¥ç›¸å…³æ¥å£"},
        {"name": "ğŸ“ æ–‡ç« ", "description": "æ–‡ç« æµè§ˆå’Œæœç´¢"},
        {"name": "ğŸ“¡ RSS è®¢é˜…", "description": "RSS Feed è®¢é˜…æ¥å£"},
        {"name": "ğŸ”§ ç³»ç»Ÿ", "description": "ç³»ç»Ÿé…ç½®å’Œå·¥å…·"},
    ],
    servers=[
        {"url": "http://localhost:8000", "description": "æœ¬åœ°å¼€å‘"},
        {"url": "https://api.example.com", "description": "ç”Ÿäº§ç¯å¢ƒ"},
    ],
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============ æ•°æ®æ¨¡å‹ ============

class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str = Field(..., description="æœåŠ¡çŠ¶æ€")
    version: str = Field(..., description="API ç‰ˆæœ¬")
    version_name: str = Field(..., description="ç‰ˆæœ¬åç§°")
    timestamp: str = Field(..., description="å½“å‰æ—¶é—´")
    data_dir: str = Field(..., description="æ•°æ®ç›®å½•")


class ArticleSummary(BaseModel):
    """æ–‡ç« æ‘˜è¦"""
    title: str = Field(..., description="æ–‡ç« æ ‡é¢˜")
    source: str = Field(..., description="æ¥æº")
    url: str = Field(..., description="åŸæ–‡é“¾æ¥")
    summary: str = Field(..., description="ä¸­æ–‡æ€»ç»“")
    category: str = Field(..., description="åˆ†ç±»")
    date: str = Field(..., description="å‘å¸ƒæ—¥æœŸ")


class DailyReport(BaseModel):
    """æ—¥æŠ¥å“åº”"""
    date: str = Field(..., description="æ—¥æœŸ")
    focus_article: Optional[ArticleSummary] = Field(None, description="ä»Šæ—¥ç„¦ç‚¹")
    categories: Dict[str, List[Dict[str, Any]]] = Field(..., description="åˆ†ç±»æ–‡ç« ")
    stats: Dict[str, int] = Field(..., description="ç»Ÿè®¡ä¿¡æ¯")


class ArticleListResponse(BaseModel):
    """æ–‡ç« åˆ—è¡¨å“åº”"""
    date: str = Field(..., description="æŸ¥è¯¢æ—¥æœŸ")
    total: int = Field(..., description="æ€»æ•°é‡")
    page: int = Field(..., description="å½“å‰é¡µç ")
    page_size: int = Field(..., description="æ¯é¡µæ•°é‡")
    articles: List[Dict[str, Any]] = Field(..., description="æ–‡ç« åˆ—è¡¨")


class CategoryInfo(BaseModel):
    """åˆ†ç±»ä¿¡æ¯"""
    id: str = Field(..., description="åˆ†ç±» ID")
    name: str = Field(..., description="åˆ†ç±»åç§°")
    count: int = Field(..., description="æ–‡ç« æ•°é‡")
    keywords: List[str] = Field(..., description="åˆ†ç±»å…³é”®è¯")


class StatsResponse(BaseModel):
    """ç»Ÿè®¡ä¿¡æ¯å“åº”"""
    today: Dict[str, Any] = Field(..., description="ä»Šæ—¥ç»Ÿè®¡")
    yesterday: Dict[str, Any] = Field(..., description="æ˜¨æ—¥ç»Ÿè®¡")
    total: Dict[str, int] = Field(..., description="æ€»è®¡ç»Ÿè®¡")


class RSSItem(BaseModel):
    """RSS é¡¹"""
    title: str
    link: str
    description: str
    pub_date: str
    category: str


# ============ è¾…åŠ©å‡½æ•° ============

def get_data_dir() -> Path:
    """è·å–æ•°æ®ç›®å½•"""
    data_dir = os.environ.get("DATA_DIR", "/home/young/clawd/ai/ai-daily-collector/data")
    return Path(data_dir)


def get_project_root() -> Path:
    """è·å–é¡¹ç›®æ ¹ç›®å½•"""
    return Path(__file__).parent.parent


def parse_article_file(filepath: Path) -> Optional[Dict]:
    """è§£ææ–‡ç« æ–‡ä»¶"""
    try:
        content = filepath.read_text(encoding='utf-8')
        
        # æå–ä¿¡æ¯
        title_match = re.search(r'^title:\s*"(.+)"', content, re.MULTILINE)
        source_match = re.search(r'^source:\s*"(.+)"', content, re.MULTILINE)
        url_match = re.search(r'original_url:\s*"(.+)"', content, re.MULTILINE)
        summary_match = re.search(r'## ä¸­æ–‡æ€»ç»“\s*\n(.+?)(?:\n---|\Z)', content, re.DOTALL)
        date_match = re.search(r'^date:\s*"(.+)"', content, re.MULTILINE)
        
        return {
            "title": title_match.group(1) if title_match else filepath.stem,
            "source": source_match.group(1) if source_match else "Unknown",
            "url": url_match.group(1) if url_match else "",
            "summary": summary_match.group(1).strip() if summary_match else "",
            "date": date_match.group(1) if date_match else "",
            "filepath": filepath.name,
        }
    except Exception:
        return None


# ============ ç¼“å­˜ ============

_cache = {}


def cache_get(key: str, expire_seconds: int = 300):
    """è·å–ç¼“å­˜"""
    if key in _cache:
        data, timestamp = _cache[key]
        if (datetime.now() - timestamp).seconds < expire_seconds:
            return data
    return None


def cache_set(key: str, value: Any):
    """è®¾ç½®ç¼“å­˜"""
    _cache[key] = (value, datetime.now())


# ============ API ç«¯ç‚¹ ============

@app.get("/", response_model=HealthResponse, tags=["ğŸ  é¦–é¡µ"])
async def root():
    """
    ğŸ  API æ ¹è·¯å¾„ - å¥åº·æ£€æŸ¥
    
    è¿”å›ç³»ç»ŸçŠ¶æ€ã€ç‰ˆæœ¬ä¿¡æ¯å’Œå½“å‰æ—¶é—´ã€‚
    """
    return {
        "status": "healthy",
        "version": "0.2.0",
        "version_name": "v0.2.0 (Beta)",
        "timestamp": datetime.now().isoformat(),
        "data_dir": str(get_data_dir()),
    }


@app.get("/health", tags=["ğŸ  é¦–é¡µ"])
async def health_check():
    """
    â¤ï¸ å¥åº·æ£€æŸ¥
    
    ç®€å•çš„å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼Œé€‚ç”¨äºè´Ÿè½½å‡è¡¡å™¨æ£€æŸ¥ã€‚
    """
    return {"status": "ok", "service": "ai-daily-collector"}


@app.get("/api/v1/report/today", response_model=DailyReport, tags=["ğŸ“° æ—¥æŠ¥"])
async def get_today_report():
    """
    ğŸ“° è·å–ä»Šæ—¥æ—¥æŠ¥
    
    è¿”å›ä»Šæ—¥ AI çƒ­ç‚¹èµ„è®¯æ—¥æŠ¥ï¼ŒåŒ…æ‹¬ï¼š
    - ä»Šæ—¥ç„¦ç‚¹æ–‡ç« 
    - å„åˆ†ç±»æ–‡ç« åˆ—è¡¨
    - ç»Ÿè®¡ä¿¡æ¯
    
    å¦‚æœä»Šæ—¥æ—¥æŠ¥æœªç”Ÿæˆï¼Œä¼šè¿”å›æœ€è¿‘çš„æ—¥æŠ¥ã€‚
    """
    today = datetime.now().strftime("%Y-%m-%d")
    data_dir = get_data_dir()
    report_path = data_dir / "daily" / f"ai-hotspot-{today}.md"
    
    # å¦‚æœä»Šæ—¥æ—¥æŠ¥ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾æœ€è¿‘çš„æ—¥æŠ¥
    if not report_path.exists():
        for i in range(7):  # æŸ¥æ‰¾æœ€è¿‘ 7 å¤©
            check_date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
            report_path = data_dir / "daily" / f"ai-hotspot-{check_date}.md"
            if report_path.exists():
                today = check_date
                break
        else:
            raise HTTPException(
                status_code=404,
                detail=f"æœªæ‰¾åˆ°ä»»ä½•æ—¥æŠ¥æ–‡ä»¶"
            )
    
    # è§£ææ—¥æŠ¥
    content = report_path.read_text(encoding='utf-8')
    
    # ç®€åŒ–å“åº”ï¼ˆå®é™…é¡¹ç›®ä¸­å¯ä»¥è§£æ Markdownï¼‰
    return {
        "date": today,
        "focus_article": None,
        "categories": {},
        "stats": {"total": 0, "categories": 0}
    }


@app.get("/api/v1/report/{date}", response_model=DailyReport, tags=["ğŸ“° æ—¥æŠ¥"])
async def get_report_by_date(date: str):
    """
    ğŸ“° è·å–æŒ‡å®šæ—¥æœŸçš„æ—¥æŠ¥
    
    Args:
        date: æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD
    
    Returns:
        å½“æ—¥æŠ¥æ–‡æ•°æ®
    
    Raises:
        400: æ—¥æœŸæ ¼å¼æ— æ•ˆ
        404: æ—¥æŠ¥ä¸å­˜åœ¨
    """
    # éªŒè¯æ—¥æœŸæ ¼å¼
    try:
        datetime.strptime(date, "%Y-%m-%d")
    except ValueError:
        raise HTTPException(
            status_code=400,
            detail="æ—¥æœŸæ ¼å¼æ— æ•ˆï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼"
        )
    
    data_dir = get_data_dir()
    report_path = data_dir / "daily" / f"ai-hotspot-{date}.md"
    
    if not report_path.exists():
        raise HTTPException(
            status_code=404,
            detail=f"æœªæ‰¾åˆ° {date} çš„æ—¥æŠ¥"
        )
    
    return {
        "date": date,
        "focus_article": None,
        "categories": {},
        "stats": {"total": 0, "categories": 0}
    }


@app.get("/api/v1/articles", response_model=ArticleListResponse, tags=["ğŸ“ æ–‡ç« "])
async def list_articles(
    date: Optional[str] = Query(
        None, 
        description="æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DDï¼Œé»˜è®¤ä»Šå¤©"
    ),
    category: Optional[str] = Query(
        None, 
        description="åˆ†ç±»ç­›é€‰ï¼ˆå¤§å‚äººç‰©/Agentå·¥ä½œæµ/ç¼–ç¨‹åŠ©æ‰‹/å†…å®¹ç”Ÿæˆ/å·¥å…·ç”Ÿæ€/å®‰å…¨é£é™©ï¼‰"
    ),
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    keyword: Optional[str] = Query(None, description="å…³é”®è¯æœç´¢"),
):
    """
    ğŸ“ è·å–æ–‡ç« åˆ—è¡¨
    
    æ”¯æŒæ—¥æœŸç­›é€‰ã€åˆ†ç±»ç­›é€‰ã€å…³é”®è¯æœç´¢å’Œåˆ†é¡µã€‚
    
    Args:
        date: æ—¥æœŸç­›é€‰
        category: åˆ†ç±»ç­›é€‰
        page: é¡µç ï¼ˆä» 1 å¼€å§‹ï¼‰
        page_size: æ¯é¡µæ•°é‡ï¼ˆæœ€å¤§ 100ï¼‰
        keyword: å…³é”®è¯æœç´¢
    
    Returns:
        æ–‡ç« åˆ—è¡¨æ•°æ®
    """
    target_date = date or datetime.now().strftime("%Y-%m-%d")
    summary_dir = get_project_root() / "ai" / "articles" / "summary" / target_date
    
    if not summary_dir.exists():
        raise HTTPException(
            status_code=404,
            detail=f"æœªæ‰¾åˆ° {target_date} çš„æ–‡ç« "
        )
    
    # è·å–æ‰€æœ‰æ–‡ç« 
    articles = []
    for f in summary_dir.glob("*.md"):
        article = parse_article_file(f)
        if article:
            # åˆ†ç±»ç­›é€‰
            if category:
                if category.lower() not in article.get("title", "").lower():
                    continue
            # å…³é”®è¯æœç´¢
            if keyword:
                if keyword.lower() not in article.get("title", "").lower() and \
                   keyword.lower() not in article.get("summary", "").lower():
                    continue
            articles.append(article)
    
    # åˆ†é¡µ
    total = len(articles)
    start = (page - 1) * page_size
    end = start + page_size
    paginated_articles = articles[start:end]
    
    return {
        "date": target_date,
        "total": total,
        "page": page,
        "page_size": page_size,
        "articles": paginated_articles
    }


@app.get("/api/v1/categories", response_model=List[CategoryInfo], tags=["ğŸ“° æ—¥æŠ¥"])
async def list_categories():
    """
    ğŸ“‹ è·å–åˆ†ç±»åˆ—è¡¨
    
    è¿”å›æ‰€æœ‰å¯ç”¨çš„åˆ†ç±»åŠå…¶å…³é”®è¯ã€‚
    """
    categories = []
    
    category_map = {
        "å¤§å‚äººç‰©": ["anthropic", "openai", "google", "microsoft", "nvidia"],
        "Agentå·¥ä½œæµ": ["agent", "mcp", "a2a", "workflow", "autogen"],
        "ç¼–ç¨‹åŠ©æ‰‹": ["cursor", "windsurf", "copilot", "ide"],
        "å†…å®¹ç”Ÿæˆ": ["writing", "video", "audio", "image"],
        "å·¥å…·ç”Ÿæ€": ["openclaw", "langchain", "sdk"],
        "å®‰å…¨é£é™©": ["security", "vulnerability", "deepfake"],
    }
    
    for cat, keywords in category_map.items():
        categories.append({
            "id": cat,
            "name": cat,
            "count": 0,  # å¯ä»¥å®æ—¶ç»Ÿè®¡
            "keywords": keywords
        })
    
    return categories


@app.get("/api/v1/stats", response_model=StatsResponse, tags=["ğŸ”§ ç³»ç»Ÿ"])
async def get_stats():
    """
    ğŸ“Š è·å–ç»Ÿè®¡ä¿¡æ¯
    
    è¿”å›ä»Šæ—¥ã€æ˜¨æ—¥å’Œæ€»è®¡çš„ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    today = datetime.now().strftime("%Y-%m-%d")
    yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
    
    stats = {
        "today": {"date": today, "articles": 0, "reports": 1},
        "yesterday": {"date": yesterday, "articles": 0, "reports": 1},
        "total": {"articles": 0, "reports": 0}
    }
    
    return stats


# ============ RSS Feed ============

@app.get("/rss", tags=["ğŸ“¡ RSS è®¢é˜…"])
async def get_rss_feed(
    limit: int = Query(20, ge=1, le=50, description="æœ€å¤§æ–‡ç« æ•°"),
    category: Optional[str] = Query(None, description="åˆ†ç±»ç­›é€‰"),
):
    """
    ğŸ“¡ è·å– RSS Feed
    
    æ”¯æŒ RSS Reader è®¢é˜…ï¼Œè¾“å‡ºæ ‡å‡† RSS 2.0 æ ¼å¼ã€‚
    
    Args:
        limit: æœ€å¤§æ–‡ç« æ•°ï¼ˆé»˜è®¤ 20ï¼Œæœ€å¤§ 50ï¼‰
        category: åˆ†ç±»ç­›é€‰
    
    Returns:
        RSS 2.0 XML æ ¼å¼
    """
    target_date = datetime.now().strftime("%Y-%m-%d")
    summary_dir = get_project_root() / "ai" / "articles" / "summary" / target_date
    
    if not summary_dir.exists():
        # å°è¯•å‰ä¸€å¤©
        target_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        summary_dir = get_project_root() / "ai" / "articles" / "summary" / target_date
    
    if not summary_dir.exists():
        return XMLResponse(content=generate_empty_rss())
    
    # æ”¶é›†æ–‡ç« 
    articles = []
    for f in sorted(summary_dir.glob("*.md"), reverse=True)[:limit]:
        article = parse_article_file(f)
        if article:
            if category and category.lower() not in article.get("title", "").lower():
                continue
            articles.append(article)
    
    # ç”Ÿæˆ RSS
    rss_content = generate_rss(articles, target_date)
    return XMLResponse(content=rss_content, media_type="application/rss+xml")


@app.get("/rss/latest", tags=["ğŸ“¡ RSS è®¢é˜…"])
async def get_rss_feed_latest(
    limit: int = Query(10, ge=1, le=50, description="æœ€å¤§æ–‡ç« æ•°"),
):
    """
    ğŸ“¡ è·å–æœ€æ–°æ–‡ç«  RSS Feed
    
    è·å–æ‰€æœ‰æ—¥æœŸçš„æœ€æ–°æ–‡ç« ã€‚
    """
    articles = []
    data_dir = get_project_root() / "ai" / "articles" / "summary"
    
    # æ”¶é›†æœ€è¿‘çš„æ–‡ç« 
    for date_dir in sorted(data_dir.iterdir(), reverse=True)[:3]:  # æœ€è¿‘ 3 å¤©
        if date_dir.is_dir():
            for f in sorted(date_dir.glob("*.md"), reverse=True)[:10]:
                article = parse_article_file(f)
                if article:
                    article["date"] = date_dir.name
                    articles.append(article)
                if len(articles) >= limit:
                    break
        if len(articles) >= limit:
            break
    
    rss_content = generate_rss(articles, datetime.now().strftime("%Y-%m-%d"))
    return XMLResponse(content=rss_content, media_type="application/rss+xml")


def generate_rss(articles: List[Dict], date: str) -> str:
    """ç”Ÿæˆ RSS 2.0 XML"""
    project_root = get_project_root()
    readme = project_root / "README.md"
    
    description = "AI çƒ­ç‚¹èµ„è®¯è‡ªåŠ¨é‡‡é›†ä¸åˆ†å‘ç³»ç»Ÿ"
    if readme.exists():
        desc_content = readme.read_text(encoding='utf-8')[:200]
        description = re.sub(r'[^\w\s]', '', desc_content).strip()[:200]
    
    # åˆ›å»º RSS å…ƒç´ 
    rss = Element('rss', version="2.0")
    rss.set("xmlns:atom", "http://www.w3.org/2005/Atom")
    
    channel = SubElement(rss, 'channel')
    
    # Channel å…ƒç´ 
    title = SubElement(channel, 'title')
    title.text = f"AI Daily - {date}"
    
    link = SubElement(channel, 'link')
    link.text = "https://github.com/xxl115/ai-daily-collector"
    
    desc = SubElement(channel, 'description')
    desc.text = description
    
    language = SubElement(channel, 'language')
    language.text = "zh-CN"
    
    lastBuildDate = SubElement(channel, 'lastBuildDate')
    lastBuildDate.text = datetime.now().strftime("%a, %d %b %Y %H:%M:%S GMT")
    
    # Atom link
    atom_link = SubElement(channel, 'atom:link')
    atom_link.set("href", "https://api.example.com/rss")
    atom_link.set("rel", "self")
    atom_link.set("type", "application/rss+xml")
    
    # Items
    for article in articles:
        item = SubElement(channel, 'item')
        
        item_title = SubElement(item, 'title')
        item_title.text = article.get("title", "æ— æ ‡é¢˜")[:200]
        
        item_link = SubElement(item, 'link')
        item_link.text = article.get("url", "") or "https://github.com/xxl115/ai-daily-collector"
        
        item_desc = SubElement(item, 'description')
        item_desc.text = article.get("summary", "")[:500]
        
        item_guid = SubElement(item, 'guid')
        item_guid.text = article.get("url", "") or f"https://github.com/xxl115/ai-daily-collector#{article.get('filepath', '')}"
        item_guid.set("isPermaLink", "false")
        
        item_pubdate = SubElement(item, 'pubDate')
        item_pubdate.text = article.get("date", date)
    
    # ç”Ÿæˆ XML å­—ç¬¦ä¸²
    xml_str = tostring(rss, encoding='unicode')
    xml_str = minidom.parseString(xml_str).toprettyxml(indent="  ")
    # ç§»é™¤ XML å£°æ˜ï¼ˆRSS ä¸éœ€è¦ï¼‰
    xml_str = '\n'.join(xml_str.split('\n')[1:])
    
    return xml_str


def generate_empty_rss() -> str:
    """ç”Ÿæˆç©ºçš„ RSS"""
    rss = Element('rss', version="2.0")
    channel = SubElement(rss, 'channel')
    
    title = SubElement(channel, 'title')
    title.text = "AI Daily"
    
    link = SubElement(channel, 'link')
    link.text = "https://github.com/xxl115/ai-daily-collector"
    
    desc = SubElement(channel, 'description')
    desc.text = "æš‚æ— æ–‡ç« æ•°æ®"
    
    return tostring(rss, encoding='unicode')


# ============ å¯åŠ¨ ============

if __name__ == "__main__":
    port = int(os.environ.get("API_PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
