"""
API v2 è·¯ç”±
é€‚é…å‰ç«¯éœ€æ±‚çš„ REST API ç«¯ç‚¹
"""
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, List
import random

from fastapi import APIRouter, Query, HTTPException
from pydantic import BaseModel

from .models import (
    ArticleModel,
    ArticleListResponse,
    ArticleListDataResponse,
    SuggestionsDataResponse,
    SuggestionsResponse,
    CategoriesDataResponse,
    SourcesDataResponse,
    StatsDataResponse,
    StatsResponse,
    StatsInfo,
    CategoryInfo,
    SourceInfo,
    SearchSuggestion,
    ArticleCategory,
    TimeFilter,
    SortOption,
)
from .utils import ArticleTransformer, CategoryClassifier, TagExtractor, cache_manager


# ==================== å·¥å…·ç±»å®ä¾‹ ====================

article_transformer = ArticleTransformer()
category_classifier = CategoryClassifier()
tag_extractor = TagExtractor()


# ==================== è·¯ç”± ====================

router = APIRouter(prefix="/api/v2", tags=["ğŸ“¡ API v2 - å‰ç«¯é€‚é…"])


def get_project_root() -> Path:
    """è·å–é¡¹ç›®æ ¹ç›®å½•"""
    return Path(__file__).parent.parent.parent


def get_articles_dir() -> Path:
    """è·å–æ–‡ç« ç›®å½•"""
    return get_project_root() / "ai" / "articles" / "summary"


def load_articles_with_cache(time_range: TimeFilter) -> List[ArticleModel]:
    """
    åŠ è½½æ–‡ç« ï¼ˆå¸¦ç¼“å­˜ï¼‰

    Args:
        time_range: æ—¶é—´èŒƒå›´

    Returns:
        æ–‡ç« åˆ—è¡¨
    """
    cache_key = f"articles_{time_range.value}"

    def fetch_articles() -> List[ArticleModel]:
        articles: List[ArticleModel] = []
        articles_dir = get_articles_dir()

        if time_range in [TimeFilter.week, TimeFilter.month]:
            days = 7 if time_range == TimeFilter.week else 30
            for i in range(days):
                check_date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
                date_dir = articles_dir / check_date

                if date_dir.exists():
                    for filepath in date_dir.glob("*.md"):
                        article = article_transformer.transform_from_file(filepath)
                        if article:
                            articles.append(article)
        else:
            target_date = (datetime.now() - timedelta(days=0)).strftime("%Y-%m-%d")
            date_dir = articles_dir / target_date

            if not date_dir.exists():
                target_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
                date_dir = articles_dir / target_date

            if date_dir.exists():
                for filepath in date_dir.glob("*.md"):
                    article = article_transformer.transform_from_file(filepath)
                    if article:
                        articles.append(article)

        return articles

    # ä½¿ç”¨ç¼“å­˜è·å–æ–‡ç« 
    return cache_manager.get_or_set(cache_key, fetch_articles, get_project_root())


def get_date_range(time_range: TimeFilter) -> tuple[str, str]:
    """
    æ ¹æ®æ—¶é—´èŒƒå›´è·å–æ—¥æœŸåŒºé—´

    Returns:
        (start_date, end_date)
    """
    today = datetime.now()

    if time_range == TimeFilter.today:
        return today.strftime("%Y-%m-%d"), today.strftime("%Y-%m-%d")
    elif time_range == TimeFilter.yesterday:
        yesterday = today - timedelta(days=1)
        return yesterday.strftime("%Y-%m-%d"), yesterday.strftime("%Y-%m-%d")
    elif time_range == TimeFilter.week:
        week_ago = today - timedelta(days=7)
        return week_ago.strftime("%Y-%m-%d"), today.strftime("%Y-%m-%d")
    elif time_range == TimeFilter.month:
        month_ago = today - timedelta(days=30)
        return month_ago.strftime("%Y-%m-%d"), today.strftime("%Y-%m-%d")


def filter_articles(
    articles: List[ArticleModel],
    keyword: Optional[str],
    sources: Optional[List[str]],
    tags: Optional[List[str]]
) -> List[ArticleModel]:
    """
    ç­›é€‰æ–‡ç« 

    Args:
        articles: æ–‡ç« åˆ—è¡¨
        keyword: å…³é”®è¯
        sources: æ¥æºåˆ—è¡¨
        tags: æ ‡ç­¾åˆ—è¡¨

    Returns:
        ç­›é€‰åçš„æ–‡ç« åˆ—è¡¨
    """
    filtered = articles

    # å…³é”®è¯ç­›é€‰
    if keyword:
        keyword_lower = keyword.lower()
        filtered = [
            article for article in filtered
            if keyword_lower in article.title.lower() or
               keyword_lower in article.summary.lower()
        ]

    # æ¥æºç­›é€‰
    if sources:
        sources_lower = [s.lower() for s in sources]
        filtered = [
            article for article in filtered
            if article.source.lower() in sources_lower
        ]

    # æ ‡ç­¾ç­›é€‰
    if tags:
        filtered = [
            article for article in filtered
            if any(tag.lower() in [t.lower() for t in article.tags] for tag in tags)
        ]

    return filtered


def sort_articles(
    articles: List[ArticleModel],
    sort_by: SortOption
) -> List[ArticleModel]:
    """
    æ’åºæ–‡ç« 

    Args:
        articles: æ–‡ç« åˆ—è¡¨
        sort_by: æ’åºæ–¹å¼

    Returns:
        æ’åºåçš„æ–‡ç« åˆ—è¡¨
    """
    if sort_by == SortOption.hot:
        return sorted(articles, key=lambda a: a.viewCount, reverse=True)
    elif sort_by == SortOption.newest:
        return sorted(articles, key=lambda a: a.publishedAt, reverse=True)
    elif sort_by == SortOption.comments:
        return sorted(articles, key=lambda a: a.commentCount, reverse=True)
    else:  # relevant - é»˜è®¤æŒ‰æµè§ˆæ•°
        return sorted(articles, key=lambda a: a.viewCount, reverse=True)


def paginate_articles(
    articles: List[ArticleModel],
    page: int,
    page_size: int
) -> ArticleListResponse:
    """
    åˆ†é¡µæ–‡ç« 

    Args:
        articles: æ–‡ç« åˆ—è¡¨
        page: é¡µç 
        page_size: æ¯é¡µæ•°é‡

    Returns:
        åˆ†é¡µå“åº”
    """
    total = len(articles)
    start = (page - 1) * page_size
    end = start + page_size
    paginated = articles[start:end]

    return ArticleListResponse(
        date=datetime.now().strftime("%Y-%m-%d"),
        timeRange="today",
        total=total,
        page=page,
        pageSize=page_size,
        articles=paginated
    )


# ==================== ç«¯ç‚¹ ====================

@router.get("/articles", response_model=ArticleListDataResponse)
async def get_articles_v2(
    keyword: Optional[str] = Query(None, description="å…³é”®è¯æœç´¢"),
    timeRange: TimeFilter = Query(TimeFilter.today, description="æ—¶é—´èŒƒå›´"),
    sources: Optional[List[str]] = Query(None, description="æ¥æºåˆ—è¡¨"),
    tags: Optional[List[str]] = Query(None, description="æ ‡ç­¾åˆ—è¡¨"),
    sortBy: SortOption = Query(SortOption.hot, description="æ’åºæ–¹å¼"),
    page: int = Query(1, ge=1, description="é¡µç "),
    pageSize: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
):
    """
    ğŸ“ è·å–æ–‡ç« åˆ—è¡¨ï¼ˆAPI v2ï¼‰

    æ”¯æŒå®Œæ•´çš„å‰ç«¯ç­›é€‰å’Œæ’åºåŠŸèƒ½ã€‚

    **å‚æ•°è¯´æ˜**:
    - `keyword`: å…³é”®è¯æœç´¢ï¼ŒåŒ¹é…æ ‡é¢˜å’Œæ‘˜è¦
    - `timeRange`: æ—¶é—´èŒƒå›´ï¼Œtoday/yesterday/week/month
    - `sources`: æ¥æºåˆ—è¡¨ï¼Œå¦‚ openai,google,anthropic
    - `tags`: æ ‡ç­¾åˆ—è¡¨ï¼Œå¦‚ LLM,GPT-4,AIç»˜ç”»
    - `sortBy`: æ’åºæ–¹å¼ï¼Œhot/newest/relevant/comments
    - `page`: é¡µç ï¼Œä» 1 å¼€å§‹
    - `pageSize`: æ¯é¡µæ•°é‡ï¼Œæœ€å¤§ 100

    **è¿”å›æ•°æ®**:
    - å®Œæ•´çš„ Article å¯¹è±¡ï¼ŒåŒ…å« id, category, tags, viewCount ç­‰
    """
    # æ”¶é›†æ–‡ç« æ–‡ä»¶
    articles: List[ArticleModel] = []
    articles_dir = get_articles_dir()

    if timeRange in [TimeFilter.week, TimeFilter.month]:
        # å¤šæ—¥æœŸèšåˆ
        days = 7 if timeRange == TimeFilter.week else 30
        for i in range(days):
            check_date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
            date_dir = articles_dir / check_date

            if date_dir.exists():
                for filepath in date_dir.glob("*.md"):
                    article = article_transformer.transform_from_file(filepath)
                    if article:
                        articles.append(article)
    else:
        # å•æ—¥æœŸ - ä»ä»Šå¤©å¼€å§‹å°è¯•ï¼Œæœ€å¤šå›æº¯ 7 å¤©
        found_data = False
        for days_back in range(7):
            target_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y-%m-%d")
            date_dir = articles_dir / target_date

            if date_dir.exists():
                for filepath in date_dir.glob("*.md"):
                    article = article_transformer.transform_from_file(filepath)
                    if article:
                        articles.append(article)
                found_data = True
                break

        if not found_data:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®ï¼Œå°è¯•åŠ è½½æ‰€æœ‰å¯ç”¨æ•°æ®
            for date_dir in sorted(articles_dir.iterdir(), reverse=True):
                if date_dir.is_dir():
                    for filepath in date_dir.glob("*.md"):
                        article = article_transformer.transform_from_file(filepath)
                        if article:
                            articles.append(article)

    # ç­›é€‰
    filtered_articles = filter_articles(articles, keyword, sources, tags)

    # æ’åº
    sorted_articles = sort_articles(filtered_articles, sortBy)

    # åˆ†é¡µ
    result = paginate_articles(sorted_articles, page, pageSize)
    result.timeRange = timeRange.value
    result.date = datetime.now().strftime("%Y-%m-%d")

    return ArticleListDataResponse(success=True, data=result)


@router.get("/suggestions", response_model=SuggestionsDataResponse)
async def get_suggestions(
    q: Optional[str] = Query(None, description="æŸ¥è¯¢è¯")
):
    """
    ğŸ” è·å–æœç´¢å»ºè®®

    è¿”å›çƒ­é—¨æœç´¢å’Œæœ€è¿‘æœç´¢çš„å»ºè®®ã€‚
    """
    # çƒ­é—¨æœç´¢ï¼ˆé¢„å®šä¹‰ï¼‰
    trending = [
        SearchSuggestion(text="GPT-4", icon="ğŸ¤–"),
        SearchSuggestion(text="Claude", icon="ğŸ§ "),
        SearchSuggestion(text="AIç»˜ç”»", icon="ğŸ¨"),
        SearchSuggestion(text="å¤šæ¨¡æ€æ¨¡å‹", icon="ğŸ‘ï¸"),
        SearchSuggestion(text="Agentå·¥ä½œæµ", icon="ğŸ¤"),
        SearchSuggestion(text="å¼€æºé¡¹ç›®", icon="ğŸ”“"),
    ]

    # æœ€è¿‘æœç´¢ï¼ˆæ¨¡æ‹Ÿï¼‰
    recent = [
        SearchSuggestion(text="Cursor IDE", icon="âŒ¨ï¸"),
        SearchSuggestion(text="Gemini Ultra", icon="ğŸ”"),
    ]

    # å¦‚æœæœ‰æŸ¥è¯¢è¯ï¼Œæœç´¢åŒ¹é…çš„æ ‡ç­¾
    if q:
        matching_tags = tag_extractor.search_tags(q, limit=5)
        if matching_tags:
            trending = [
                SearchSuggestion(text=tag['name'], icon=tag['emoji'])
                for tag in matching_tags
            ]

    return SuggestionsDataResponse(
        success=True,
        data=SuggestionsResponse(trending=trending, recent=recent)
    )


@router.get("/categories", response_model=CategoriesDataResponse)
async def get_categories():
    """
    ğŸ“‹ è·å–åˆ†ç±»åˆ—è¡¨

    è¿”å›æ‰€æœ‰å¯ç”¨çš„æ–‡ç« åˆ†ç±»ã€‚
    """
    categories = [
        CategoryInfo(
            id=ArticleCategory.hot,
            name="çƒ­é—¨",
            emoji="ğŸ”¥",
            description="é«˜çƒ­åº¦å†…å®¹"
        ),
        CategoryInfo(
            id=ArticleCategory.deep,
            name="æ·±åº¦",
            emoji="ğŸ“°",
            description="æ·±åº¦ç ”ç©¶å†…å®¹"
        ),
        CategoryInfo(
            id=ArticleCategory.new,
            name="æ–°å“",
            emoji="ğŸ†•",
            description="æœ€æ–°å‘å¸ƒå†…å®¹"
        ),
        CategoryInfo(
            id=ArticleCategory.breaking,
            name="çªå‘",
            emoji="âš¡",
            description="çªå‘æ–°é—»"
        ),
    ]

    return CategoriesDataResponse(success=True, data=categories)


@router.get("/sources", response_model=SourcesDataResponse)
async def get_sources():
    """
    ğŸ“‹ è·å–æ¥æºåˆ—è¡¨

    è¿”å›æ‰€æœ‰æ•°æ®æ¥æºåŠå…¶æ–‡ç« æ•°é‡ã€‚
    """
    # æ”¶é›†å®é™…æ¥æº
    sources = {}
    articles_dir = get_articles_dir()

    # æ£€æŸ¥æ‰€æœ‰å¯ç”¨çš„æ•°æ®ç›®å½•
    for date_dir in sorted(articles_dir.iterdir(), reverse=True):
        if date_dir.is_dir():
            for filepath in date_dir.glob("*.md"):
                article = article_transformer.transform_from_file(filepath)
                if article:
                    source = article.source
                    if source not in sources:
                        sources[source] = {
                            'id': source,
                            'name': source.replace('-', ' ').title(),
                            'count': 0
                        }
                    sources[source]['count'] += 1

    # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
    sources_list = [
        SourceInfo(**info)
        for info in sorted(sources.values(), key=lambda x: -x['count'])
    ]

    return SourcesDataResponse(success=True, data=sources_list)


@router.get("/stats", response_model=StatsDataResponse)
async def get_stats():
    """
    ğŸ“Š è·å–ç»Ÿè®¡ä¿¡æ¯

    è¿”å›ä»Šæ—¥å’Œæ€»è®¡çš„ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    today = datetime.now().strftime("%Y-%m-%d")

    # æ”¶é›†æ–‡ç« æ•°æ®
    total_articles = 0
    today_articles = 0
    total_sources = set()

    articles_dir = get_articles_dir()

    # æ£€æŸ¥æ‰€æœ‰å¯ç”¨çš„æ•°æ®ç›®å½•
    for date_dir in sorted(articles_dir.iterdir(), reverse=True):
        if date_dir.is_dir():
            for filepath in date_dir.glob("*.md"):
                article = article_transformer.transform_from_file(filepath)
                if article:
                    total_articles += 1
                    total_sources.add(article.source)

                    if article.publishedAt.startswith(today):
                        today_articles += 1

    # ç”Ÿæˆç»Ÿè®¡æ•°æ®
    today_stats = StatsInfo(
        date=today,
        articles=today_articles,
        views=today_articles * random.randint(100, 300),
        comments=today_articles * random.randint(5, 20)
    )

    total_stats = StatsInfo(
        date=today,
        articles=total_articles,
        views=total_articles * random.randint(150, 400),
        comments=total_articles * random.randint(10, 50)
    )

    return StatsDataResponse(
        success=True,
        data=StatsResponse(today=today_stats, total=total_stats)
    )


@router.get("/health")
async def health_check():
    """
    â¤ï¸ å¥åº·æ£€æŸ¥
    """
    return {
        "status": "ok",
        "service": "ai-daily-collector-api-v2",
        "version": "2.0.0"
    }
