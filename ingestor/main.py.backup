#!/usr/bin/env python3
"""Ingestion pipeline entrypoint supporting all source types."""
from __future__ import annotations

import argparse
import logging
from typing import List, Dict, Any

try:
    import yaml
except Exception:
    yaml = None

# Import all scrapers
from ingestor.scrapers.rss_scraper import fetch_rss
from ingestor.scrapers.newsnow_scraper import fetch_newsnow
from ingestor.scrapers.hackernews_scraper import fetch_hackernews
from ingestor.scrapers.devto_scraper import fetch_devto
from ingestor.scrapers.v2ex_scraper import fetch_v2ex
from ingestor.scrapers.reddit_scraper import fetch_reddit
from ingestor.scrapers.arxiv_scraper import fetch_arxiv
from ingestor.transformers.article_transformer import transform
from ingestor.storage.db import LocalDBAdapter
from shared.models import ArticleModel


def _load_config(path: str) -> dict:
    if yaml is None:
        logging.warning("PyYAML not installed; using minimal inline config.")
        return {}
    try:
        with open(path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f) or {}
    except FileNotFoundError:
        logging.warning("Config file not found at %s; using empty config.", path)
        return {}


def _fetch_from_source(src: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Fetch articles from a single source based on its type."""
    source_type = src.get("type", "rss")
    
    if source_type == "rss":
        url = src.get("url")
        if url:
            return fetch_rss(url)
    
    elif source_type == "newsnow":
        platform_id = src.get("platform_id", "toutiao")
        keyword = src.get("filters", {}).get("keyword", "")
        hours = src.get("filters", {}).get("hours", 24)
        max_articles = src.get("filters", {}).get("max_articles", 20)
        return fetch_newsnow(platform_id, keyword, hours, max_articles)
    
    elif source_type == "hackernews":
        keyword = src.get("filters", {}).get("keyword", "")
        hours = src.get("filters", {}).get("hours", 24)
        max_articles = src.get("filters", {}).get("max_articles", 30)
        return fetch_hackernews(keyword, hours, max_articles)
    
    elif source_type == "devto":
        max_articles = src.get("filters", {}).get("max_articles", 15)
        return fetch_devto("AI", max_articles)
    
    elif source_type == "v2ex":
        keyword = src.get("filters", {}).get("keyword", "")
        max_articles = src.get("filters", {}).get("max_articles", 20)
        return fetch_v2ex(keyword, max_articles)
    
    elif source_type == "reddit":
        subreddit = src.get("subreddit", "MachineLearning")
        keyword = src.get("filters", {}).get("keyword", "")
        max_articles = src.get("filters", {}).get("max_articles", 15)
        return fetch_reddit(subreddit, keyword, max_articles)
    
    elif source_type == "arxiv":
        max_articles = src.get("filters", {}).get("max_articles", 15)
        return fetch_arxiv("cat:cs.AI", max_articles)
    
    elif source_type in ("ai_blogs", "tech_media", "podcast"):
        # These are typically RSS-based
        url = src.get("url")
        if url:
            return fetch_rss(url)
    
    elif source_type == "producthunt":
        # Product Hunt RSS
        url = src.get("url")
        if url:
            return fetch_rss(url)
    
    elif source_type == "youtube":
        # YouTube RSS feeds are available via: https://www.youtube.com/feeds/videos.xml?channel_id=XXX
        channel_id = src.get("channel_id")
        if channel_id:
            url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
            return fetch_rss(url)
    
    return []


def main() -> int:
    parser = argparse.ArgumentParser(prog="ingest", description="Ingestion pipeline supporting all source types")
    parser.add_argument("--config", type=str, default="config/sources.yaml", help="Path to config YAML")
    parser.add_argument("--dry-run", action="store_true", help="Run without persisting data")
    parser.add_argument("--source-type", type=str, help="Only process specific source type")
    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s"
    )
    logging.info("Ingestion started with config: %s", args.config)

    config = _load_config(args.config)
    sources = []
    if isinstance(config, dict):
        sources = config.get("sources") or []
    
    if not sources:
        logging.warning("No sources configured")
        return 0

    # Filter by source type if specified
    if args.source_type:
        sources = [s for s in sources if s.get("type") == args.source_type]
        logging.info("Filtered to %d sources of type: %s", len(sources), args.source_type)

    storage = LocalDBAdapter(connection_string=None)
    articles_written = 0
    source_stats: Dict[str, int] = {}

    for src in sources:
        # Skip disabled sources
        if not src.get("enabled", True):
            continue
        
        source_name = src.get("name", "unknown")
        source_type = src.get("type", "unknown")
        
        try:
            logging.info("Fetching from: %s (type: %s)", source_name, source_type)
            items = _fetch_from_source(src)
            
            count = 0
            for it in items:
                article = transform(it)
                article_obj = article if isinstance(article, ArticleModel) else ArticleModel(**article)  # type: ignore[call-arg]
                if not args.dry_run:
                    storage.upsert_article(article_obj)
                    articles_written += 1
                    count += 1
            
            source_stats[source_name] = count
            logging.info("  -> Fetched %d articles from %s", count, source_name)
            
        except Exception as e:
            logging.error("Error fetching from %s: %s", source_name, e)
            continue

    logging.info("=" * 50)
    logging.info("Ingestion completed!")
    logging.info("Total articles written: %d", articles_written)
    for name, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
        logging.info("  - %s: %d articles", name, count)
    
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
