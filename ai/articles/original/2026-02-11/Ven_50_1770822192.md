---
title: "Nvidia releases DreamDojo, a robot âworld modelâ trained on 44,000 hours of human video"
url: "https://venturebeat.com/technology/nvidia-releases-dreamdojo-a-robot-world-model-trained-on-44-000-hours-of"
source: "VentureBeat"
date: 2026-02-11
score: 50
---

# Nvidia releases DreamDojo, a robot âworld modelâ trained on 44,000 hours of human video

**来源**: [VentureBeat](https://venturebeat.com/technology/nvidia-releases-dreamdojo-a-robot-world-model-trained-on-44-000-hours-of) | **热度**: 50

## 原文内容

Title: Nvidia releases DreamDojo, a robot ‘world model’ trained on 44,000 hours of human video

URL Source: http://venturebeat.com/technology/nvidia-releases-dreamdojo-a-robot-world-model-trained-on-44-000-hours-of

Published Time: 2026-02-09T01:30-08:00

Markdown Content:
Nvidia releases DreamDojo, a robot ‘world model’ trained on 44,000 hours of human video | VentureBeat
===============

[](http://venturebeat.com/)

*   [Orchestration](http://venturebeat.com/category/orchestration)
*   [Infrastructure](http://venturebeat.com/category/infrastructure)
*   [Data](http://venturebeat.com/category/data)
*   [Security](http://venturebeat.com/category/security)

More

[Newsletters](http://venturebeat.com/newsletters)

[All Posts](http://venturebeat.com/)
Nvidia releases DreamDojo, a robot ‘world model’ trained on 44,000 hours of human video
=======================================================================================

[Michael Nuñez](http://venturebeat.com/author/michael_nunez) February 9, 2026 

![Image 1: nuneybits Vector art of robot manipulating cardboard box plasti f32a598d-057f-46c5-90dc-8522518bd4ac](http://venturebeat.com/_next/image?url=https%3A%2F%2Fimages.ctfassets.net%2Fjdtwqhzvc2n1%2F6VkLsjhtrqIPRjwrtn2XF0%2F62a42d08ef8c5a9fc5ac31fa6a3f6c2b%2Fnuneybits_Vector_art_of_robot_manipulating_cardboard_box_plasti_f32a598d-057f-46c5-90dc-8522518bd4ac.webp%3Fw%3D1000%26q%3D100&w=3840&q=85)

Credit: VentureBeat made with Midjourney

[](https://www.google.com/preferences/source?q=venturebeat.com "Add to Google Preferred Source")

A team of researchers led by [Nvidia](https://www.nvidia.com/en-us/) has released [DreamDojo](https://dreamdojo-world.github.io/), a new AI system designed to teach robots how to interact with the physical world by watching tens of thousands of hours of human video — a development that could significantly reduce the time and cost required to train the next generation of humanoid machines.

The [research](https://arxiv.org/abs/2602.06949), published this month and involving collaborators from [UC Berkeley](https://www.berkeley.edu/), [Stanford](https://www.stanford.edu/), the [University of Texas at Austin](https://www.utexas.edu/), and several other institutions, introduces what the team calls "the first robot world model of its kind that demonstrates strong generalization to diverse objects and environments after post-training."

At the core of [DreamDojo](https://dreamdojo-world.github.io/) is what the researchers describe as "a large-scale video dataset" comprising "44k hours of diverse human egocentric videos, the largest dataset to date for world model pretraining." The dataset, called [DreamDojo-HV](https://dreamdojo-world.github.io/), is a dramatic leap in scale — "15x longer duration, 96x more skills, and 2,000x more scenes than the previously largest dataset for world model training," according to the project documentation.

![Image 2: Nvidia - Workshop and box packing image](http://venturebeat.com/_next/image?url=https%3A%2F%2Fimages.ctfassets.net%2Fjdtwqhzvc2n1%2F1OqVMyeQ9td0MJSho8IQHM%2F0269fac024a5073bb2a60c5fbc10b8ea%2FScreenshot_2026-02-09_at_1.06.53%C3%A2__PM.png%3Fw%3D1000%26q%3D100&w=3840&q=75)

A simulated robot places a cup into a cardboard box in a workshop setting, one of thousands of scenarios DreamDojo can model after training on 44,000 hours of human video. (Credit: Nvidia)

**Inside the two-phase training system that teaches robots to see like humans**
-------------------------------------------------------------------------------

The system operates in two distinct phases. First, [DreamDojo](https://dreamdojo-world.github.io/) "acquires comprehensive physical knowledge from large-scale human datasets by pre-training with latent actions." Then it undergoes "post-training on the target embodiment with continuous robot actions" — essentially learning general physics from watching humans, then fine-tuning that knowledge for specific robot hardware.

For enterprises considering humanoid robots, this approach addresses a stubborn bottleneck. Teaching a robot to manipulate objects in unstructured environments traditionally requires massive amounts of robot-specific demonstration data — expensive and time-consuming to collect. [DreamDojo](https://dreamdojo-world.github.io/) sidesteps this problem by leveraging existing human video, allowing robots to learn from observation before ever touching a physical object.

One of the technical breakthroughs is speed. Through a distillation process, the researchers achieved "real-time interactions at 10 FPS for over 1 minute" — a capability that enables practical applications like live teleoperation and on-the-fly planning. The team demonstrated the system working across multiple robot platforms, including the [GR-1](https://www.fftai.com/products-gr1), [G1](https://www.unitree.com/g1/), [AgiBot](https://www.agibot.com/), and [YAM](https://i2rt.com/collections/yam-arm) humanoid robots, showing what they call "realisti

---
*自动采集于 2026-02-11 23:03:17 (北京时间)*
