---
title: "Is a secure AI assistant possible?"
url: "https://www.technologyreview.com/2026/02/11/1132768/is-a-secure-ai-assistant-possible/"
source: "MIT Tech Review"
date: 2026-02-12
score: 50
---

# Is a secure AI assistant possible?

**来源**: [MIT Tech Review](https://www.technologyreview.com/2026/02/11/1132768/is-a-secure-ai-assistant-possible/) | **热度**: 50

## 原文内容

Title: Is a secure AI assistant possible?

URL Source: http://www.technologyreview.com/2026/02/11/1132768/is-a-secure-ai-assistant-possible/

Published Time: 2026-02-11T15:08:35-05:00

Markdown Content:
AI agents are a risky business. Even when stuck inside the chatbox window, LLMs will make mistakes and behave badly. Once they have tools that they can use to interact with the outside world, such as web browsers and email addresses, the consequences of those mistakes become far more serious.

That might explain why the first breakthrough LLM personal assistant came not from one of the major AI labs, which have to worry about reputation and liability, but from an independent software engineer, Peter Steinberger. In November of 2025, Steinberger uploaded his tool, now called OpenClaw, to GitHub, and in late January the project went viral.

OpenClaw harnesses existing LLMs to let users create their own bespoke assistants. For some users, this means handing over reams of personal data, from years of emails to the contents of their hard drive. That has security experts thoroughly freaked out. The risks posed by OpenClaw are so extensive that it would probably take someone the better part of a week to [read](https://www.crowdstrike.com/en-us/blog/what-security-teams-need-to-know-about-openclaw-ai-super-agent/)[all](https://www.bitsight.com/blog/openclaw-ai-security-risks-exposed-instances)[of](https://blogs.cisco.com/ai/personal-ai-agents-like-openclaw-are-a-security-nightmare)[the](https://1password.com/blog/from-magic-to-malware-how-openclaws-agent-skills-become-an-attack-surface)[security](https://www.security.com/expert-perspectives/rise-openclaw)[blog](https://www.trendmicro.com/en_us/research/26/b/what-openclaw-reveals-about-agentic-assistants.html)[posts](https://www.paloaltonetworks.com/blog/network-security/why-moltbot-may-signal-ai-crisis/) on it that have cropped up in the past few weeks. The Chinese government took the step of [issuing a public warning](https://www.reuters.com/world/china/china-warns-security-risks-linked-openclaw-open-source-ai-agent-2026-02-05/) about OpenClaw’s security vulnerabilities.

In response to these concerns, [Steinberger posted on X](https://x.com/steipete/status/2015954706883846540) that nontechnical people should not use the software. (He did not respond to a request for comment for this article.) But there’s a clear appetite for what OpenClaw is offering, and it’s not limited to people who can run their own software security audits. Any AI companies that hope to get in on the personal assistant business will need to figure out how to build a system that will keep users’ data safe and secure. To do so, they’ll need to borrow approaches from the cutting edge of agent security research.

### **Risk management**

OpenClaw is, in essence, a mecha suit for LLMs. Users can choose any LLM they like to act as the pilot; that LLM then gains access to improved memory capabilities and the ability to set itself tasks that it repeats on a regular cadence. Unlike the agentic offerings from the major AI companies, OpenClaw agents are meant to be on 24-7, and users can communicate with them using WhatsApp or other messaging apps. That means they can act like a superpowered personal assistant who wakes you each morning with a personalized to-do list, plans vacations while you work, and spins up new apps in its spare time.

But all that power has consequences. If you want your AI personal assistant to manage your inbox, then you need to give it access to your email—and all the sensitive information contained there. If you want it to make purchases on your behalf, you need to give it your credit card info. And if you want it to do tasks on your computer, such as writing code, it needs some access to your local files.

There are a few ways this can go wrong. The first is that the AI assistant might make a mistake, as when a user’s Google Antigravity coding agent reportedly [wiped his entire hard drive](https://www.tomshardware.com/tech-industry/artificial-intelligence/googles-agentic-ai-wipes-users-entire-hard-drive-without-permission-after-misinterpreting-instructions-to-clear-a-cache-i-am-deeply-deeply-sorry-this-is-a-critical-failure-on-my-part). The second is that someone might gain access to the agent using conventional hacking tools and use it to either extract sensitive data or run malicious code. In the weeks since OpenClaw went viral, security researchers have demonstrated [numerous](https://www.404media.co/silicon-valleys-favorite-new-ai-agent-has-serious-security-flaws/)[such](https://www.theregister.com/2026/02/09/openclaw_instances_exposed_vibe_code/)[vulnerabilities](https://www.theverge.com/news/874011/openclaw-ai-skill-clawhub-extensions-security-nightmare) that put security-naïve users at risk.

Both of these dangers can be managed: Some users are choosing to run their OpenClaw agents on separate computers or in the cloud, which protects data on their hard drives from bein

---
*自动采集于 2026-02-12 15:11:00 (北京时间)*
