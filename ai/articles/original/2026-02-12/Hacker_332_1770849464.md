---
title: "GLM-5: From Vibe Coding to Agentic Engineering"
url: "https://z.ai/blog/glm-5"
source: "Hacker News"
date: 2026-02-12
score: 332
---

# GLM-5: From Vibe Coding to Agentic Engineering

**来源**: [Hacker News](https://z.ai/blog/glm-5) | **热度**: 332

## 原文内容

Title: GLM-5: From Vibe Coding to Agentic Engineering

URL Source: http://z.ai/blog/glm-5

Published Time: Wed, 11 Feb 2026 17:23:53 GMT

Markdown Content:
We are launching GLM-5, targeting complex systems engineering and long-horizon agentic tasks. Scaling is still one of the most important ways to improve the intelligence efficiency of Artificial General Intelligence (AGI). Compared to GLM-4.5, GLM-5 scales from 355B parameters (32B active) to 744B parameters (40B active), and increases pre-training data from 23T to 28.5T tokens. GLM-5 also integrates DeepSeek Sparse Attention (DSA), significantly reducing deployment cost while preserving long-context capacity.

Reinforcement learning aims to bridge the gap between competence and excellence in pre-trained models. However, deploying it at scale for LLMs is a challenge due to RL training inefficiency. To this end, we developed [slime](https://github.com/THUDM/slime), a novel **asynchronous RL infrastructure** that substantially improves training throughput and efficiency, enabling more fine-grained post-training iterations. With advances in both pre-training and post-training, GLM-5 delivers significant improvement compared to GLM-4.7 across a wide range of academic benchmarks and achieves best-in-class performance among all open-source models in the world on reasoning, coding, and agentic tasks, closing the gap with frontier models.

![Image 1](https://z-cdn-media.chatglm.cn/prompts-rich-media-resources/5-blog/20260212-010724.png)

GLM-5 is designed for complex systems engineering and long-horizon agentic tasks. On our internal evaluation suite CC-Bench-V2, GLM-5 significantly outperforms GLM-4.7 across frontend, backend, and long-horizon tasks, narrowing the gap to Claude Opus 4.5.

![Image 2](https://z-cdn-media.chatglm.cn/prompts-rich-media-resources/5-blog/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_2026-02-11_232804_259.png)

On [Vending Bench 2](https://andonlabs.com/evals/vending-bench-2), a benchmark that measures long-term operational capability, GLM-5 ranks #1 among open-source models. Vending Bench 2 requires the model to run a simulated vending machine business over a one-year horizon; GLM-5 finishes with a final account balance of $4,432, approaching Claude Opus 4.5 and demonstrating strong long-term planning and resource management.

![Image 3](https://z-cdn-media.chatglm.cn/prompts-rich-media-resources/5-blog/bf5c97ae6ba5f07ba980ed9bcc116f47.PNG)

GLM-5 is open-sourced on [Hugging Face](https://huggingface.co/zai-org/GLM-5) and [ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-5), with model weights released under the MIT License. GLM-5 is also available on developer platform [api.z.ai](https://api.z.ai/) and [BigModel.cn](https://bigmodel.cn/), with compatibility with Claude Code and OpenClaw. You can also try it for free on [Z.ai](https://chat.z.ai/).

| Benchmark | GLM-5 (Thinking) | GLM-4.7 (Thinking) | DeepSeek-V3.2 (Thinking) | Kimi K2.5 (Thinking) | Claude Opus 4.5 (Extend Thinking) | Gemini 3.0 Pro (High Thinking Level) | GPT-5.2 (xhigh) |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Reasoning |
| Humanity's Last Exam | 30.5 | 24.8 | 25.1 | 31.5 | 28.4 | 37.2 | 35.4 |
| Humanity's Last Exam w/ Tools | 50.4 | 42.8 | 40.8 | 51.8 | 43.4* | 45.8* | 45.5* |
| AIME 2026 I | 92.7 | 92.9 | 92.7 | 92.5 | 93.3 | 90.6 | - |
| HMMT Nov. 2025 | 96.9 | 93.5 | 90.2 | 91.1 | 91.7 | 93.0 | 97.1 |
| IMOAnswerBench | 82.5 | 82.0 | 78.3 | 81.8 | 78.5 | 83.3 | 86.3 |
| GPQA-Diamond | 86.0 | 85.7 | 82.4 | 87.6 | 87.0 | 91.9 | 92.4 |
| Coding |
| SWE-bench Verified | 77.8 | 73.8 | 73.1 | 76.8 | 80.9 | 76.2 | 80.0 |
| SWE-bench Multilingual | 73.3 | 66.7 | 70.2 | 73.0 | 77.5 | 65.0 | 72.0 |
| Terminal-Bench 2.0 Terminus-2 | 56.2 / 60.7† | 41.0 | 39.3 | 50.8 | 59.3 | 54.2 | 54.0 |
| Terminal-Bench 2.0 Claude Code | 56.2 / 61.1† | 32.8 | 46.4 | - | 57.9 | - | - |
| CyberGym | 43.2 | 23.5 | 17.3 | 41.3 | 50.6 | 39.9 | - |
| General Agent |
| BrowseComp | 62.0 | 52.0 | 51.4 | 60.6 | 37.0 | 37.8 | - |
| BrowseComp w/ Context Manage | 75.9 | 67.5 | 67.6 | 74.9 | 67.8 | 59.2 | 65.8 |
| BrowseComp-Zh | 72.7 | 66.6 | 65.0 | 62.3 | 62.4 | 66.8 | 76.1 |
| τ²-Bench | 89.7 | 87.4 | 85.3 | 80.2 | 91.6 | 90.7 | 85.5 |
| MCP-Atlas Public Set | 67.8 | 52.0 | 62.2 | 63.8 | 65.2 | 66.6 | 68.0 |
| Tool-Decathlon | 38.0 | 23.8 | 35.2 | 27.8 | 43.5 | 36.4 | 46.3 |
| Vending Bench 2 | $4,432.12 | $2,376.82 | $1,034.00 | $1,198.46 | $4,967.06 | $5,478.16 | $3,591.33 |

> *: refers to their scores of full set.
> 
> 
> †: A [verified version](http://z.ai/blog/glm-5#footnote-terminal-bench) of Terminal-Bench 2.0 that fixes some ambiguous instructions.
> 
> 
> See footnote for more evaluation details.

Office
------

Foundation models are moving from “chat” to “work,” much like Office tools for knowledge workers and programming tools for engineers.

GLM-4.5 is our first step for reasoning, coding, and agent, enabling the model to complete complex tasks. With GLM-5, we further enha

---
*自动采集于 2026-02-12 06:37:45 (北京时间)*
