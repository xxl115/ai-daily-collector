---
title: "Anthropic published the prompt injection failure rates that enterprise security teams have been asking every vendor for"
url: "https://venturebeat.com/security/prompt-injection-measurable-security-metric-one-ai-developer-publishes-numbers"
source: "VentureBeat"
date: 2026-02-12
score: 50
---

# Anthropic published the prompt injection failure rates that enterprise security teams have been asking every vendor for

**来源**: [VentureBeat](https://venturebeat.com/security/prompt-injection-measurable-security-metric-one-ai-developer-publishes-numbers) | **热度**: 50

## 原文内容

Title: Anthropic published the prompt injection failure rates that enterprise security teams have been asking every vendor for

URL Source: http://venturebeat.com/security/prompt-injection-measurable-security-metric-one-ai-developer-publishes-numbers

Published Time: 2026-02-10T10:00-08:00

Markdown Content:
Run a prompt injection attack against [Claude Opus 4.6](https://venturebeat.com/technology/anthropics-claude-opus-4-6-brings-1m-token-context-and-agent-teams-to-take) in a constrained coding environment, and it fails every time, 0% success rate across 200 attempts, no safeguards needed. Move that same attack to a GUI-based system with extended thinking enabled, and the picture changes fast. A single attempt gets through 17.8% of the time without safeguards. By the 200th attempt, the breach rate hits 78.6% without safeguards and 57.1% with them.

The latest models’ [212-page system card](https://www-cdn.anthropic.com/0dd865075ad3132672ee0ab40b05a53f14cf5288.pdf), released February 5, breaks out attack success rates by surface, by attempt count, and by safeguard configuration.

Why surface-level differences determine enterprise risk
-------------------------------------------------------

For years, prompt injection was a known risk that no one quantified. Security teams treated it as theoretical. AI developers treated it as a research problem. That changed when Anthropic made prompt injection measurable across four distinct agent surfaces, with attack success rates that security leaders can finally build procurement decisions around.

[OpenAI's GPT-5.2](https://venturebeat.com/ai/openais-gpt-5-2-is-here-what-enterprises-need-to-know) system card includes prompt injection benchmark results, including scores on evaluations like Agent JSK and PlugInject, but does not break out attack success rates by agent surface or show how those rates change across repeated attempts. The original GPT-5 system card described more than 5,000 hours of red teaming from over 400 external testers. The [Gemini 3 model card](https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf) describes it as "our most secure model yet" with "increased resistance to prompt injections," sharing relative safety improvements versus previous models but not publishing absolute attack success rates by surface or persistence scaling data.

What each developer discloses and what they withhold
----------------------------------------------------

**Disclosure Category****Anthropic (Opus 4.6)****OpenAI (GPT-5.2)****Google (Gemini 3)**
**Per-surface attack success rates**Published (0% to 78.6%)Benchmark scores only Relative improvements only
**Attack persistence scaling**Published (1 to 200 attempts)Not published Not published
**Safeguard on/off comparison**Published Not published Not published
**Agent monitoring evasion data**Published (SHADE-Arena)Not published Not published
**Zero-day discovery counts**500+ with projects named Not published Not published
**Third-party red teaming**Gray Swan, UK AISI, Apollo 400+ external testers UK AISI, Apollo, Vaultis, Dreadnode

Third-party testing highlights why granular vendor disclosures matter. Promptfoo's independent red team evaluation of GPT-5.2 found [jailbreak success rates climbing](https://www.promptfoo.dev/blog/gpt-5.2-trust-safety-assessment/) from a 4.3% baseline to 78.5% in multi-turn scenarios, the kind of persistence-scaled data that reveals how defenses degrade under sustained attack. OpenAI's own system card does not include equivalent metrics.

When the agent evades its own maker's monitor
---------------------------------------------

Buried in the alignment assessment is a finding that should reshape how enterprises think about agent governance. On [SHADE-Arena](https://www.anthropic.com/research/shade-arena-sabotage-monitoring), a benchmark designed to test whether models can complete suspicious tasks without triggering automated oversight, Opus 4.6 succeeded 18% of the time when extended thinking was enabled. The monitor was calibrated to Anthropic's own internal detection threshold.

The system card states the model has "an improved ability to complete suspicious side tasks without attracting the attention of automated monitors." The agent built by Anthropic evades the monitor built by Anthropic.

Anthropic’s separately published [Sabotage Risk Report](https://www-cdn.anthropic.com/f21d93f21602ead5cdbecb8c8e1c765759d9e232.pdf) uses this same 18% figure to argue the opposite conclusion: that a model with genuine hidden goals would be “overwhelmingly likely to be caught” at current capability levels. But the report also acknowledges that the number “represents an increase relative to recent models.” For enterprise security teams, the trend line matters more than the current number. A capability that improves with each model generation is not a capability you can safely ignore in your risk model.

For security teams, the takeaway is architectural. Constrain what

---
*自动采集于 2026-02-12 06:37:41 (北京时间)*
