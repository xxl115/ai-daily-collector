---
title: "MIT's new fine-tuning method lets LLMs learn new skills without losing old ones"
url: "https://venturebeat.com/orchestration/mits-new-fine-tuning-method-lets-llms-learn-new-skills-without-losing-old"
source: "VentureBeat"
date: 2026-02-12
score: 50
---

# MIT's new fine-tuning method lets LLMs learn new skills without losing old ones

**来源**: [VentureBeat](https://venturebeat.com/orchestration/mits-new-fine-tuning-method-lets-llms-learn-new-skills-without-losing-old) | **热度**: 50

## 原文内容

Title: MIT's new fine-tuning method lets LLMs learn new skills without losing old ones

URL Source: http://venturebeat.com/orchestration/mits-new-fine-tuning-method-lets-llms-learn-new-skills-without-losing-old

Published Time: 2026-02-11T12:00-08:00

Markdown Content:
When enterprises fine-tune LLMs for new tasks, they risk breaking everything the models already know. This forces companies to maintain separate models for every skill.

Researchers at MIT, the Improbable AI Lab and ETH Zurich have developed a new technique that enables large language models to learn new skills and knowledge without forgetting their past capabilities.

Their technique, called [self-distillation fine-tuning](https://arxiv.org/abs/2601.19897) (SDFT), allows models to learn directly from demonstrations and their own experiments by leveraging the inherent in-context learning abilities of modern LLMs. Experiments show that SDFT consistently outperforms traditional supervised fine-tuning (SFT) while addressing the limitations of reinforcement learning algorithms.

For enterprise applications, the method enables a single model to accumulate multiple skills over time without suffering from performance regression on earlier tasks. This offers a potential pathway for building AI agents that can adapt to dynamic business environments, gathering new proprietary knowledge and skills as needed without requiring expensive retraining cycles or losing their general reasoning abilities.

The challenge of continual learning
-----------------------------------

Once an LLM is trained and deployed, it remains static. It does not update its parameters to acquire new skills, internalize new knowledge, or improve from experience. To build truly adaptive AI, the industry needs to solve "[continual learning](https://venturebeat.com/technology/four-ai-research-trends-enterprise-teams-should-watch-in-2026)," allowing systems to accumulate knowledge much like humans do throughout their careers.

The most effective way for models to learn is through "on-policy learning.” In this approach, the model learns from data it generates itself allowing it to correct its own errors and reasoning processes. This stands in contrast to learning by simply mimicking static datasets. Without on-policy learning, models are prone to "[catastrophic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference)," a phenomenon where learning a new task causes the model to lose its past knowledge and ability to perform previous tasks.

However, on-policy learning typically requires [reinforcement learning](https://venturebeat.com/orchestration/why-reinforcement-learning-plateaus-without-representation-depth-and-other) (RL), which depends on an explicit reward function to score the model's outputs. This works well for problems with clear outcomes, such as math and coding. But in many real-world enterprise scenarios (e.g., writing a legal brief or summarizing a meeting), defining a mathematical reward function is difficult or impossible.

RL methods also often fail when trying to teach a model entirely new information, such as a specific company protocol or a new product line. As Idan Shenfeld, a doctorate student at MIT and co-author of the paper, told VentureBeat, "No matter how many times the base model tries, it cannot generate correct answers for a topic it has zero knowledge about," meaning it never gets a positive signal to learn from.

The standard alternative is supervised fine-tuning (SFT), where the model is trained on a fixed dataset of expert demonstrations. While SFT provides clear ground truth, it is inherently "off-policy." Because the model is just mimicking data rather than learning from its own attempts, it often fails to generalize to out-of-distribution examples and suffers heavily from catastrophic forgetting.

SDFT seeks to bridge this gap: enabling the benefits of on-policy learning using only prerecorded demonstrations, without needing a reward function.

How SDFT works
--------------

SDFT solves this problem by using "distillation," a process where a student model learns to mimic a teacher. The researchers’ insight was to use the model's own "in-context learning" (ICL) capabilities to create a feedback loop within a single model.

In-context learning is the phenomenon where you provide the LLM with a difficult task and one or more demonstrations of how similar problems are solved. Most advanced LLMs are designed to solve new problems with ICL examples, without any parameter updates.

![Image 1: Self-distillation fine-tuning](https://venturebeat.com/_next/image?url=https%3A%2F%2Fimages.ctfassets.net%2Fjdtwqhzvc2n1%2FckgUJB2KfI2mjeMI7dM6K%2F0d7f317a1bd79c2fc24b150cdc84879d%2FGemini_Generated_Image_mepu1jmepu1jmepu.png%3Fw%3D1000%26q%3D100&w=3840&q=75)
During the training cycle, SDFT employs the model in two roles.

**The teacher:** A frozen version of the model is fed the query along with expert demonstrations. Using ICL, the teacher deduces the co

---
*自动采集于 2026-02-12 06:37:35 (北京时间)*
