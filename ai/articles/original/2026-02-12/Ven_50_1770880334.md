---
title: "z.ai's open source GLM-5 achieves record low hallucination rate and leverages new RL 'slime' technique"
url: "https://venturebeat.com/technology/z-ais-open-source-glm-5-achieves-record-low-hallucination-rate-and-leverages"
source: "VentureBeat"
date: 2026-02-12
score: 50
---

# z.ai's open source GLM-5 achieves record low hallucination rate and leverages new RL 'slime' technique

**来源**: [VentureBeat](https://venturebeat.com/technology/z-ais-open-source-glm-5-achieves-record-low-hallucination-rate-and-leverages) | **热度**: 50

## 原文内容

Title: z.ai's open source GLM-5 achieves record low hallucination rate and leverages new RL 'slime' technique

URL Source: http://venturebeat.com/technology/z-ais-open-source-glm-5-achieves-record-low-hallucination-rate-and-leverages

Published Time: 2026-02-11T19:09-05:00

Markdown Content:
Chinese AI startup Zhupai aka z.ai is back this week with an eye-popping new frontier large language model: [GLM-5](https://x.com/Zai_org/status/2021638634739527773).

The latest in z.ai's ongoing and continually impressive GLM series, it retains an open source MIT License — perfect for enterprise deployment – and, in one of several notable achievements, achieves a record-low hallucination rate on the independent [Artificial Analysis Intelligence Index v4.0](https://x.com/ArtificialAnlys/status/2021678229418066004).

With a score of -1 on the AA-Omniscience Index—representing a massive 35-point improvement over its predecessor—GLM-5 now leads the entire AI industry, including U.S. competitors like Google, OpenAI and Anthropic, in knowledge reliability by knowing when to abstain rather than fabricate information.

![Image 1: Screenshot 2026-02-11 at 5.09.50 PM](https://venturebeat.com/_next/image?url=https%3A%2F%2Fimages.ctfassets.net%2Fjdtwqhzvc2n1%2FjS7cLuviORC6DeVjLhSOj%2Fe0d7f0fc544e285bcb82e1ea709afe2f%2FScreenshot_2026-02-11_at_5.09.50%C3%A2__PM.png%3Fw%3D1000%26q%3D100&w=3840&q=75)
Beyond its reasoning prowess, GLM-5 is built for high-utility knowledge work. It features native "Agent Mode" capabilities that allow it to turn raw prompts or source materials directly into professional office documents, including ready-to-use `.docx`, `.pdf`, and `.xlsx` files.

Whether generating detailed financial reports, high school sponsorship proposals, or complex spreadsheets, GLM-5 delivers results in real-world formats that integrate directly into enterprise workflows.

It is also disruptively priced at roughly $0.80 per million input tokens and $2.56 per million output tokens, approximately 6x cheaper than proprietary competitors like Claude Opus 4.6, making state-of-the-art agentic engineering more cost-effective than ever before. Here's what else enterprise decision makers should know about the model and its training.

**Technology: scaling for agentic efficiency**
----------------------------------------------

At the heart of GLM-5 is a massive leap in raw parameters. The model scales from the 355B parameters of GLM-4.5 to a staggering 744B parameters, with 40B active per token in its Mixture-of-Experts (MoE) architecture. This growth is supported by an increase in pre-training data to 28.5T tokens.

To address training inefficiencies at this magnitude, Zai developed "[slime](https://github.com/THUDM/slime)," a novel asynchronous reinforcement learning (RL) infrastructure.

Traditional RL often suffers from "long-tail" bottlenecks; Slime breaks this lockstep by allowing trajectories to be generated independently, enabling the fine-grained iterations necessary for complex agentic behavior.

By integrating system-level optimizations like Active Partial Rollouts (APRIL), slime addresses the generation bottlenecks that typically consume over 90% of RL training time, significantly accelerating the iteration cycle for complex agentic tasks.

The framework’s design is centered on a tripartite modular system: a high-performance training module powered by Megatron-LM, a rollout module utilizing SGLang and custom routers for high-throughput data generation, and a centralized Data Buffer that manages prompt initialization and rollout storage.

By enabling adaptive verifiable environments and multi-turn compilation feedback loops, slime provides the robust, high-throughput foundation required to transition AI from simple chat interactions toward rigorous, long-horizon systems engineering.

To keep deployment manageable, GLM-5 integrates DeepSeek Sparse Attention (DSA), preserving a 200K context capacity while drastically reducing costs.

**End-to-end knowledge work**
-----------------------------

Zai is framing GLM-5 as an "office" tool for the AGI era. While previous models focused on snippets, GLM-5 is built to deliver ready-to-use documents.

It can autonomously transform prompts into formatted .docx, .pdf, and .xlsx files—ranging from financial reports to sponsorship proposals.

In practice, this means the model can decompose high-level goals into actionable subtasks and perform "Agentic Engineering," where humans define quality gates while the AI handles execution.

**High performance**
--------------------

GLM-5’s benchmarks make it the new most powerful open source model in the world, according to [Artificial Analysis](https://x.com/ArtificialAnlys/status/2021678229418066004), surpassing Chinese rival [Moonshot's new Kimi K2.5](https://venturebeat.com/orchestration/moonshot-ai-debuts-kimi-k2-5-most-powerful-open-source-llm-beating-opus-4-5) released just two weeks ago, showing that Chinese AI companies are nearly caught up 

---
*自动采集于 2026-02-12 15:12:17 (北京时间)*
