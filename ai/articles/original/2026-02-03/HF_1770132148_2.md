---
title: "Introducing NVIDIA Cosmos Policy for Advanced Robot Control"
url: "https://huggingface.co/blog/Photoroom/prx-part2"
source: "Hugging Face"
date: "2026-02-03"
---

# Introducing NVIDIA Cosmos Policy for Advanced Robot Control

**Êù•Ê∫ê**: [Hugging Face](https://huggingface.co/blog/Photoroom/prx-part2)

## ÂéüÊñáÂÜÖÂÆπ

Title: Training Design for Text-to-Image Models: Lessons from Ablations

URL Source: http://huggingface.co/blog/Photoroom/prx-part2

Markdown Content:
Training Design for Text-to-Image Models: Lessons from Ablations
===============

[![Image 1: Hugging Face's logo](http://huggingface.co/front/assets/huggingface_logo-noborder.svg)Hugging Face](http://huggingface.co/)

*   [Models](http://huggingface.co/models)
*   [Datasets](http://huggingface.co/datasets)
*   [Spaces](http://huggingface.co/spaces)
*    Community  
*   [Docs](http://huggingface.co/docs)
*   [Enterprise](http://huggingface.co/enterprise)
*   [Pricing](http://huggingface.co/pricing)
*    
*   
* * *

*   [Log In](http://huggingface.co/login)
*   [Sign Up](http://huggingface.co/join)

[Back to Articles](http://huggingface.co/blog)

[](http://huggingface.co/blog/Photoroom/prx-part2#training-design-for-text-to-image-models-lessons-from-ablations) Training Design for Text-to-Image Models: Lessons from Ablations
===================================================================================================================================================================================

[Team Article](http://huggingface.co/blog)Published February 3, 2026

[- [x] Upvote 17](http://huggingface.co/login?next=%2Fblog%2FPhotoroom%2Fprx-part2)
*   [![Image 2](https://cdn-avatars.huggingface.co/v1/production/uploads/1608816539831-noauth.png)](http://huggingface.co/eliotandres "eliotandres")
*   [![Image 3](http://huggingface.co/avatars/80e890c0c0b3c3e2b89d0bb555d2c658.svg)](http://huggingface.co/dim "dim")
*   [![Image 4](http://huggingface.co/avatars/194c9a17b98d1c96251effd32b1e5816.svg)](http://huggingface.co/simonamaggio "simonamaggio")
*   [![Image 5](http://huggingface.co/avatars/14297005d52915ddd3f46134ac376d8c.svg)](http://huggingface.co/MatthieuT "MatthieuT")
*   [![Image 6](http://huggingface.co/avatars/c4994e23f91ce9669dd2c24cad9b4b19.svg)](http://huggingface.co/mearco "mearco")
*   [![Image 7](http://huggingface.co/avatars/364aa363f427c1674f92dd2bfc6e08ab.svg)](http://huggingface.co/David-PHR "David-PHR")
*   +11

[![Image 8: David Bertoin's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PRXu5oOVO8CBToCdsxUgq.png)](http://huggingface.co/Bertoin)

[David Bertoin Bertoin Follow](http://huggingface.co/Bertoin)

[![Image 9: Photoroom's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/1608816711384-5fe497b773152f333e959e6f.png)](http://huggingface.co/Photoroom "Photoroom")[Photoroom](http://huggingface.co/Photoroom)

[![Image 10: Roman Frigg's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/680a58121b2c7c159d2bd481/NgSIR20w0QzOPQnL7HJDG.jpeg)](http://huggingface.co/photoroman)

[Roman Frigg photoroman Follow](http://huggingface.co/photoroman)

[![Image 11: Photoroom's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/1608816711384-5fe497b773152f333e959e6f.png)](http://huggingface.co/Photoroom "Photoroom")[Photoroom](http://huggingface.co/Photoroom)

[![Image 12: Jon Almaz√°n's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/68d136d7307413e80188d819/M8xB6XWB6Q9yox1hoSPRy.jpeg)](http://huggingface.co/jon-almazan)

[Jon Almaz√°n jon-almazan Follow](http://huggingface.co/jon-almazan)

[![Image 13: Photoroom's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/1608816711384-5fe497b773152f333e959e6f.png)](http://huggingface.co/Photoroom "Photoroom")[Photoroom](http://huggingface.co/Photoroom)

*   [The Baseline](http://huggingface.co/blog/Photoroom/prx-part2#the-baseline "The Baseline")

*   [Benchmarking Metrics](http://huggingface.co/blog/Photoroom/prx-part2#benchmarking-metrics "Benchmarking Metrics")

*   [Representation Alignment](http://huggingface.co/blog/Photoroom/prx-part2#representation-alignment "Representation Alignment")
    *   [REPA (Yu et al., 2024)](http://huggingface.co/blog/Photoroom/prx-part2#repa-yu-et-al-2024httpsarxivorgabs241006940 "REPA (Yu et al., 2024)")

    *   [iREPA (Singh et al., 2025)](http://huggingface.co/blog/Photoroom/prx-part2#irepa-singh-et-al-2025httpsarxivorgabs251210794 "iREPA (Singh et al., 2025)")

    *   [About Using REPA During the Full Training:](http://huggingface.co/blog/Photoroom/prx-part2#about-using-repa-during-the-full-training "About Using REPA During the Full Training:")

    *   [Alignment in the Token Latent Space](http://huggingface.co/blog/Photoroom/prx-part2#alignment-in-the-token-latent-space "Alignment in the Token Latent Space")

*   [Training Objectives: Beyond Vanilla Flow Matching](http://huggingface.co/blog/Photoroom/prx-part2#training-objectives-beyond-vanilla-flow-matching "Training Objectives: Beyond Vanilla Flow Matching")
    *   [Contrastive Flow Matching (Stoica et al., 2025)](http://huggingface.co/blog/Photoroom/prx-part2#contrastive-flow-matching-stoica-et-al-2025httpsarxivorgabs250605350 "Contrastive Flow Matching (Stoica et al., 2025)")

    *   [What we observed](http://huggingface.co/blog/Photoroom/prx-part2#what-we-observed-3 "What we observed")

    *   [JiT (Li and He, 2025)](http://huggingface.co/blog/Photoroom/prx-part2#jit-li-and-he-2025httpsarxivorgabs251113720 "JiT (Li and He, 2025)")

*   [Token Routing and Sparsification to Reduce Compute Costs](http://huggingface.co/blog/Photoroom/prx-part2#token-routing-and-sparsification-to-reduce-compute-costs "Token Routing and Sparsification to Reduce Compute Costs")

*   [Data](http://huggingface.co/blog/Photoroom/prx-part2#data "Data")
    *   [Long vs. Short Captions](http://huggingface.co/blog/Photoroom/prx-part2#long-vs-short-captions "Long vs. Short Captions")

    *   [Bootstrapping With Synthetic Images](http://huggingface.co/blog/Photoroom/prx-part2#bootstrapping-with-synthetic-images "Bootstrapping With Synthetic Images")

    *   [SFT With Alchemist: Small Dataset, Real Impact](http://huggingface.co/blog/Photoroom/prx-part2#sft-with-alchemist-small-dataset-real-impact "SFT With Alchemist: Small Dataset, Real Impact")

*   [More Useful Tips for Training](http://huggingface.co/blog/Photoroom/prx-part2#more-useful-tips-for-training "More Useful Tips for Training")
    *   [Muon Optimizer](http://huggingface.co/blog/Photoroom/prx-part2#muon-optimizer "Muon Optimizer")

    *   [Precision Gotcha: Casting vs. Storing weights in BF16](http://huggingface.co/blog/Photoroom/prx-part2#precision-gotcha-casting-vs-storing-weights-in-bf16 "Precision Gotcha: Casting vs. Storing weights in BF16")

*   [Summary](http://huggingface.co/blog/Photoroom/prx-part2#summary "Summary")

[![Image 14: image](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/sAbRvxS84gKtil9BEaqXP.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/sAbRvxS84gKtil9BEaqXP.png)
Welcome back! This is the second part of our series on training efficient text-to-image models from scratch.

In the [first post of this series](https://huggingface.co/blog/Photoroom/prx-part1-architectures), we introduced our goal: training a competitive text-to-image foundation model entirely from scratch, in the open, and at scale. We focused primarily on architectural choices and motivated the core design decisions behind our model **PRX**. We also [released an early, small (1.2B parameters) version of the model](https://huggingface.co/spaces/Photoroom/PRX-1024-beta-version) as a preview of what we are building (go try it if you haven't already üòâ).

In this post, we shift our focus from architecture to training. The goal is to document what actually moved the needle for us when trying to make models train faster, converge more reliably, and learn better representations. The field is moving quickly and the list of ‚Äútraining tricks‚Äù keeps growing, so rather than attempting an exhaustive survey, we structured this as an experimental logbook: we reproduce (or adapt) a set of recent ideas, implement them in a consistent setup, and report how they affect optimization and convergence in practice. Finally, we do not only report these techniques in isolation; we also explore which ones remain useful when combined.

In the next post, we will publish the full training recipe as code, including the experiments in this post. **We will also run and report on a public "speedrun"** where we put the best pieces together into a single configuration and stress-test it end-to-end. This exercise will serve both as a stress test of our current training pipeline and as a concrete demonstration of how far careful training design can go under tight constraints. If you haven‚Äôt already, we invite you to join our [Discord](https://discord.gg/HXp7Znc3) to continue the discussion. A significant part of this project has been shaped by exchanges with community members, and we place a high value on external feedback, ablations, and alternative interpretations of the results.

[](http://huggingface.co/blog/Photoroom/prx-part2#the-baseline) The Baseline
----------------------------------------------------------------------------

Before introducing any training-efficiency techniques, we first establish a clean reference run. This baseline is intentionally simple. It uses standard components, avoids auxiliary objectives, and does not rely on architectural shortcuts or tricks to save compute resources. Its role is to serve as a stable point of comparison for all subsequent experiments.

Concretely, this is a **pure Flow Matching** ([Lipman et al., 2022](https://arxiv.org/abs/2210.02747)) training setup (as introduced in [Part 1](https://huggingface.co/blog/Photoroom/prx-part1-architectures)) with no extra objectives and no architectural speed hacks. We will use the small PRX-1.2B model we presented in the first post of this series (single stream architecture with global attention for the image tokens and text tokens) as our baseline and train it in Flux VAE latent space, keeping the configuration fixed across all comparisons unless stated otherwise.

The baseline training setup is as follows:

| Setting | Value |
| --- | --- |
| Steps | 100k |
| Dataset | [Public 1M synthetic image generated with MidJourneyV6](https://huggingface.co/datasets/BitTranslate/Bittensor_subnet_19_06_04_24) |
| Resolution | 256√ó256 |
| Global batch size | 256 |
| Optimizer | AdamW |
| lr | 1e-4 |
| weight_decay | 0.0 |
| eps | 1e-15 |
| betas | (0.9, 0.95) |
| Text encoder | GemmaT5 |
| Positional encoding | Rotary (RoPE) |
| Attention mask | Padding mask |
| EMA | Disabled |

This baseline configuration provides a transparent and reproducible anchor. It allows us to attribute observed improvements and regressions to specific training interventions, rather than to shifting hyperparameters or hidden setup changes. Throughout the remainder of this post, every technique is evaluated against this reference with a single guiding question in mind:

> _Does this modification improve convergence or training efficiency relative to the baseline?_

![Image 15](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/AvE0WPccu6P5lF5T5xidf.png)![Image 16](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/1j82fCWMFOxC8HzPFqIST.png)![Image 17](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/Tq2DJCziRtPjNEWS3AxQQ.png)![Image 18](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/fMCpFObEhj2K3AL5LkeF5.png)![Image 19](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/ZCiFmMXbrecJR89MHR_wn.png)

Examples of baseline model generations after 100K training steps.

[](http://huggingface.co/blog/Photoroom/prx-part2#benchmarking-metrics) Benchmarking Metrics
--------------------------------------------------------------------------------------------

To keep this post grounded, we rely on a small set of metrics to monitor checkpoints over time. None of them is a perfect proxy for perceived image quality, but together they provide a practical scoreboard while we iterate.

*   **Fr√©chet Inception Distance (FID):** ([Heusel et al., 2017](https://proceedings.neurips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf)) Measures how close the distributions of generated and real images are, using Inception-v3 feature statistics (mean and covariance). Lower values typically correlate with higher sample fidelity.

*   **CLIP Maximum Mean Discrepancy (CMMD):** ([Jayasumana et al., 2024](https://openaccess.thecvf.com/content/CVPR2024/papers/Jayasumana_Rethinking_FID_Towards_a_Better_Evaluation_Metric_for_Image_Generation_CVPR_2024_paper.pdf)) Measures the distance between real and generated image distributions using CLIP image embeddings and Maximum Mean Discrepancy (MMD). Unlike FID, CMMD does not assume Gaussian feature distributions and can be more sample-efficient; in practice it often tracks perceptual quality better than FID, though it is still an imperfect proxy.

*   **DINOv2 Maximum Mean Discrepancy (DinoMMD):** Same MMD-based distance as CMMD, but computed on DINOv2 ([Oquab et al. 2023](https://arxiv.org/abs/2304.07193)) image embeddings instead of CLIP. This provides a complementary view of distribution shift under a self-supervised vision backbone.

*   **Network throughput:** Average number of samples processed per second (samples/s), as a measure of end-to-end training efficiency.

With the scoreboard defined, we can now dive into the methods we explored, grouped into four buckets: **Representation Alignment**, **Training Objectives**, **Token Routing and Sparsification**, and **Data**.

[](http://huggingface.co/blog/Photoroom/prx-part2#representation-alignment) Representation Alignment
----------------------------------------------------------------------------------------------------

Diffusion and flow models are typically trained with a single objective: predict a noise-like target (or vector field) from a corrupted input. Early in training, that one objective is doing two jobs at once: it must build a useful internal representation and learn to denoise on top of it. Representation alignment makes this explicit by keeping the denoising objective and adding an auxiliary loss that directly supervises intermediate features using a strong, frozen vision encoder. This tends to speed up early learning and bring the model‚Äôs features closer to those of modern self-supervised encoders. As a result, you often need less compute to hit the same quality.

A useful way to view it is to decompose the denoiser into an implicit encoder that produces intermediate hidden states, and a decoder that maps those states to the denoising target. The claim is that representation learning is the bottleneck: diffusion and flow transformers do learn discriminative features, but they lag behind foundation vision encoders when training is compute-limited. Therefore, borrowing a powerful representation space can make the denoising problem easier.

### [](http://huggingface.co/blog/Photoroom/prx-part2#repa-yu-et-al-2024) REPA ([Yu et al., 2024](https://arxiv.org/abs/2410.06940))

![Image 20: PRX block](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/jchZULI-fHCl-PbWIPzdl.png)

Representation alignment with a pre-trained visual encoder. Figure from [arXiv:2410.06940](https://arxiv.org/abs/2410.06940).

**REPA** adds a representation matching term on top of the base flow-matching objective.

Let x 0‚àºp data x_0 \sim p_{\text{data}}x 0‚Äã‚àºp data‚Äã be a clean sample and x 1‚àºp prior x_1 \sim p_{\text{prior}}x 1‚Äã‚àºp prior‚Äã be the noise sample. The model is trained on an interpolated state x t x_t x t‚Äã (for t‚àà[0,1]t \in [0,1]t‚àà[0,1]) and predicts a vector field v Œ∏(x t,t)v_\theta(x_t, t)v Œ∏‚Äã(x t‚Äã,t). In REPA, a pretrained vision encoder f f f processes the clean sample x 0 x_0 x 0‚Äã to produce patch embeddings y 0=f(x 0)‚ààR N√óD y_0 = f(x_0) \in \mathbb{R}^{N \times D}y 0‚Äã=f(x 0‚Äã)‚ààR N√óD, where N N N is the number of patch tokens and D D D is the teacher embedding dimension. In parallel, the denoiser processes x t x_t x t‚Äã and produces intermediate hidden tokens h t h_t h t‚Äã (one token per patch). A small projection head h œï h_\phi h œï‚Äã maps these student hidden tokens into the teacher embedding space, and an auxiliary loss maximizes patch-wise similarity between corresponding teacher and student tokens: L REPA(Œ∏,œï)=‚àíE x 0,x 1,t[1 N‚àën=1 N sim(y 0,[n],h œï(h t,[n]))] \mathcal{L}_{\text{REPA}}(\theta,\phi) = -\mathbb{E}_{x_0,x_1,t}\Big[\frac{1}{N}\sum_{n=1}^{N} \text{sim}\big(y_{0,[n]},\, h_\phi(h_{t,[n]})\big)\Big] L REPA‚Äã(Œ∏,œï)=‚àíE x 0‚Äã,x 1‚Äã,t‚Äã[N 1‚Äãn=1‚àëN‚Äãsim(y 0,[n]‚Äã,h œï‚Äã(h t,[n]‚Äã))] Here n‚àà{1,‚Ä¶,N}n \in \{1,\dots,N\}n‚àà{1,‚Ä¶,N} indexes patch tokens, y 0,[n]y_{0,[n]}y 0,[n]‚Äã is the teacher embedding for patch n n n, h t,[n]h_{t,[n]}h t,[n]‚Äã is the corresponding student hidden token at time t t t, and sim(‚ãÖ,‚ãÖ)\text{sim}(\cdot,\cdot)sim(‚ãÖ,‚ãÖ) is typically cosine similarity.

This term is combined with the main flow-matching loss:

L=L FM+Œª L REPA \mathcal{L} = \mathcal{L}_{\text{FM}} + \lambda\,\mathcal{L}_{\text{REPA}} L=L FM‚Äã+Œª L REPA‚Äã

with Œª\lambda Œª controlling the trade-off.

In practice, the student is trained to produce _noise-robust, data-consistent patch representations_ from x t x_t x t‚Äã, so later layers can focus on predicting the vector field and generating details rather than rediscovering a semantic scaffold from scratch.

#### [](http://huggingface.co/blog/Photoroom/prx-part2#what-we-observed) What we observed

We ran REPA on top of our baseline PRX training, using two frozen teachers: **DINOv2** and **DINOv3** ([Sim√©oni et al., 2025](https://arxiv.org/abs/2508.10104)). The pattern was very consistent: **adding alignment improves quality metrics**, and the stronger teacher helps more, at the cost of a bit of speed.

[![Image 21: image](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/komSJyY0PAWwwC90ckmin.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/komSJyY0PAWwwC90ckmin.png)

| Method | FID ‚Üì | CMMD ‚Üì | DINO-MMD ‚Üì | batches/sec ‚Üë |
| --- | --- | --- | --- | --- |
| Baseline | 18.2 | 0.41 | 0.39 | 3.95 |
| REPA-Dinov3 | 14.64 | 0.35 | 0.3 | 3.46 |
| REPA-Dinov2 | 16.6 | 0.39 | 0.31 | 3.66 |

On the quality metrics, both teachers improve over the baseline. The effect is strongest with DINOv3, which achieves the best overall numbers in this run.

REPA is not free: we pay for an extra frozen teacher forward and the patch-level similarity loss, which shows up as a throughput drop from **3.95 batches/s** to **3.66** (DINOv2) or **3.46** (DINOv3). In other words, DINOv3 prioritizes maximum representation quality at the cost of slower training, while DINOv2 offers a more efficient tradeoff, still delivering substantial gains with a smaller slowdown.

Our practical takeaway is that **REPA is a strong lever for text-to-image training**. In our setup, the throughput trade-off is real and the _net speedup_ (time required to reach a given level of image quality) felt a bit less dramatic than what the authors of the paper report on ImageNet-style, class-conditioned generation. That said, the **quality gains are still clearly significant**. Qualitatively, we also saw the difference early: after ~100K steps, samples trained with alignment tended to lock in **cleaner global structure and more coherent layouts**, which makes it easy to see why REPA (and alignment variants more broadly) have become a go-to ingredient in modern T2I training recipes.

| **Baseline** | **Repa-DinoV2** | **Repa-DinoV3** |
| --- | --- | --- |
| [![Image 22: media_images_txt2img_12_ Photography of a powerful, full-maned lion in mid-leap, emerging from a large, moss-covere..._100000_6011eb14deee0e4853fd(1)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/wH0iMWE6E9a9vkY9CdsaC.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/wH0iMWE6E9a9vkY9CdsaC.png) | [![Image 23: media_images_txt2img_12_ Photography of a powerful, full-maned lion in mid-leap, emerging from a large, moss-covere..._100000_e9f0e5ab2b08fd7a26ac](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/zP7r5EZAivYI2kSwZbvPi.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/zP7r5EZAivYI2kSwZbvPi.png) | [![Image 24: media_images_txt2img_12_ Photography of a powerful, full-maned lion in mid-leap, emerging from a large, moss-covere..._100000_8c29450cd3e318204dc1](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/GROruaYvwwUvL1o9pzkpR.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/GROruaYvwwUvL1o9pzkpR.png) |

### [](http://huggingface.co/blog/Photoroom/prx-part2#irepa-singh-et-al-2025) iREPA ([Singh et al., 2025](https://arxiv.org/abs/2512.10794))

A natural follow-up to REPA is: _what exactly should we be aligning?_**iREPA** argues that the answer is **spatial structure**, not global semantics. Across a large sweep of 27 vision encoders, the authors find that ImageNet-style ‚Äúglobal‚Äù quality (e.g., linear-probe accuracy on patch tokens) is only weakly predictive of downstream generation quality under REPA, while simple measures of **patch-token spatial self-similarity** correlate much more strongly with FID. Based on that diagnosis, iREPA makes two tiny but targeted changes to the REPA recipe to better preserve and transfer spatial information:

*   Replace the usual MLP projection head with a lightweight **3√ó3 convolutional projection** operating on the patch grid.
*   Apply a **spatial normalization** to teacher patch tokens that removes a global overlay (mean across spatial locations) to increase local contrast.

Despite representing ‚Äúless than 4 lines of code‚Äù, these tweaks consistently speed up convergence and improve quality across encoders, model sizes, and even REPA-adjacent training recipes.

#### [](http://huggingface.co/blog/Photoroom/prx-part2#what-we-observed-1) What we observed

In our setup, we observed a similar kind of boost when applying the iREPA spatial tweaks on top of **DINOv2**: convergence was a bit smoother and the metrics improved more steadily over the first 100K steps. Interestingly, the same changes did **not** transfer as cleanly when applied on top of a **DINOv3 teacher** and they tended to degrade performance rather than help. We do not want to over-interpret that result: this could easily be an interaction with our specific architecture, resolution/patching, loss weighting, or even small implementation details. Still, given this inconsistency across teachers, we will likely **not** include these tweaks in our default recipe, even if they remain an interesting option to revisit when tuning for a specific setup.

[![Image 25: image](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/P6pNA1lFjxwWyqF9p88od.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/P6pNA1lFjxwWyqF9p88od.png)

### [](http://huggingface.co/blog/Photoroom/prx-part2#about-using-repa-during-the-full-training) About Using REPA During the Full Training:

The paper [_REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training_ (Wang et al., 2025)](https://arxiv.org/abs/2505.16792) highlights a key caveat: REPA is a powerful _early_ accelerator, but it can plateau or even become a brake later in training. The authors describe a **capacity mismatch**. Once the generative model starts fitting the full data distribution (especially high-frequency details), forcing it to stay close to a frozen recognition encoder‚Äôs lower-dimensional embedding manifold becomes constraining. Their practical takeaway is simple: keep alignment for the ‚Äúburn-in‚Äù phase, then **turn it off** with a stage-wise schedule.

We observed the same qualitative pattern in our own runs. When training our preview model, **removing REPA after ~200K steps** noticeably improved the _overall feel_ of image quality, textures, micro-contrast, and fine detail continued to sharpen instead of looking slightly muted. For that reason, we also recommend treating representation alignment as a transient scaffold. Use it to get fast early progress, then **drop it after a while** once the model‚Äôs own generative features have caught up.

### [](http://huggingface.co/blog/Photoroom/prx-part2#alignment-in-the-token-latent-space) Alignment in the Token Latent Space

So far, ‚Äúalignment‚Äù meant **regularizing the generator‚Äôs internal features** against a frozen teacher while treating the tokenizer / latent space as fixed. A more direct lever is to **shape the latent space itself** so the representation presented to the flow backbone is intrinsically easier to model, without sacrificing the reconstruction fidelity needed for editing and downstream workflows.

**REPA-E** ([Leng et al., 2025](https://arxiv.org/abs/2504.10483)) makes this concrete. Its starting point is a failure mode: if you simply backprop the diffusion / flow loss into the VAE, the tokenizer quickly learns a pathologically easy latent for the denoiser, which can even degrade final generation quality. REPA-E‚Äôs fix is a two-signal training recipe:

*   keep the diffusion loss, but apply a stop-gradient so it only updates the latent diffusion model (not the VAE); 
*   update both the VAE and the diffusion model using an end-to-end REPA alignment loss.

Thanks to these two tricks, the tokenizer is explicitly optimized to produce latents that yield higher alignment and empirically better generations.

In parallel, Black Forest Labs‚Äô [FLUX.2 AE work](https://bfl.ai/research/representation-comparison) frames latent design as a trade-off between learnability, quality, and compression.Their core argument is that improving learnability requires injecting semantic structure into the representation, rather than treating the tokenizer as a pure compression module. This motivates retraining the latent space to explicitly target ‚Äúbetter learnability and higher image quality at the same time". They do not share the full recipe, but they do clearly state the key idea: make the AE‚Äôs latent space more learnable by adding semantic or representation alignment, and explicitly point to REPA-style alignment with a frozen vision encoder as the mechanism they build on and integrate into the FLUX.2 AE.

#### [](http://huggingface.co/blog/Photoroom/prx-part2#what-we-observed-2) What we observed

To probe alignment in the latent space, we compared two **pretrained autoencoders** as drop-in tokenizers for the same flow backbone: a **REPA-E-VAE** (where we _do_ add the REPA alignment objective, as in the paper) and the **Flux2-AE** (where we _do not_ add REPA, following their recommendation). The results were, honestly, extremely impressive, both quantitatively and qualitatively. In samples, the gap is immediately visible: generations show more coherent global structure and cleaner layouts, with far fewer ‚Äúearly training‚Äù artifacts.

[![Image 26: image](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/tbe05KV4hG0JCvVy8Bjiz.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/tbe05KV4hG0JCvVy8Bjiz.png)

| Method | FID ‚Üì | CMMD ‚Üì | DINO-MMD ‚Üì | batches/sec ‚Üë |
| --- | ---: | ---: | ---: | ---: |
| Baseline | 18.20 | 0.41 | 0.39 | 3.95 |
| Flux2-AE | 12.07 | 0.09 | 0.08 | 1.79 |
| REPA-E-VAE | 12.08 | 0.26 | 0.18 | 3.39 |

A first striking point is that both latent-space interventions lower the FID by ~6 points (18.20 to ~12.08), which is a much larger jump than what we typically get from ‚Äújust‚Äù aligning intermediate features. This strongly supports the core idea: if the tokenizer produces a representation that is intrinsically more learnable, the flow model benefits everywhere.

The two AEs then behave quite differently in the details. **Flux2-AE** dominates most metrics (very low CMMD and DINO_MMD, but it comes with a huge throughput penalty: batches/sec drops from **3.95 to 1.79**. In our case this slowdown is explained by practical factors they also emphasize: the model is simply heavier, and it also produces a **larger latent (32 channels)**, which increases the amount of work the diffusion backbone has to do per step.

**REPA-E-VAE** is the ‚Äúbalanced‚Äù option: it reaches essentially the same FID as Flux2-AE while keeping throughput much closer to the baseline (**3.39 batches/sec**).

| Baseline | Flux2-AE | REPA-E-VAE |
| --- | --- | --- |
| [![Image 27: Baseline sample](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/I4E6Yf3oUDxtuRfeH8zuw.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/I4E6Yf3oUDxtuRfeH8zuw.png) | [![Image 28: FLUX2_AE sample](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/J2D_0OjE6kLk_HIZouPQD.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/J2D_0OjE6kLk_HIZouPQD.png) | [![Image 29: REPA-E sample](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/9c5pwry-zTbUQrBEfHvQW.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/9c5pwry-zTbUQrBEfHvQW.png) |

[](http://huggingface.co/blog/Photoroom/prx-part2#training-objectives-beyond-vanilla-flow-matching) Training Objectives: Beyond Vanilla Flow Matching
-----------------------------------------------------------------------------------------------------------------------------------------------------

Architecture gets you capacity, but the training objective is what decides how that capacity is used. In practice, small changes to the loss often have outsized effects on convergence speed, conditional fidelity, and how quickly a model ‚Äúlocks in‚Äù global structure. In the sections below, we will go through the objectives we tested on top of our baseline rectified flow setup, starting with a simple but surprisingly effective modification: Contrastive Flow Matching.

### [](http://huggingface.co/blog/Photoroom/prx-part2#contrastive-flow-matching-stoica-et-al-2025) Contrastive Flow Matching ([Stoica et al., 2025](https://arxiv.org/abs/2506.05350))

Flow matching has a nice property in the unconditional case: trajectories are implicitly encouraged to be unique (flows should not intersect). But once we move to conditional generation (class- or text-conditioned), different conditions can still induce overlapping flows, which empirically shows up as ‚Äúaveraging‚Äù behavior: weaker conditional specificity, and muddier global structure. Contrastive flow matching addresses this directly by adding a contrastive term that pushes conditional flows away from other flows in the batch.

![Image 30: PRX block](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/vhLpYTMmLPaXOlYTnawF6.png)

Contrastive flow matching makes class-conditional flows more distinct, reducing overlap seen in standard flow matching, and produces higher-quality images that better represent each class. Figure from [arXiv:2506.05350](https://arxiv.org/abs/2506.05350).

For a given training triplet (x,y,Œµ)(x, y, \varepsilon)(x,y,Œµ), standard conditional flow matching trains the model velocity v Œ∏(x t,t,y)v_\theta(x_t,t,y)v Œ∏‚Äã(x t‚Äã,t,y) to match the target transport direction. Contrastive flow matching keeps that positive term, but additionally samples a _negative_ pair (x~,y~,Œµ~)(\tilde{x}, \tilde{y}, \tilde{\varepsilon})(x~,y~‚Äã,Œµ~) from the batch and penalizes the model if its predicted flow is also compatible with that other trajectory. In the paper‚Äôs notation, this becomes:

L Œî FM(Œ∏)=E[‚à•v Œ∏(x t,t,y)‚àí(Œ±Àôt x+œÉÀôt Œµ)‚à•2‚àíŒª‚à•v Œ∏(x t,t,y)‚àí(Œ±Àôt x~+œÉÀôt Œµ~)‚à•2] \mathcal{L}_{\Delta \text{FM}}(\theta) = \mathbb{E}\Big[ \|v_\theta(x_t,t,y)-(\dot{\alpha}_t x+\dot{\sigma}_t\varepsilon)\|^2 \;-\; \lambda \|v_\theta(x_t,t,y)-(\dot{\alpha}_t \tilde{x}+\dot{\sigma}_t\tilde{\varepsilon})\|^2 \Big] L Œî FM‚Äã(Œ∏)=E[‚à•v Œ∏‚Äã(x t‚Äã,t,y)‚àí(Œ±Àôt‚Äãx+œÉÀôt‚ÄãŒµ)‚à•2‚àíŒª‚à•v Œ∏‚Äã(x t‚Äã,t,y)‚àí(Œ±Àôt‚Äãx~+œÉÀôt‚ÄãŒµ~)‚à•2]

where Œª‚àà[0,1)\lambda\in[0,1)Œª‚àà[0,1) controls the strength of the ‚Äúpush-away‚Äù term. Intuitively: **match your own trajectory, and be incompatible with someone else‚Äôs**.

The authors show that contrastive flow matching produces **more discriminative trajectories** and that this translates into both **quality and efficiency gains**: faster convergence (reported up to **9√ó fewer training iterations** to reach similar FID) and fewer sampling steps (reported up to **5√ó fewer denoising steps**) on **ImageNet** ([Deng et al. 2009](https://ieeexplore.ieee.org/document/5206848)) and **CC3M**([Sharma et al., 2018](https://aclanthology.org/P18-1238/)) experiments.

A key advantage is that the objective is almost a drop-in replacement: you keep the usual flow-matching loss, then add a single contrastive ‚Äúpush-away‚Äù term using other samples in the same batch as negatives which provides the extra supervision without introducing additional model passes.

### [](http://huggingface.co/blog/Photoroom/prx-part2#what-we-observed-3) What we observed

| Method | FID ‚Üì | CMMD ‚Üì | DINO-MMD ‚Üì | batches/sec ‚Üë |
| --- | ---: | ---: | ---: | ---: |
| Baseline | 18.20 | 0.41 | 0.39 | 3.95 |
| Contrastive-FM | 20.03 | 0.40 | 0.36 | 3.75 |

On this run, contrastive flow matching yields a small but measurable improvement on the representation-driven metrics: CMMD goes from **0.41 ‚Üí 0.40** and DINO-MMD from **0.39 ‚Üí 0.36**. The magnitude of the gain is smaller than what the paper reports on ImageNet, which is not too surprising: text conditioning is much more complex than discrete classes, and the training data distribution is likely less ‚Äúseparable‚Äù than ImageNet, making the contrastive signal harder to exploit.

We do not see an improvement in FID in this specific experiment (it slightly worsens), but the **throughput cost is negligible** in practice (3.95 ‚Üí 3.75 batches/sec). Given the simplicity of the change and the consistent movement in the right direction for the conditioning/representation metrics, we will likely still keep contrastive flow matching in our training pipeline as a low-cost regularizer.

### [](http://huggingface.co/blog/Photoroom/prx-part2#jit-li-and-he-2025) JiT ([Li and He, 2025](https://arxiv.org/abs/2511.13720))

[_Back to Basics: Let Denoising Generative Models Denoise_](https://arxiv.org/abs/2511.13720) is probably one of our favorite recent papers in the diffusion space because it is not a new trick but a reset: stop asking the network to predict off-manifold quantities (noise or velocity) and just let it denoise. Most modern diffusion and flow models train the network to predict **noise Œµ\varepsilon Œµ** or a **mixed quantity** like **velocity v v v**. Under the manifold assumption, natural images live on a low-dimensional manifold, while Œµ\varepsilon Œµ and v v v are inherently **off-manifold**, so predicting them can be a harder learning problem than it looks.

![Image 31: PRX block](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/fyy9m5Nh7F-IkksdlmnlR.png)

Under the manifold assumption, clean images lie on the data manifold while noise and velocity do not. Thus training the model to predict clean images is fundamentally easier than training it to predict noise-like targets. Figure from [arXiv:2511.13720](https://arxiv.org/abs/2511.13720).

The authors frame the problem with the standard linear interpolation between the clean image x x x and the noise Œµ\varepsilon Œµ: z t=t x+(1‚àít)Œµ, z_t = t\,x + (1-t)\,\varepsilon, z t‚Äã=t x+(1‚àít)Œµ, and the corresponding flow velocity: v=d z t d t=x‚àíŒµ. v = \frac{d z_t}{dt} = x - \varepsilon. v=d t d z t‚Äã‚Äã=x‚àíŒµ.

Instead of outputting v Œ∏ v_\theta v Œ∏‚Äã directly, the model predicts a clean image estimate: x Œ∏(z t,t):=n e t Œ∏(z t,t), x_\theta(z_t,t) := \mathrm{net}_\theta(z_t,t), x Œ∏‚Äã(z t‚Äã,t):=net Œ∏‚Äã(z t‚Äã,t), and we **convert** it to a velocity prediction via: v Œ∏(z t,t)=x Œ∏(z t,t)‚àíz t 1‚àít. v_\theta(z_t,t) = \frac{x_\theta(z_t,t) - z_t}{1-t}. v Œ∏‚Äã(z t‚Äã,t)=1‚àít x Œ∏‚Äã(z t‚Äã,t)‚àíz t‚Äã‚Äã.

Then we can keep the exact same flow-style objective in **v-space**: L v=E t,x,Œµ[‚à•v Œ∏(z t,t)‚àív‚à•2 2]with v=x‚àíŒµ. \mathcal{L}_{v} = \mathbb{E}_{t,x,\varepsilon}\left[\left\|v_\theta(z_t,t) - v\right\|_2^2\right] \quad\text{with}\quad v = x-\varepsilon. L v‚Äã=E t,x,Œµ‚Äã[‚à•v Œ∏‚Äã(z t‚Äã,t)‚àív‚à•2 2‚Äã]with v=x‚àíŒµ.

This formulation makes the learning problem substantially easier in high dimensions: instead of predicting noise or velocity (which are essentially unconstrained in pixel space), the network predicts the clean image x x x, i.e., something that lies on the data manifold. In practice, this makes it feasible to train large-patch Transformers directly on pixels without a VAE or tokenizer while keeping optimization stable and the total number of tokens manageable.

#### [](http://huggingface.co/blog/Photoroom/prx-part2#what-we-observed-4) What we observed

We first evaluated **x-prediction** in the same setting as the rest of our objective experiments, namely **training in the FLUX latent space at 256√ó256 resolution**. [![Image 32: image](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/62As0PavSB4rxTIm-OHYY.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/62As0PavSB4rxTIm-OHYY.png)

| Method | FID ‚Üì | CMMD ‚Üì | DINO-MMD ‚Üì | batches/sec ‚Üë |
| --- | ---: | ---: | ---: | ---: |
| Baseline | 18.20 | 0.41 | 0.39 | 3.95 |
| X-Pred | 16.80 | 0.54 | 0.49 | 3.95 |

In this regime, the benefit of x-prediction is **unclear**. While **FID improves slightly** compared to the baseline, both **CMMD** and **DINO-MMD** degrade noticeably, and throughput is unchanged. This suggests that, when working in an already well-structured latent space, predicting clean images instead of velocity does not consistently dominate the baseline objective, and can even hurt representation-level alignment.

That said, this experiment is not where x-prediction really shines.

The exciting part is that **x-prediction stabilizes high-dimensional training**, making it feasible to use **larger patches** and denoise **directly in pixel space**, without a VAE, at much higher resolutions. Using JiT, we trained a model **directly on 1024√ó1024 images** with **32√ó32 patches**, instead of operating in a compressed latent space. Despite the much higher resolution and the absence of a tokenizer, optimization remained stable and fast. We reached **FID 17.42**, **DINO_MMD 0.56**, and **CMMD 0.71** with a throughput of **1.33 batches/sec**.

These results are remarkable: training directly on 1024√ó1024 images is only about **3√ó slower** than training in a 256√ó256 latent space, while operating on raw pixels. This strongly supports the core claim of _Back to Basics_: letting the model predict clean images makes the learning problem significantly easier, and opens the door to high-resolution, tokenizer-free text-to-image training without prohibitive compute costs.

As a result, we plan to use this formulation as the backbone of our upcoming **speedrun experiments**, to see how far we can push it when combined with the other efficiency and sparsification techniques discussed above. The main downside for now is that this approach does not let us benefit from the very nice properties of the **FLUX.2 VAE**; exploring whether some form of alignment or hybrid training could make these two worlds compatible is an open direction we plan to investigate further.

|  |  |  |
| --- | --- | --- |
| [![Image 33: Sample 1](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/Q4arVHf3xw8CHeMZX4xE3.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/Q4arVHf3xw8CHeMZX4xE3.png) | [![Image 34: Sample 2](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/d_wI2o18mj48JKC73F5oV.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/d_wI2o18mj48JKC73F5oV.png) | [![Image 35: Sample 3](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/dWDtgRQRUtWBz2m7XPfiQ.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/dWDtgRQRUtWBz2m7XPfiQ.png) |

[](http://huggingface.co/blog/Photoroom/prx-part2#token-routing-and-sparsification-to-reduce-compute-costs) Token Routing and Sparsification to Reduce Compute Costs
--------------------------------------------------------------------------------------------------------------------------------------------------------------------

So far, most of the techniques we discussed focus on making **each training step more effective**: improving the objective, shaping the representations, or accelerating convergence. The next lever is orthogonal: **make each step cheaper**.

For diffusion and flow transformers, the dominant cost is running deep transformer stacks over a large set of image/latent tokens where attention scales poorly with sequence length. _Token sparsification_ methods target this directly by ensuring that only a subset of tokens pays the full compute price in the expensive parts of the network, while still preserving enough information flow to keep quality high.

Most masking approaches accelerate training by **removing tokens from the forward pass**, then asking the model to hallucinate the missing content from learned placeholders. That works surprisingly well, but it violates the spirit of iterative denoising. Instead of refining all the content in each step, we are reconstructing parts from scratch.

Two recent papers illustrate a cleaner alternative: instead of deleting information, they reorganize where compute is spent. **TREAD** and **SPRINT** share the same high-level objective of avoiding full-depth computation for every token at every layer, but they pursue it through complementary strategies.

**TREAD**'s ([Krause et al., 2025](https://openaccess.thecvf.com/content/ICCV2025/papers/Krause_TREAD_Token_Routing_for_Efficient_Architecture-agnostic_Diffusion_Training_ICCV_2025_paper.pdf)) core idea is to replace compute reduction through information loss, such as dropping or masking tokens, with compute reduction through information transport using token routing. It introduces a **route**: for each training sample, it randomly selects a fraction of tokens and _temporarily bypasses_ a contiguous chunk of layers, then **re-injects those tokens later**. Tokens are not discarded. Instead, they avoid paying the cost of full depth. Concretely, for a denoiser with a stack of blocks L 0,‚Ä¶,L B‚àí1 L_0, \dots, L_{B-1}L 0‚Äã,‚Ä¶,L B‚àí1‚Äã, TREAD defines a route r i t o j r_{i\\to j}r i t o j‚Äã (start layer i i i, end layer j j j). A subset of tokens follows the cheap path (identity) across L i,‚Ä¶,L j L_i,\dots,L_j L i‚Äã,‚Ä¶,L j‚Äã, while the rest follows the normal full computation. Then both streams merge again at L j L_j L j‚Äã. In practice, the paper shows that routing up to **50% of tokens** remains effective, while higher rates begin to degrade quality.

![Image 36: PRX block](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/2eO2iKhoZfIgHJTMdPYe6.png)

. TREAD enhances training efficiency by routing tokens around certain layers. Figure from [arXiv:/2501.04765](https://arxiv.org/abs//2501.04765).

**SPRINT** ([Park et al., 2025](https://arxiv.org/pdf/2510.21986)) extends this approach by introducing sparsity in the most computationally expensive parts of the network, while preserving a dense information pathway. Its recipe is intentionally structured: run **dense early layers** over all tokens to build reliable low-level features, then keep only a subset of tokens through the **sparse middle layers** where compute is heaviest, and finally go **dense again** by re-expanding and **fusing** sparse deep features with a **dense residual stream** from the early layers, before producing the output. The key distinction from TREAD is where robustness comes from: TREAD keeps tokens ‚Äúpresent‚Äù but shallower (routing), whereas SPRINT allows many tokens to be _absent_ in the middle blocks, relying on the dense residual path to preserve full-resolution information. This is what enables **more aggressive sparsification** in practice. The paper explores drop ratios around **75%**, versus ~50% for TREAD.

![Image 37: PRX block](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/27K_mTbSIxdT5pPetxrpQ.png)

SPRINT goes beyond TREAD by dropping most tokens in the middle layers while keeping a dense residual path to preserve full-resolution information. Figure from [arXiv:/2510.21986](https://arxiv.org/abs//2510.21986).

#### [](http://huggingface.co/blog/Photoroom/prx-part2#what-we-observed-5) What we observed

| Method | FID ‚Üì | CMMD ‚Üì | DINO-MMD ‚Üì | batches/sec ‚Üë |
| --- | ---: | ---: | ---: | ---: |
| Baseline | 18.20 | 0.41 | 0.39 | 3.95 |
| TREAD | 21.61 | 0.55 | 0.41 | 4.11 |
| SPRINT | 22.56 | 0.72 | 0.42 | 4.20 |

Under our standard 256√ó256 latent setup, both methods deliver the primary benefit we were targeting. TREAD goes from 3.95 ‚Üí 4.11 batches/sec, and SPRINT pushes it a bit further to 4.20 batches/sec. The cost is that under our evaluation protocol, this extra throughput comes with a clear loss in quality: FID rises from 18.20 to 21.61 (TREAD) and 22.56 (SPRINT), with the same pattern observed in CMMD and DINO-MMD.

Taken at face value, routing yields a modest **~7‚Äì9% throughput gain**, but it comes with worse metrics in this benchmark, with **SPRINT** (the more aggressive scheme) degrading quality slightly more than **TREAD**.

One important caveat is that **token-sparse / routed models tend to score worse under vanilla Classifier-Free Guidance (CFG)**, and this effect is likely amplified here because these runs are still relatively **undertrained** in our setting. The authors of _Guiding Token-Sparse Diffusion Models_ ([Krause et al., 2025](https://arxiv.org/abs/2601.01608)) argue this is partly an evaluation mismatch: routing changes the model‚Äôs effective capacity, and plain ‚Äúconditional vs. unconditional‚Äù CFG often becomes less effective, which can artificially reduce quality. We deliberately did **not** use specialized guidance schemes to keep our benchmark consistent across methods, and at this stage it would also not be very meaningful to treat the sparse model as a ‚Äúbad version of itself‚Äù for guidance. As a result, we consider these numbers directionally useful, but still **pessimistic** and worth interpreting with caution.

At **256√ó256**, routing only gave modest gains because the model processes relatively few tokens. At **1024√ó1024**, the picture changes completely. With **1024 tokens**, routing finally targets the dominant cost, and the results are striking.

[![Image 38: image](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/p7EfNrasU3pDy9EuTRB-K.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/p7EfNrasU3pDy9EuTRB-K.png)

| Method | FID ‚Üì | CMMD ‚Üì | DINO-MMD ‚Üì | batches/sec ‚Üë |
| --- | ---: | ---: | ---: | ---: |
| Baseline | 17.42 | 0.71 | 0.56 | 1.33 |
| TREAD | 14.10 | 0.46 | 0.37 | 1.64 |
| SPRINT | 16.90 | 0.51 | 0.41 | 1.89 |

Both **TREAD** and **SPRINT** deliver **large throughput gains** over the dense baseline, with SPRINT pushing speed the furthest. More importantly, this time the gains do **not** come at the expense of quality but quite the opposite. **TREAD** in particular stands out, with a dramatic drop in FID (**17.42 ‚Üí 14.10**) alongside strong improvements in CMMD and DINO-MMD. **SPRINT** is slightly more aggressive and a bit noisier in quality, but still clearly improves over the baseline while being the fastest option.

In short, this is the regime where token routing really shines: **high resolution, many tokens, and JiT-style pixel-space training**. Here, routing is no longer a marginal optimization‚Äîit‚Äôs a major lever that improves both **how fast** and **how well** the model trains.

[](http://huggingface.co/blog/Photoroom/prx-part2#data) Data
------------------------------------------------------------

After covering representation alignment, the core training objective, and token routing, we turned to the fourth axis that kept constantly mattered in practice: **data**. We found that the choice of training data, including how it is described through captions, can influence the trajectory of a training run as much as optimization techniques. Below are three concrete data experiments that consistently moved the needle in our setup.

### [](http://huggingface.co/blog/Photoroom/prx-part2#long-vs-short-captions) Long vs. Short Captions

Captions are an essential part of the training set: for a text-to-image model, they are not just metadata, they are the supervision. The [DALL¬∑E 3 (Betker et al., 2023) research paper](https://cdn.openai.com/papers/dall-e-3.pdf) showed that richer captions can be one of the strongest levers for improving training signal and prompt-following. To isolate the effect in our setup, we kept everything else fixed and changed only the caption style to compare:

*   Long, descriptive captions (our baseline): multi-clause captions that mention composition, attributes, lighting, materials, and relationships.

> **Example**
> 
> _"A photograph depicts a fluffy lop-eared rabbit sitting on a weathered wooden surface outdoors. The rabbit is predominantly white with patches of light brown and tan fur, particularly on its head and ears. Its ears droop noticeably, and its fur appears soft and thick. The rabbit's eyes are dark and expressive. It is positioned slightly off-center, facing towards the left of the frame. Behind the rabbit, slightly out of focus, is a miniature dark red metal wheelbarrow. A partially visible orange apple sits to the left of the rabbit. Fallen autumn leaves, predominantly reddish-brown, are scattered around the rabbit and apple on the wooden surface. The background is a blurred but visible expanse of green grass, suggesting an outdoor setting. The lighting is soft and natural, likely diffused daylight, casting no harsh shadows. The overall atmosphere is calm, peaceful, and autumnal. The aesthetic is rustic and charming, with a focus on the rabbit as the main subject. The color palette is muted and natural, consisting mainly of whites, browns, oranges, and greens. The style is naturalistic and straightforward, without any overt artistic manipulation. The vibe is gentle and heartwarming."_

*   Short, one-line captions: minimal descriptions with much less structure.

> **Example**
> 
> _"A rabbit sitting on top of a wooden table."_

#### [](http://huggingface.co/blog/Photoroom/prx-part2#what-we-observed-6) What we observed

[![Image 39: image](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/B2Ot5-iPSTgMm613HOXE-.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/B2Ot5-iPSTgMm613HOXE-.png)

| Method | FID ‚Üì | CMMD ‚Üì | DINO-MMD ‚Üì | batches/sec ‚Üë |
| --- | ---: | ---: | ---: | ---: |
| Baseline | 18.20 | 0.41 | 0.39 | 3.95 |
| Short-Captions | 36.84 | 0.98 | 1.14 | 3.95 |

The outcome was unambiguous: switching to short captions severely hurt convergence across all metrics. Long captions provide a richer supervision signal: beyond prompt adherence, there is a very practical optimization reason. More tokens usually means more information, and therefore more learning signal for the denoiser. When the conditioning text specifies composition, attributes, lighting, materials, and relationships, the model gets a sharper ‚Äútarget‚Äù for what the denoising trajectory should preserve and refine, especially early in training.

The fun paradox is that this extra detail often makes the learning problem _easier_, not harder: intuitively, one might expect longer prompts, with more attributes, constraints, and relationships, to increase complexity and burden the model. In practice, the opposite happens. Short captions leave many degrees of freedom unspecified, forcing the model to learn under ambiguity and implicitly average across multiple plausible interpretations. Long captions collapse that uncertainty by turning implicit choices into explicit constraints, allowing the denoiser to focus its capacity on refining a well-posed solution instead of guessing what matters.

Long captions are a strong training-time accelerator, but we still want the model to behave well on short prompts because that is how people actually use these systems. A simple workaround is to end training with a short fine-tuning stage on a mixture of long and short captions. That keeps the benefits of rich supervision early, while teaching the model to stay robust when conditioning is sparse.

### [](http://huggingface.co/blog/Photoroom/prx-part2#bootstrapping-with-synthetic-images) Bootstrapping With Synthetic Images

Another data-related research question we explore is whether a low-cost synthetic corpus can accelerate early training compared to a real corpus of similar size. For this benchmark, we trained a model on a dataset of **real images collected from [Pexels](https://www.pexels.com/)** and compare it with our Baseline which was trained on synthetic data generated with MidjourneyV6, both of which have around 1M images. We evaluated both runs against the same **[Unsplash](https://unsplash.com/)** reference set, composed exclusively of real images. [![Image 40: image](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/tqlQWXQJzA7HqXzBy-0B_.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/tqlQWXQJzA7HqXzBy-0B_.png)

| Method | FID ‚Üì | CMMD ‚Üì | DINO-MMD ‚Üì | batches/sec ‚Üë |
| --- | ---: | ---: | ---: | ---: |
| Synthetic images | 18.20 | 0.41 | 0.39 | 3.95 |
| Real images | 16.6 | 0.5 | 0.46 | 3.95 |

**The synthetic-trained model scores better on CMMD and DINO-MMD**, while **the model trained on real images achieves a lower FID**. Rather than a contradiction, this split mostly reflects what these metrics emphasize.

FID is particularly sensitive to **low-level image statistics**: fine textures, high-frequency detail, noise patterns, and the subtle irregularities of real photography. Since our evaluation reference is composed of real images, a model trained on real photos naturally matches those statistics more closely, which translates into a better FID. Synthetic images, by contrast, often exhibit slightly different high-frequency signatures, cleaner edges, smoother micro-textures, more uniform noise, which are barely noticeable qualitatively but still get penalized by distributional metrics like FID.

Qualitatively, this difference is easy to spot. Models trained on synthetic data tend to produce images with cleaner global structure and stronger compositional and object coherence, but also exhibit a more synthetic appearance, characterized by smoother textures and reduced photographic noise. In contrast, models trained on real images better capture the irregular, fine-grained textures typical of natural photographs, though they often require more training to achieve comparable global structure.

One plausible explanation synthetic data remains so effective early on is that it exposes the model to a wider range of **compositional collisions**: unusual pairings of objects, attributes, styles, and viewpoints that rarely co-occur in natural datasets. While this can hurt realism at the texture level, it forces the model to explain a broader space of combinations, which appears to help with **early disentanglement and structure learning**.

Considered jointly, this suggests a simple but practical strategy: synthetic data is an efficient way to bootstrap training and lock in global structure quickly, while real images remain important later on if matching photographic texture statistics is the priority.

| Model trained with real data | [![Image 41: dog_cropped](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/5-3TE7jXhqCJVlknE1Tjh.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/5-3TE7jXhqCJVlknE1Tjh.png) | [![Image 42: kid_cropped](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/KpXibG5uK9QbrefG3yynR.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/KpXibG5uK9QbrefG3yynR.png) | [![Image 43: bike_cropped](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/0ONhsdnDelBZmuDkrwHA7.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/0ONhsdnDelBZmuDkrwHA7.png) | [![Image 44: media_images_txt2img_06_ An exquisite crystal bottle of luxury perfume resting on a mirrored surface. Soft, diffuse..._100000_9a3d35747cdb5d20c649](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/Mh8GkTVlMzboE8F6ttoYo.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/Mh8GkTVlMzboE8F6ttoYo.png) | [![Image 45: media_images_txt2img_00_ A curious tabby cat perched on a moss-covered log in a lush, misty forest at dawn. Sunbeam..._100000_558072142d560edc7553](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/vjmcpiI_1KegTDuUv4dMV.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/vjmcpiI_1KegTDuUv4dMV.png) | [![Image 46: people_cropped](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/c1U9iVAfPx6zuCghhgzi4.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/c1U9iVAfPx6zuCghhgzi4.png) |
| --- | --- | --- | --- | --- | --- | --- |
| Model trained with synthetic data | [![Image 47: media_images_txt2img_17_ The image is a photography of a calm, serene dog in a meditative pose, sitting on a lush g..._100000_428d53247e6cac4f09eb(1)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/P5fGS2iRChjizxL7lJVou.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/P5fGS2iRChjizxL7lJVou.png) | [![Image 48: media_images_txt2img_08_ A carefree young child with tousled hair and rosy cheeks, laughing joyfully while running ..._100000_4e389f3f00e6bc85e55a](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/xfbMia525Csco8AiNY3DF.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/xfbMia525Csco8AiNY3DF.png) | [![Image 49: media_images_txt2img_24_ An old, cobblestone street in a European city. Colorful buildings with flower boxes in the..._100000_2db1f8aece2aa6bbeb36](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/Pbz17ckHk_GvS2l8I8Wz-.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/Pbz17ckHk_GvS2l8I8Wz-.png) | [![Image 50: media_images_txt2img_06_ An exquisite crystal bottle of luxury perfume resting on a mirrored surface. Soft, diffuse..._100000_8b3a0abb1807c27ad5a4](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/yBFy5Wjp7XlpRSajSTj1B.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/yBFy5Wjp7XlpRSajSTj1B.png) | [![Image 51: media_images_txt2img_00_ A curious tabby cat perched on a moss-covered log in a lush, misty forest at dawn. Sunbeam..._100000_7cb7d005b969e32c3b81](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/iIRdUhwMV5HPxAJkQb9G0.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/iIRdUhwMV5HPxAJkQb9G0.png) | [![Image 52: media_images_txt2img_20_ Photography of a bustling city street at dusk. Neon signs illuminate the scene, reflecting..._100000_cc633614b68c98ab11e3](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/2hOQ-cxfoncFMsVLBHuZV.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/2hOQ-cxfoncFMsVLBHuZV.png) |

### [](http://huggingface.co/blog/Photoroom/prx-part2#sft-with-alchemist-small-dataset-real-impact) SFT With Alchemist: Small Dataset, Real Impact

Finally, we experimented with a targeted Supervised Fine-Tuning (SFT) pass using **Alchemist** ([Startsev et al., 2025](https://arxiv.org/abs/2505.19297)), a compact dataset explicitly curated for high-impact. Alchemist is small by design (3,350 image‚Äìtext pairs), but is constructed through a sophisticated curation pipeline that starts from a web-scale pool and progressively distills it down to visually exceptional samples.

In our setup, we fine-tuned our preview models for 20K steps on Alchemist. Despite the dataset‚Äôs small size, it had an outsized effect: it adds a distinct ‚Äústyle layer‚Äù with better composition, more photographic polish, and richer scenes without a clear impact on generalization.

The samples below show a **side-by-side comparison** of generations from the same base model, before and after the Alchemist fine-tuning pass.

| Without SFT | [![Image 53](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/tbWxKwi5MIzisD7RYZ3Om.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/tbWxKwi5MIzisD7RYZ3Om.png) | [![Image 54](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/ayTlbjvvL_BsGMCefnsgz.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/ayTlbjvvL_BsGMCefnsgz.png) | [![Image 55](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/kJ6g3HU7Op7hciLr3_DVy.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/kJ6g3HU7Op7hciLr3_DVy.png) | [![Image 56](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/Nx1IlqAvUQ4WeFtUQkdo5.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/Nx1IlqAvUQ4WeFtUQkdo5.png) | [![Image 57](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/FDH3ekKI8_S5Gg6nlJ93R.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/FDH3ekKI8_S5Gg6nlJ93R.png) | [![Image 58](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/8K-ZHtSsL3uAQFJRAcxzH.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/8K-ZHtSsL3uAQFJRAcxzH.png) | [![Image 59](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/HuUjxgbpe9jUebEJDPB-g.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/HuUjxgbpe9jUebEJDPB-g.png) | [![Image 60](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/jnBYAQWY4HJeAwFHf915S.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/jnBYAQWY4HJeAwFHf915S.png) |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| With SFT | [![Image 61](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/ti6sX1n4rcbG2jD64nGy4.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/ti6sX1n4rcbG2jD64nGy4.png) | [![Image 62](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/qwPzYsdq2xdm4tke2mI8M.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/qwPzYsdq2xdm4tke2mI8M.png) | [![Image 63](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/6QbjTTsPl06F2Geqn38cX.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/6QbjTTsPl06F2Geqn38cX.png) | [![Image 64](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/YdaTUv9EGRn6hKXSNokQs.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/YdaTUv9EGRn6hKXSNokQs.png) | [![Image 65](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/LdWbPCJ_q6hMKnIUxwydS.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/LdWbPCJ_q6hMKnIUxwydS.png) | [![Image 66](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/u82EW1XyHi1164-PyZl-F.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/u82EW1XyHi1164-PyZl-F.png) | [![Image 67](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/WoiS_HIxOkpEqQuJU1Rbk.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/WoiS_HIxOkpEqQuJU1Rbk.png) | [![Image 68](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/OJ25nDSAmi6xjUivz3QC5.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/OJ25nDSAmi6xjUivz3QC5.png) |

[](http://huggingface.co/blog/Photoroom/prx-part2#more-useful-tips-for-training) More Useful Tips for Training
--------------------------------------------------------------------------------------------------------------

Last but not least, we will briefly cover two practical training details that turned out to matter more than we expected. These factors are easily overlooked and in our case they had a clear impact on convergence and final image quality.

### [](http://huggingface.co/blog/Photoroom/prx-part2#muon-optimizer) Muon Optimizer

We generally default to AdamW for our benchmarks because it‚Äôs predictable and easy to compare across runs. However, lately, we have seen a renewed interest in optimizers that try to behave more like a good preconditioner without the full overhead of second-order methods. One recent example is **Muon** ([Jordan et al., 2024](https://kellerjordan.github.io/posts/muon/)), which, at a high level, tries to improve optimization by applying a better-conditioned update step, often translating into faster convergence and cleaner progress early in training.

In our setup, Muon was one of the rare cases in which a change of optimizer produced an immediatly observable effect on the metrics.

[![Image 69: image](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/IhD8XksIA4rN2N339ZH47.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/IhD8XksIA4rN2N339ZH47.png)

| Method | FID ‚Üì | CMMD ‚Üì | DINO-MMD ‚Üì |
| --- | --- | --- | --- |
| Baseline | 18.20 | 0.41 | 0.39 |
| Muon | 15.55 | 0.36 | 0.35 |

For this experiment, we used the official PyTorch implementation of Muon, which at the moment supports Distributed Data Parallel (DDP) training only. If you‚Äôre running Fully Sharded Data Parallel (FSDP), there are community variants available; for example [here](https://github.com/samsja/muon_fsdp_2).

While we refrain from broad conclusions based on a single benchmark, these results indicate that optimizer choice extends beyond stability considerations and can yield tangible gains in time-to-quality.

### [](http://huggingface.co/blog/Photoroom/prx-part2#precision-gotcha-casting-vs-storing-weights-in-bf16) Precision Gotcha: Casting vs. Storing weights in BF16

We eventually identified an error in our setup, where the denoiser weights were mistakenly stored in bfloat16 for a period of time.

To be clear, using the BF16 autocast is great. Running the forward and backward passes in BF16 or mixed precision is standard and usually what you want for speed and memory. The problem arises from keeping the parameters in BF16 precision, which negatively impacts numerically sensitive operations.

In practice, some layers and operations are much less tolerant to reduced parameter precision:

*   normalization layers (e.g. LayerNorm / RMSNorm statistics),
*   attention softmax/logits paths,
*   RoPE,
*   optimizers‚Äô internal state / update dynamics.

[![Image 70: image](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/b2tC-L9u5NxU_PSvPg8DJ.png)](https://cdn-uploads.huggingface.co/production/uploads/68e529af513e724edd8702f6/b2tC-L9u5NxU_PSvPg8DJ.png)

| Method | FID ‚Üì | CMMD ‚Üì | DINO-MMD ‚Üì |
| --- | --- | --- | --- |
| Baseline | 18.20 | 0.41 | 0.39 |
| BF16 weights (bug) | 21.87 | 0.61 | 0.57 |

So the rule we now follow very strictly is: use BF16 autocast for compute, but keep weights (and optimizer state) in FP32 or at least ensure numerically sensitive modules stay FP32.

It is not a glamorous trick but it is exactly the kind of ‚Äúsilent‚Äù detail that can cost you multiple days of work if you do not notice it early.

[](http://huggingface.co/blog/Photoroom/prx-part2#summary) Summary
------------------------------------------------------------------

We ran a systematic set of ablations on PRX training, comparing a range of optimization, representation, efficiency, and data choices against a clean flow-matching baseline using both quality metrics and throughput.

The biggest gains came from alignment: REPA boosts early convergence (best used as a burn-in, then turned off), and better latents/tokenizers (REPA-E/FLUX2-AE) give a large jump in quality with clear speed trade-offs. Objective tweaks were mixed‚Äîcontrastive FM helped slightly, while x-prediction mattered most by enabling stable 1024¬≤ pixel training. Token routing (TREAD/SPRINT) is minor at 256¬≤ but becomes a major win at high resolution. Data and practical details also mattered: long captions are critical, synthetic vs. real data shifts texture vs. structure, small SFT adds polish, Muon helped, and BF16-stored weights quietly hurt training.

That‚Äôs it for Part 2! If you want to play with an earlier public checkpoint from this series, the **PRX-1024 T2I beta** is still available [here](https://huggingface.co/Photoroom/prx-1024-t2i-beta).

Weare really excited about what‚Äôs next: **in the coming weeks we will release the full source code of the PRX training framework**, and we will do a public **24-hour ‚Äúspeedrun‚Äù** where we combine the best ideas from this post into a single run and see how far the full recipe can go in one day.

If you made it this far, first of all thank you very much for your interest. Furthermore, we would love to have you join our [Discord](https://discord.gg/HXp7Znc3) community where we discuss PRX progress and results, along with everything related to diffusion and text-to-image models.

### Community

Edit Preview

Upload images, audio, and videos by dragging in the text input, pasting, or clicking here. 

Tap or paste here to upload images

 

 Comment 
¬∑[Sign up](http://huggingface.co/join?next=%2Fblog%2FPhotoroom%2Fprx-part2) or [log in](http://huggingface.co/login?next=%2Fblog%2FPhotoroom%2Fprx-part2) to comment

[- [x] Upvote 17](http://huggingface.co/login?next=%2Fblog%2FPhotoroom%2Fprx-part2)
*   [![Image 71](https://cdn-avatars.huggingface.co/v1/production/uploads/1608816539831-noauth.png)](http://huggingface.co/eliotandres "eliotandres")
*   [![Image 72](http://huggingface.co/avatars/80e890c0c0b3c3e2b89d0bb555d2c658.svg)](http://huggingface.co/dim "dim")
*   [![Image 73](http://huggingface.co/avatars/194c9a17b98d1c96251effd32b1e5816.svg)](http://huggingface.co/simonamaggio "simonamaggio")
*   [![Image 74](http://huggingface.co/avatars/14297005d52915ddd3f46134ac376d8c.svg)](http://huggingface.co/MatthieuT "MatthieuT")
*   [![Image 75](http://huggingface.co/avatars/c4994e23f91ce9669dd2c24cad9b4b19.svg)](http://huggingface.co/mearco "mearco")
*   [![Image 76](http://huggingface.co/avatars/364aa363f427c1674f92dd2bfc6e08ab.svg)](http://huggingface.co/David-PHR "David-PHR")
*   [![Image 77](https://cdn-avatars.huggingface.co/v1/production/uploads/6340651b388c3fa40f9a5bc0/av1C4_S7bHGxAzOu8lOmG.jpeg)](http://huggingface.co/adamm-hf "adamm-hf")
*   [![Image 78](https://cdn-avatars.huggingface.co/v1/production/uploads/6565ba6d66a4bce451a4440b/2T0laYv_7aP_TtZBJf8nd.png)](http://huggingface.co/aurelhubert "aurelhubert")
*   [![Image 79](https://cdn-avatars.huggingface.co/v1/production/uploads/6565baaa02ad0b04a96e900a/9jkyv2Y--IOZyoc935JHK.jpeg)](http://huggingface.co/eliamaino "eliamaino")
*   [![Image 80](http://huggingface.co/avatars/f947ec9fe620ae4cffa83b371acdd571.svg)](http://huggingface.co/natalie5 "natalie5")
*   [![Image 81](https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AQhtcAnR6yOoffCBqqCWT.jpeg)](http://huggingface.co/v-pradeilles "v-pradeilles")
*   [![Image 82](http://huggingface.co/avatars/234238fe6fde5a92a8d761663df60a2e.svg)](http://huggingface.co/thomasbphoto "thomasbphoto")
*   +5

 System theme 

Company

[TOS](http://huggingface.co/terms-of-service)[Privacy](http://huggingface.co/privacy)[About](http://huggingface.co/huggingface)[Careers](https://apply.workable.com/huggingface/)[](http://huggingface.co/)

Website

[Models](http://huggingface.co/models)[Datasets](http://huggingface.co/datasets)[Spaces](http://huggingface.co/spaces)[Pricing](http://huggingface.co/pricing)[Docs](http://huggingface.co/docs)

---
*Ëá™Âä®ÈááÈõÜ‰∫é 2026-02-03*
