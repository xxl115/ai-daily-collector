---
title: "China Moon Mission: Aiming for 2030 Lunar Landing"
url: "https://spectrum.ieee.org/china-moon-mission-mengzhou-artemis"
source: "Hacker News"
date: "2026-02-03"
score: "65"
author: "rbanffy"
---

# China Moon Mission: Aiming for 2030 Lunar Landing

**来源**: [Hacker News](https://news.ycombinator.com/item?id=46876047) | **评分**: 65 | **作者**: @rbanffy

## 原文内容

Title: China Moon Mission: Aiming for 2030 Lunar Landing

URL Source: http://spectrum.ieee.org/china-moon-mission-mengzhou-artemis

Published Time: 2026-02-02T20:44:01Z

Markdown Content:
China Moon Mission: Aiming for 2030 Lunar Landing - IEEE Spectrum
===============

Opens in a new window Opens an external website Opens an external website in a new window

This website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising. [Privacy Policy](https://www.ieee.org/security-privacy.html)

Accept Deny Non-Essential Manage Preferences 

[IEEE.org](https://www.ieee.org/)[IEEE Xplore Digital Library](https://ieeexplore.ieee.org/Xplore/home.jsp)[IEEE Standards](https://standards.ieee.org/)[More Sites](https://www.ieee.org/sitemap.html)

[Sign In](https://www.ieee.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&sourceCode=spectrum&signinurl=https%3A%2F%2Fspectrum.ieee.org%2Fcore%2Fsaml%2Fmain%2Flogin%3Fnext_url%3Dhttps%3A%2F%2Fspectrum.ieee.org%2Fcore%2Fintegrations%2Fieee%2Fchanges%3Fredirect%3Dhttps%3A%2F%2Fspectrum.ieee.org%2Fchina-moon-mission-mengzhou-artemis&url=https%3A%2F%2Fspectrum.ieee.org%2Fcore%2Fsaml%2Fmain%2Flogin%3Fnext_url%3Dhttps%3A%2F%2Fspectrum.ieee.org%2Fcore%2Fintegrations%2Fieee%2Fchanges%3Fredirect%3Dhttps%3A%2F%2Fspectrum.ieee.org%2Fchina-moon-mission-mengzhou-artemis&autoSignin=Y&car=IEEE-Spectrum)[Join IEEE](http://spectrum.ieee.org/join)

The February issue of _IEEE Spectrum_ is here!

[Download PDF↓](http://spectrum.ieee.org/china-moon-mission-mengzhou-artemis#)

[Close bar](javascript:void(0);)

[](http://spectrum.ieee.org/ "Return to homepage")

 NASA’s Rivalry/Not-Rivalry With China’s Space Agency Takes Off 

Share

FOR THE TECHNOLOGY INSIDER

Search:  

Explore by topic

[Aerospace](http://spectrum.ieee.org/topic/aerospace/)[AI](http://spectrum.ieee.org/topic/artificial-intelligence/)[Biomedical](http://spectrum.ieee.org/topic/biomedical/)[Climate Tech](http://spectrum.ieee.org/topic/climate-tech/)[Computing](http://spectrum.ieee.org/topic/computing/)[Consumer Electronics](http://spectrum.ieee.org/topic/consumer-electronics/)[Energy](http://spectrum.ieee.org/topic/energy/)[History of Technology](http://spectrum.ieee.org/topic/tech-history/)[Robotics](http://spectrum.ieee.org/topic/robotics/)[Semiconductors](http://spectrum.ieee.org/topic/semiconductors/)[Telecommunications](http://spectrum.ieee.org/topic/telecommunications/)[Transportation](http://spectrum.ieee.org/topic/transportation/)

[IEEE Spectrum](http://spectrum.ieee.org/)

FOR THE TECHNOLOGY INSIDER

### Topics

[Aerospace](http://spectrum.ieee.org/topic/aerospace/)[AI](http://spectrum.ieee.org/topic/artificial-intelligence/)[Biomedical](http://spectrum.ieee.org/topic/biomedical/)[Climate Tech](http://spectrum.ieee.org/topic/climate-tech/)[Computing](http://spectrum.ieee.org/topic/computing/)[Consumer Electronics](http://spectrum.ieee.org/topic/consumer-electronics/)[Energy](http://spectrum.ieee.org/topic/energy/)[History of Technology](http://spectrum.ieee.org/topic/tech-history/)[Robotics](http://spectrum.ieee.org/topic/robotics/)[Semiconductors](http://spectrum.ieee.org/topic/semiconductors/)[Telecommunications](http://spectrum.ieee.org/topic/telecommunications/)[Transportation](http://spectrum.ieee.org/topic/transportation/)

### Sections

[Features](http://spectrum.ieee.org/type/feature/)[News](http://spectrum.ieee.org/type/news/)[Opinion](http://spectrum.ieee.org/type/opinion/)[Careers](http://spectrum.ieee.org/topic/careers/)[DIY](http://spectrum.ieee.org/topic/diy/)[Engineering Resources](http://spectrum.ieee.org/engineering-resources/)

### More

[Newsletters](http://spectrum.ieee.org/newsletters/)[Special Reports](http://spectrum.ieee.org/special-reports/)[Collections](http://spectrum.ieee.org/collections/)[Explainers](http://spectrum.ieee.org/type/explainer/)[Top Programming Languages](http://spectrum.ieee.org/top-programming-languages)[Robots Guide ↗](https://robotsguide.com/)[IEEE Job Site ↗](https://jobs.ieee.org/)

### For IEEE Members

[Current Issue](http://spectrum.ieee.org/magazine/current-issue)[Magazine Archive](http://spectrum.ieee.org/magazine/)[The Institute](http://spectrum.ieee.org/the-institute/)[The Institute Archive](http://spectrum.ieee.org/the-institute/ti-archive/)

### For IEEE Members

[Current Issue](http://spectrum.ieee.org/magazine/current-issue)[Magazine Archive](http://spectrum.ieee.org/magazine/)[The Institute](http://spectrum.ieee.org/the-institute/)[The Institute Archive](http://spectrum.ieee.org/the-institute/ti-archive/)

### IEEE Spectrum

[About Us](http://spectrum.ieee.org/about)[Contact Us](http://spectrum.ieee.org/contact)[Reprints & Permissions ↗](https://www.parsintl.com/publications/ieee-media/)[Advertising ↗](https://advertise.ieee.org/)

### Follow IEEE Spectrum

[](https://twitter.com/ieeespectrum)[](https://www.facebook.com/IEEE.Spectrum)[](https://www.instagram.com/ieeespectrum/)[](https://www.threads.net/@ieeespectrum)[](https://www.linkedin.com/company/ieee-spectrum/)[](https://www.youtube.com/c/ieeespectrum)[](https://www.tiktok.com/@ieeespectrum)[](https://spectrum.ieee.org/customfeeds/feed/all-topics/rss)

### Support IEEE Spectrum

_IEEE Spectrum_ is the flagship publication of the IEEE — the world’s largest professional organization devoted to engineering and applied sciences. Our articles, videos, and infographics inform our readers about developments in technology, engineering, and science.

[Subscribe](https://ieee.dragonforms.com/spectrum_subscribe?utm_source=ieee-spectrum&utm_medium=nav&utm_content=hugemenu)

[About IEEE](https://www.ieee.org/about/)[Contact & Support](https://www.ieee.org/about/contact.html)[Accessibility](https://www.ieee.org/accessibility-statement.html)[Nondiscrimination Policy](https://www.ieee.org/about/corporate/governance/p9-26.html)[Terms](https://www.ieee.org/about/help/site-terms-conditions.html)[IEEE Privacy Policy](https://www.ieee.org/security-privacy.html)[Cookie Preferences](http://spectrum.ieee.org/china-moon-mission-mengzhou-artemis#)[Ad Privacy Options](https://spectrum.ieee.org/st/ppid-info)

© Copyright 2026 IEEE — All rights reserved. A public charity, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

[](javascript:;)
Enjoy more free content and benefits by creating an account
-----------------------------------------------------------

Saving articles to read later requires an IEEE Spectrum account
---------------------------------------------------------------

The Institute content is only available for members
---------------------------------------------------

Downloading full PDF issues is exclusive for IEEE Members
---------------------------------------------------------

Downloading this e-book is exclusive for IEEE Members
-----------------------------------------------------

Access to _Spectrum_ 's Digital Edition is exclusive for IEEE Members
---------------------------------------------------------------------

Following topics is a feature exclusive for IEEE Members
--------------------------------------------------------

Adding your response to an article requires an IEEE Spectrum account
--------------------------------------------------------------------

Create an account to access more content and features on _IEEE Spectrum_ , including the ability to save articles to read later, download Spectrum Collections, and participate in conversations with readers and editors. For more exclusive content and features, consider [Joining IEEE](http://spectrum.ieee.org/join) .
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Join the world’s largest professional organization devoted to engineering and applied sciences and get access to all of Spectrum’s articles, archives, PDF downloads, and other benefits. [Learn more about IEEE →](http://spectrum.ieee.org/join)
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Join the world’s largest professional organization devoted to engineering and applied sciences and get access to this e-book plus all of _IEEE Spectrum’s_ articles, archives, PDF downloads, and other benefits. [Learn more about IEEE →](http://spectrum.ieee.org/join)
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

[CREATE AN ACCOUNT](https://www.ieee.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&sourceCode=spectrum&signinurl=https%3A%2F%2Fspectrum.ieee.org%2Fcore%2Fsaml%2Fmain%2Flogin%3Fnext_url%3Dhttps%3A%2F%2Fspectrum.ieee.org%2Fcore%2Fintegrations%2Fieee%2Fchanges%0A&url=https://spectrum.ieee.org/&autoSignin=Y&car=IEEE-Spectrum)[SIGN IN](http://spectrum.ieee.org/core/saml/main/login?next_url=https://spectrum.ieee.org/core/integrations/ieee/changes)

[JOIN IEEE](https://www.ieee.org/membership-application/public/join.html?promo=JOINLITE&style=SPECTRUM&joinlite=TRUE)[SIGN IN](http://spectrum.ieee.org/core/saml/main/login?next_url=https://spectrum.ieee.org/core/integrations/ieee/changes)

[Close](javascript:;)
Access Thousands of Articles — Completely Free
----------------------------------------------

Create an account and get exclusive content and features: **Save articles, download collections,** and **post comments** — all free! For full access and benefits, [subscribe](https://ieee.dragonforms.com/spectrum_subscribe) to _Spectrum_.
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

[CREATE AN ACCOUNT](https://www.ieee.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&sourceCode=spectrum3c&signinurl=https%3A%2F%2Fspectrum.ieee.org%2Fcore%2Fsaml%2Fmain%2Flogin%3Fnext_url%3Dhttps%3A%2F%2Fspectrum.ieee.org%2Fcore%2Fintegrations%2Fieee%2Fchanges%0A&url=https://spectrum.ieee.org/&autoSignin=Y&car=IEEE-Spectrum)[SIGN IN](http://spectrum.ieee.org/core/saml/main/login?next_url=https://spectrum.ieee.org/core/integrations/ieee/changes)

[Aerospace](https://spectrum.ieee.org/topic/aerospace/)[News](https://spectrum.ieee.org/type/news/)

NASA’s Rivalry/Not-Rivalry With China’s Space Agency Takes Off
==============================================================

China’s Mengzhou is putting pressure on NASA’s lunar time frame
---------------------------------------------------------------

[Ned Potter](https://spectrum.ieee.org/u/ned-potter)

02 Feb 2026

5 min read

[](javascript:;) 1 

![Image 2: A lunar lander during takeoff at a test site. ](https://spectrum.ieee.org/media-library/a-lunar-lander-during-takeoff-at-a-test-site.jpg?id=63413010&width=980&quality=85)

The Lanyue lunar lander undergoes an engine test in 2025 in China’s Hebei province.

 Zhang Bin/Xinhua/Getty Images 

[](mailto:?subject=NASA%E2%80%99s%20Rivalry/Not-Rivalry%20With%20China%E2%80%99s%20Space%20Agency%20Takes%20Off&body=https://spectrum.ieee.org/china-moon-mission-mengzhou-artemis)

[space-race](https://spectrum.ieee.org/tag/space-race)[artemis](https://spectrum.ieee.org/tag/artemis)[nasa](https://spectrum.ieee.org/tag/nasa)[moon](https://spectrum.ieee.org/tag/moon)[china](https://spectrum.ieee.org/tag/china)

Slow and steady wins the race, or so goes the [fable](https://read.gov/aesop/025.html). The [China Manned Space Agency](https://en.cmse.gov.cn/), or CMSA, has repeatedly denied any rivalry with the [United States](https://spectrum.ieee.org/tag/united-states) akin to the race to the [moon](https://spectrum.ieee.org/tag/moon) in the 1960s. But step-by-step, one element at a time over a period of decades, it has built a [human space program](https://spectrum.ieee.org/china-moon-mission-artemis) with goals that include landing [astronauts on the moon](https://spectrum.ieee.org/china-aims-for-a-permanent-moon-base-in-the-2030s) by [2030](https://global.chinadaily.com.cn/a/202503/03/WS67c55e07a310c240449d8414.html#:~:text=The%20United%20States%20successfully%20made,at%20this%20year's%20two%20sessions?) and starting a base there in the following years. And—partly because launch dates for NASA’s [Artemis](https://spectrum.ieee.org/tag/artemis) III moon landing keep slipping toward that same time frame—U.S. space leaders are ratcheting up the [space race](https://spectrum.ieee.org/tag/space-race) rhetoric.

“We are in a great competition with a rival that has the will and means to challenge American exceptionalism across multiple domains, including in the high ground of space,” said [Jared Isaacman](https://www.nasa.gov/people/jared-isaacman/), the new head of [NASA](https://spectrum.ieee.org/tag/nasa), in December. “This is not the time for delay, but for action, because if we fall behind—if we make a mistake—we may never catch up, and the consequences could shift the balance of power here on Earth.”

NASA’s [Artemis II](https://spectrum.ieee.org/artemis-ii-launch-nasa-orion) is almost ready to take its crew on a circumlunar test flight, and the [White House](https://spectrum.ieee.org/tag/white-house) has ordered that U.S. [astronauts](https://spectrum.ieee.org/tag/astronauts) should prioritize [a lunar landing by 2028](https://www.whitehouse.gov/presidential-actions/2025/12/ensuring-american-space-superiority/)—but could [China](https://spectrum.ieee.org/tag/china) slip in ahead? How would a Chinese moon flight work? Does the Chinese space program have technology that matches or beats the United States?

RELATED: [Inside the Spacecraft That Will Carry Humans Around the Moon](https://spectrum.ieee.org/artemis-ii-launch-nasa-orion)

“Nobody [in China] would argue that we are in a space race,” says [Namrata Goswami](https://www.linkedin.com/in/namratagoswami/), a professor at [Johns Hopkins University](https://sais.jhu.edu/admissions/masters-program-admissions/how-apply/us-military-and-veteran-applicants/us-space-force-schriever-west-space-scholars-program-ile-sle) who has [written](https://www.jstor.org/stable/26333878?searchText=au%3A%22Namrata+Goswami%22&searchUri=%2Faction%2FdoBasicSearch%3FQuery%3Dau%253A%2522Namrata%2BGoswami%2522%26so%3Drel&ab_segments=0%2Fbasic_phrase_search%2Fcontrol&refreqid=fastly-default%3A8879f2e0aa2cce8be1737996d10650df&seq=3)[extensively](https://www.google.com/books/edition/Scramble_for_the_Skies/3XsGEAAAQBAJ?hl=en&gbpv=1) about China’s space effort, “but they might be engaged in activity that showcases China as a space power, and they are very serious about getting somewhere first.”

What Are the Mengzhou and Lanyue Spacecraft?
--------------------------------------------

China’s [lunar hardware](https://www.cmse.gov.cn/xwzx/yzjz/202003/t20200331_45264.html) builds on existing engineering. It is based on a multipurpose crew ship called [Mengzhou](https://news.cgtn.com/news/2025-11-01/China-s-2026-space-mission-lineup-Mengzhou-1-Long-March-10A-to-debut-1HWYMJGSIkE/p.html), with capacity for [six or seven astronauts](https://www.space-agencies.com/2025/10/08/mengzhou-chinas-next-crewed-lunar-spacecraft/), though as few as three may actually fly on a trip from Earth to low lunar orbit. (China watchers dispense with the word “taikonaut” for its crew members, by the way; [the word was coined in 1998](https://science.thewire.in/the-sciences/infinite-in-all-directions-a-science-workshop-and-why-vyomanaut-is-not-cool/) and has not been used by the Chinese government itself. China generally uses the word [_\_yuhangyuan\__](https://www.oxfordreference.com/display/10.1093/oi/authority.20110803101916587), roughly translated as “traveler of the universe.”)

Mengzhou, according to what the CMSA has shown, includes a crew section in the shape of a truncated cone or [frustum](https://mathworld.wolfram.com/Frustum.html), with a [service module](https://www.universetoday.com/articles/china-names-its-capsule-and-lander-for-its-upcoming-human-lunar-missions#:~:text=According%20to%20Chinese%20state%20media%2C%20the%20Mengzhou%20spacecraft&text=In%20addition%20to%20this%2C%20there%20will%20be,module%20that%20is%20home%20to%20power%20and) holding power and propulsion systems in the rear. If you squint at it, you’ll see a resemblance to the American [Artemis](https://images.nasa.gov/details/art001e000415) or [Apollo](https://images.nasa.gov/details/as15-88-11974) spacecraft, the [SpaceX](https://spectrum.ieee.org/tag/spacex)[Crew Dragon](https://images.nasa.gov/details/KSC-20180520-PH_SPX01_0001), or the yet-to-be-flown European [Nyx](https://spectrum.ieee.org/spacex-competitors). Basic [aerodynamics](https://spectrum.ieee.org/tag/aerodynamics) make a blunt cone a very efficient shape for safely launching a spacecraft and returning it through Earth’s atmosphere.

![Image 3: A crewed spacecraft with deployed parachutes gently descends back to Earth.](blob:http://localhost/58a11aff1ed059461dea84e1cff52165)The Mengzhou command ship uses parachutes and airbags during a 2025 landing test in northwest China.Wang Heng/Xinhua/Getty Images

Mengzhou is billed as reusable, with an outer heat shield that can be replaced after flight. Landings would take place in China’s western desert. “Coupled with the landing method of airbag cushioning,” says the CMSA in a [translated statement](https://www.cmse.gov.cn/xwzx/yzjz/202003/t20200331_45264.html), “the spacecraft itself can be better protected from damage and allow the reuse of the spacecraft.”

The ship would be launched by a new heavy-lift [Long March 10 booster](https://english.www.gov.cn/news/202508/15/content_WS689eec3dc6d0868f4e8f4dcb.html), one of two used for a given moon mission. The Long March 10, as configured for lunar flight, would stand 92.5 meters high at launch and generate thrust of 2,678 tonnes. (The [rocket for Artemis II](https://www.nasa.gov/reference/space-launch-system/) is more powerful: 3,992 tonnes.)

Mengzhou would leave for the moon after another Long March 10 has launched a [lunar landing](https://spectrum.ieee.org/tag/lunar-landing) craft called [Lanyue](https://www.chinadaily.com.cn/a/202402/24/WS65d9506aa31082fc043b8df2.html). The two would rendezvous and dock in lunar orbit. Two astronauts would transfer to Lanyue and land on the moon’s surface; Mengzhou would wait for them in orbit for the trip home. Lanyue has a stated mass of 26 tonnes and could carry a 200-kilogram rover.

Chinese authorities [say](https://www.rand.org/pubs/commentary/2025/11/china-is-going-to-the-moon-by-2030-heres-whats-known.html#:~:text=Officials%20envisage%20two%20versions%20initially,the%20lunar%20surface%20in%202030.) testing of Lanyue began in 2024. Mengzhou should go on its first robotic flight in 2026; Lanyue, in 2027. The first joint test mission is planned for 2028 or 2029, with the first crew going to the moon a year after that.

What Is China’s Long-Term Plan for Space?
-----------------------------------------

But to focus on their hardware is to miss out on a major difference between the Chinese and U.S. moon-landing efforts. Artemis is the product of a start-again stop-again debate that’s been going on in the U.S. government since [Apollo](https://spectrum.ieee.org/tag/apollo) ended in the 1970s. Goals have shifted repeatedly—often when new presidents took office. Conversely, the [Chinese campaign](https://spectrum.ieee.org/taikonauts-prepare-for-liftoff) is the outgrowth of a plan called [Project 921](https://english.cas.cn/newsroom/archive/china_archive/cn2003/200909/t20090923_40427.shtml), first backed by the Chinese Communist Party in 1992. There have been updates and some technical setbacks, but China has pretty much stuck to it ever since.

“What the Chinese space effort has done that others have not is integrate everything,” says Goswami. “It’s not just ‘We’re going to mount a mission.’ It’s bigger than that. They view space as an activity and not missions.”

In other words, she says, each new piece of technology is part of a coordinated effort to create a [sustained presence in space](https://spectrum.ieee.org/could-china-get-to-mars-first), which pays [economic, geopolitical, and sometimes military dividends](https://thediplomat.com/2021/05/china-moves-toward-a-permanent-space-presence/). Each part, so far, has fit together with other parts: The first orbiting capsule, called [Shenzhou](https://spectrum.ieee.org/tag/shenzhou) 1 in 1999, led to the first flight by an astronaut, [Yang Lewei](https://news.cgtn.com/news/2025-04-25/Yang-Liwei-China-s-first-man-in-space-1CRp6L9OiR2/p.html), on Shenzhou 5 in 2003. That led to [space stations](https://spectrum.ieee.org/tag/space-stations) (the Tiangong series, starting in 2011), to which Shenzhou crews have been flying since in regular rotation ([Shenzhou 22](https://english.news.cn/20251125/33b434deeca34129a221eb1d158ac9b8/c.html) launched in November). Mengzhou will eventually take over as the workhorse crew vehicle for Earth-orbiting flights.

In the meantime, there has been a steady cadence of robotic lunar orbiters and landers ([Chang’e-6](https://www.cnsa.gov.cn/english/n6465652/n6465653/c10573102/content.html) returned the first-ever soil sample from the moon’s far side in 2024), soon to be followed, we’re now told, by Chinese astronauts.

They started slowly, deliberately, with long breaks between missions, only recently picking up speed. At times they have unabashedly looked to other countries for guidance: The Shenzhou crew capsule in the 1990s [borrowed heavily from the design of the Russian Soyuz](https://spectrum.ieee.org/taikonauts-prepare-for-liftoff). And several engineers today point out that the Mengzhou-Lanyue plan sounds in many ways like what then-administrator Michael Griffin proposed for [NASA’s Constellation program](https://www.academia.edu/36857670/NASA_Constellation_Missions_Program_Full_) back in 2005—a crewed ship launched by one rocket, a moon lander by another, with astronauts transferring to the lander once they reach lunar orbit. A crew capsule and [lunar lander](https://spectrum.ieee.org/tag/lunar-lander) would be too much for one launch, as with the Apollo–Saturn V, because landings would be more ambitious than could be achieved with Apollo’s minimalist lunar module, with longer stays and equipment for a [lunar base](https://spectrum.ieee.org/tag/lunar-base).

“The Chinese are pursuing an architecture a lot like the Apollo architecture was. Which is understandable because their ambitions are to go fast, and Apollo worked,” says a former senior NASA manager who, like several others, asked not to be quoted by name.

“I have a lot of friends who have been watching the Chinese space program for the last couple of decades,” this person continued. “And the one hallmark that we can say is that when China announces dates for things, they typically maintain them.”

“Our Great Rival”
-----------------

And that is why Jared Isaacman talks of urgency at NASA. He has so far generally avoided the word “China” in public. The Chinese, in his words, are usually “our great rival” or “a competitor.” Some NASA [veterans](https://spectrum.ieee.org/tag/veterans) say China may turn out to be giving the agency a helpful push to be faster and more agile. They say Apollo succeeded, in large part, because of the race to beat the [Soviet Union](https://spectrum.ieee.org/tag/soviet-union). A Chinese challenge—even unstated, even illusory—may help Artemis move along.

“We have a great competitor that is moving at absolutely impressive speeds,” Isaacman [told](https://www.youtube.com/watch?v=WdQiPJ6KmRc) NASA employees, “and it’s unsettling to consider the implications if we fail to maintain our technological, scientific, or economic edge in space. And the clock is running.”

_This is part 2 of a three-part series, Back to the Moon_ _\_. [Part 1](https://spectrum.ieee.org/artemis-ii-launch-nasa-orion) is about the technology behind NASA’s Artemis II mission. Part 3 will look at how NASA reinvigorated its [human spaceflight](https://spectrum.ieee.org/tag/human-spaceflight) program.\__

From Your Site Articles

*   [The 5 Spacecraft Behind China’s Moon Rock Sample Mission ›](https://spectrum.ieee.org/china-moon-landing-uncrewed-chang-e6)
*   [China's Moon Missions Shadow NASA Artemis's Pace - IEEE ... ›](https://spectrum.ieee.org/china-moon-mission-artemis)
*   [China Aims for a Permanent Moon Base in the 2030s - IEEE Spectrum ›](https://spectrum.ieee.org/china-aims-for-a-permanent-moon-base-in-the-2030s)

Related Articles Around the Web

*   [There's a Bad Moon on the Rise: Why Congress and NASA Must ... ›](https://www.commerce.senate.gov/2025/9/there-s-a-bad-moon-on-the-rise-why-congress-and-nasa-must-thwart-china-in-the-space-race_2)

[space-race](https://spectrum.ieee.org/tag/space-race)[artemis](https://spectrum.ieee.org/tag/artemis)[nasa](https://spectrum.ieee.org/tag/nasa)[moon](https://spectrum.ieee.org/tag/moon)[china](https://spectrum.ieee.org/tag/china)

[Ned Potter](https://spectrum.ieee.org/u/ned-potter)

is a New York writer who spent more than 25 years as an ABC News and CBS News correspondent covering science, technology, space, and the environment. [See full bio→](https://spectrum.ieee.org/u/ned-potter)

[![Image 4: Circular and spiral tracks are shown as light blue lines against a darker blue background. ](blob:http://localhost/e7a03d85199d77ef45060d4d95d40126)](https://spectrum.ieee.org/particle-physics-ai)

[AI](https://spectrum.ieee.org/topic/artificial-intelligence/)[Magazine](https://spectrum.ieee.org/magazine/)[Feature](https://spectrum.ieee.org/type/feature/)

[AI Hunts for the Next Big Thing in Physics](https://spectrum.ieee.org/particle-physics-ai)
-------------------------------------------------------------------------------------------

7 hours ago

18 min read

[](javascript:;) 1 

[![Image 5: Brad Theilman leaning over a crate of neuromorphic computers in a lab.](blob:http://localhost/c4804d1e082ee1810361cf79e35b4d22)](https://spectrum.ieee.org/3d-printed-batteries)

[Transportation](https://spectrum.ieee.org/topic/transportation/)[News](https://spectrum.ieee.org/type/news/)[Consumer Electronics](https://spectrum.ieee.org/topic/consumer-electronics/)

[3D-Printed Batteries Put Power in Every Nook and Cranny](https://spectrum.ieee.org/3d-printed-batteries)
---------------------------------------------------------------------------------------------------------

9 hours ago

4 min read

[](javascript:;)

[![Image 6: Ramses Alcaide wearing over-ear headphones while concentrating on a laptop computer screen.](blob:http://localhost/f3e1cf15db87ab31986f4d80522190d9)](https://spectrum.ieee.org/ieee-safety-guidelines-neurotech)

[The Institute](https://spectrum.ieee.org/topic/the-institute/)[Careers](https://spectrum.ieee.org/topic/careers/)[Consumer Electronics](https://spectrum.ieee.org/topic/consumer-electronics/)[Article](https://spectrum.ieee.org/type/article/)

[IEEE Safety Guidelines For Neurotech Consumer Products](https://spectrum.ieee.org/ieee-safety-guidelines-neurotech)
--------------------------------------------------------------------------------------------------------------------

23 hours ago

3 min read

[](javascript:;) 1 

Related Stories
---------------

[Semiconductors](https://spectrum.ieee.org/topic/semiconductors/)[News](https://spectrum.ieee.org/type/news/)[Energy](https://spectrum.ieee.org/topic/energy/)[Consumer Electronics](https://spectrum.ieee.org/topic/consumer-electronics/)

[No Signs Yet of Gallium or Germanium Shortage](https://spectrum.ieee.org/gallium-and-germanium)
------------------------------------------------------------------------------------------------

[Aerospace](https://spectrum.ieee.org/topic/aerospace/)[News](https://spectrum.ieee.org/type/news/)

[Artemis II: The First Crewed Moon Mission in 50 Years](https://spectrum.ieee.org/artemis-ii-launch-nasa-orion)
---------------------------------------------------------------------------------------------------------------

[Aerospace](https://spectrum.ieee.org/topic/aerospace/)[Magazine](https://spectrum.ieee.org/magazine/)[Feature](https://spectrum.ieee.org/type/feature/)[February 2026](https://spectrum.ieee.org/magazine/2026/february/)

[The Quest to Build a Radio Telescope That Can Hear the Cosmic Dark Ages](https://spectrum.ieee.org/lunar-radio-telescope)
--------------------------------------------------------------------------------------------------------------------------

[Energy](https://spectrum.ieee.org/topic/energy/)[Feature](https://spectrum.ieee.org/type/feature/)

[This $5,200 Conductive Suit Makes Power Line Work Safer](https://spectrum.ieee.org/transmission-line-safety-suit)
------------------------------------------------------------------------------------------------------------------

### Anti-induction gear protects workers from deadly electric current

[Peter Fairley](https://spectrum.ieee.org/u/peter-fairley-a)

13 Jan 2026

4 min read

[](javascript:;) 4 

[](mailto:?subject=This%20%245%2C200%20Conductive%20Suit%20Could%20Make%20Power-Line%20Work%20Safer&body=https://spectrum.ieee.org/transmission-line-safety-suit)

[![Image 7: Side by side photos show a woman wearing specialized hearing aids, each with a cord coming down from it, and a close up of the device, showing electrodes embedded in it.](blob:http://localhost/537a32a8ea6e6d4dc366836300944535)](https://spectrum.ieee.org/transmission-line-safety-suit)

Specialized conductive jumpsuits made by Electrostatics of Budapest can absorb hundreds of amperes of current without combusting.

BME HVL

**In 2018, Justin Kropp** was working on a transmission circuit in Southern California when disaster struck. Grid operators had earlier shut down the 115-kilovolt circuit, but six high-voltage lines that shared the corridor were still operating, and some of their power snuck onto the deenergized wires he was working on. That rogue current shot to the ground through Kropp’s body and his elevated work platform, killing the 32-year-old father of two.

“It went in both of his hands and came out his stomach, where he was leaning against the platform rail,” says Justin’s father, [Barry Kropp](https://www.linkedin.com/in/barry-kropp-65140676/), who is himself a retired line worker. “Justin got hung up on the wire. When they finally got him on the ground, it was too late.”

Budapest-based Electrostatics makes conductive suits that protect line workers from unexpected current. Electrostatics

Justin’s accident was caused by induction: a hazard that occurs when an electric or magnetic field causes current to flow through equipment whose intended power supply has been cut off. Safety practices seek to prevent such induction shocks by grounding all conductive objects in a work zone, giving electricity alternative paths. But accidents happen. In Justin’s case, his platform unexpectedly swung into the line before it could be grounded.

Conductive Suits Protect Line Workers
-------------------------------------

Adding a layer of defense against induction injuries is the motivation behind Budapest-based [Electrostatics’](https://www.linkedin.com/company/electrostatics/?originalSubdomain=hu) specialized conductive jumpsuits, which are designed to protect against burns, cardiac fibrillation, and other ills. “If my boy had been wearing one, I know he’d be alive today,” says the elder Kropp, who purchased a line-worker safety training business after Justin’s death. The Mesa, Ariz.–based company, Electrical Safety Consulting International ([ESCI](https://esci.net/)), now distributes those suits.

![Image 8: The lower half of a man\u2019s legs clothed in pants and socks that are connected by straps ](blob:http://localhost/49685eb82c362694456863e226e93cd2)Conductive socks that are connected to the trousers complete the protective suit. BME HVL

[Eduardo Ramirez Bettoni](https://www.linkedin.com/in/eduardo-ramirez-bettoni-2070a345/), one of the developers of the suits, dug into induction risk after a series of major accidents in the [United States](https://spectrum.ieee.org/tag/united-states) in 2017 and 2018, including Justin Kropp’s. At the time, he was principal engineer for transmission and substation standards at Minneapolis-based [Xcel Energy](https://corporate.my.xcelenergy.com/s/). In talking to Xcel line workers and fellow safety engineers, he sensed that the accident cluster might be the tip of an iceberg. And when he and two industry colleagues scoured data from the U.S. Bureau of Labor Statistics, they found 81 induction accidents between 1985 and 2021 and 60 deaths, which they documented in [a 2022 report](https://ieeexplore.ieee.org/document/9925039).

“Unfortunately, it is really common. I would say there are hundreds of induction contacts every year in the United States alone,” says Ramirez Bettoni, who is now technical director of R&D for the Houston-based power-distribution equipment firm [Powell Industries](https://www.powellind.com/). He bets that such “contacts”—exposures to dangerous levels of induction—are increasing as grid operators [boost grid capacity](https://spectrum.ieee.org/grid-congestion-uk) by [squeezing additional circuits](https://spectrum.ieee.org/dynamic-line-rating-grid-congestion) into transmission corridors.

Electrostatics’ suits are an enhancement of the standard protective gear that line workers wear when their tasks involve working close to or even touching energized live lines, or “bare-hands” work. Both are interwoven with conductive materials such as stainless steel threads, which form a [Faraday cage](https://spectrum.ieee.org/tag/faraday-cage) that shields the wearer against the lines’ [electric fields](https://spectrum.ieee.org/tag/electric-fields). But the standard suits have limited capacity to shunt current because usually they don’t need to. Like a bird on a wire, bare-hands workers are electrically floating, rather than grounded, so current largely bypasses them via the line itself.

Induction Safety Suit Design
----------------------------

Backed by a US $250,000 investment from Xcel in 2019, Electrostatics adapted its standard suits by adding low-resistance conductive straps that pass current around a worker’s body. “When I’m touching a conductor with one hand and the other hand is grounded, the current will flow through the straps to get out,” says [Bálint Németh](http://linkedin.com/in/n%25C3%25A9meth-b%25C3%25A1lint-1a366586?originalSubdomain=hu), Electrostatics’ CEO and director of the [High Voltage Laboratory](https://nfl.vet.bme.hu/en/) at Budapest University of Technology and Economics.

![Image 9: A man holds one side of his jacket open revealing conductive straps inside.  ](blob:http://localhost/d824e406393fc4e8141cdc51e071ba05)A strapping system links all the elements of the suit—the jacket, trousers, gloves, and socks—and guides current through a controlled path outside the body. BME HVL

The company began selling the suits in 2023, and they have since been adopted by over a dozen transmission operators in the United States and Europe, as well as other countries including [Canada](https://spectrum.ieee.org/tag/canada), [Indonesia](https://spectrum.ieee.org/tag/indonesia), and Turkey. They cost about $5,200 in the United States.

Electrostatics’ suits had to meet a crucial design threshold: keeping body exposure below the 6-milliampere “let-go” threshold, beyond which electrocuted workers become unable to remove themselves from a circuit. “If you lose control of your muscles, you’re going to hold onto the conductor until you pass out or possibly die,” says Ramirez Bettoni.

The gear, which includes the suit, gloves, and socks, protects against 100 amperes for 10 seconds and 50 A for 30 seconds. It also has insulation to protect against heat created by high current and flame retardants to protect against electric arcs.

Kropp, Németh, and Ramirez Bettoni are hoping that developing industry standards for induction safety gear, including [ones published in October](https://knowledge.bsigroup.com/products/standard-performance-specification-for-conductive-clothing-for-industry-applications), will broaden their use. Meanwhile, the recently enacted [Justin Kropp Safety Act](https://autl.assembly.ca.gov/system/files/2025-03/ab-365-schiavo.pdf) in California, for which the elder Kropp lobbied, mandates automated defibrillators at power-line work sites.

_This article was updated on 14 January 2026._

From Your Site Articles

*   [This Low-Cost Stopgap Tech Can Fix the Grid ›](https://spectrum.ieee.org/grid-congestion-uk)
*   [A Faster, Cheaper Way to Double Power Line Capacity ›](https://spectrum.ieee.org/grid-enhancing-technologies)
*   [Power Grid Congestion Is a Problem. Here’s a Solution ›](https://spectrum.ieee.org/dynamic-line-rating-grid-congestion)

Related Articles Around the Web

*   [How Dangerous is Lineman Work | HAAS Alert ›](https://www.haasalert.com/blog/how-dangerous-is-lineman-work)

Keep Reading ↓Show less

{"imageShortcodeIds":[]}

[Energy](https://spectrum.ieee.org/topic/energy/)[Climate Tech](https://spectrum.ieee.org/topic/climate-tech/)[Sponsored Article](https://spectrum.ieee.org/type/sponsored/)

[Real-World Diagnostics and Prognostics for Grid-Connected Battery Energy Storage Systems](https://spectrum.ieee.org/sheffield-battery-energy-storage-system-research)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------

### The core challenge of clean energy

[The University of Sheffield](https://spectrum.ieee.org/u/university-of-sheffield)

[The University of Sheffield](https://sheffield.ac.uk/) is a research university in Sheffield, South Yorkshire, England, renowned for the excellence, impact and distinctiveness of its research-led learning and teaching.

12 Dec 2025

7 min read

[](javascript:;) 13 

[![Image 10: Power lines tower over a rural landscape at twilight, with pink and blue clouds in the sky.](blob:http://localhost/b10eca67b81e282785610e87cbad603d)](https://spectrum.ieee.org/sheffield-battery-energy-storage-system-research)

 The University of Sheffield 

[](mailto:?subject=Real-World%20Diagnostics%20and%20Prognostics%20for%20Grid-Connected%20Battery%20Energy%20Storage%20Systems&body=https://spectrum.ieee.org/sheffield-battery-energy-storage-system-research)

_This is a sponsored article brought to you by [The University of Sheffield](https://ad.doubleclick.net/ddm/clk/629147697;436392438;c;gdpr=%24%7BGDPR%7D;gdpr\_consent=%24%7BGDPR\_CONSENT\_755%7D)._

Across global electricity networks, the shift to renewable energy has fundamentally changed the behavior of power systems. Decades of engineering assumptions, predictable inertia, dispatchable baseload generation, and slow, well-characterized system dynamics, are now eroding as wind and solar become dominant sources of electricity. Grid operators face increasingly steep ramp events, larger frequency excursions, faster transients, and prolonged periods where fossil generation is minimal or absent.

In this environment, battery energy storage systems (BESS) have emerged as essential tools for maintaining stability. They can respond in milliseconds, deliver precise power control, and operate flexibly across a range of services. But unlike conventional generation, batteries are sensitive to operational history, thermal environment, state of charge window, system architecture, and degradation mechanisms. Their long-term behavior cannot be described by a single model or simple efficiency curve, it is the product of complex electrochemical, thermal, and control interactions.

Most laboratory tests and simulations attempt to capture these effects, but they rarely reproduce the operational irregularities of the grid. Batteries in real markets are exposed to rapid fluctuations in power demand, partial state of charge cycling, fast recovery intervals, high-rate events, and unpredictable disturbances. As [Professor Dan Gladwin](https://sheffield.ac.uk/eee/people/academic-staff/dan-gladwin), who leads Sheffield’s research into grid-connected energy storage, puts it, “you only understand how storage behaves when you expose it to the conditions it actually sees on the grid.”

This disconnect creates a fundamental challenge for the industry: How can we trust degradation models, lifetime predictions, and operational strategies if they have never been validated against genuine grid behavior?

Few research institutions have access to the infrastructure needed to answer that question. [The University of Sheffield](https://ad.doubleclick.net/ddm/clk/629147697;436392438;c;gdpr=%24%7BGDPR%7D;gdpr_consent=%24%7BGDPR_CONSENT_755%7D) is one of them.

![Image 11: A crewed spacecraft with deployed parachutes gently descends back to Earth.](blob:http://localhost/58a11aff1ed059461dea84e1cff52165)Sheffield’s Centre for Research into Electrical Energy Storage and Applications (CREESA) operates one of the UK’s only research-led, grid-connected, multi-megawatt battery energy storage testbeds. The University of Sheffield

Sheffield’s unique facility
---------------------------

The [Centre for Research into Electrical Energy Storage and Applications (CREESA)](https://sheffield.ac.uk/creesa) operates one of the UK’s only research-led, grid-connected, multi-megawatt battery energy storage testbeds. This environment enables researchers to test storage technologies not just in simulation or controlled cycling rigs, but under full-scale, live grid conditions. As Professor Gladwin notes, “we aim to bridge the gap between controlled laboratory research and the demands of real grid operation.”

At the heart of the facility is an 11 kV, 4 MW network connection that provides the electrical and operational realism required for advanced diagnostics, fault studies, control algorithm development, techno-economic analysis, and lifetime modeling. Unlike microgrid scale demonstrators or isolated laboratory benches, Sheffield’s environment allows energy storage assets to interact with the same disturbances, market signals, and grid dynamics they would experience in commercial deployment.

“The ability to test at scale, under real operational conditions, is what gives us insights that simulation alone cannot provide.” **—Professor Dan Gladwin, The University of Sheffield**

The facility includes:

*   A 2 MW / 1 MWh lithium titanate system, among the first independent grid-connected BESS of its kind in the UK
*   A 100 kW second-life EV battery platform, enabling research into reuse, repurposing, and circular-economy models
*   Support for flywheel systems, supercapacitors, hybrid architectures, and fuel-cell technologies
*   More than 150 laboratory cell-testing channels, environmental chambers, and impedance spectroscopy equipment
*   High-speed data acquisition and integrated control systems for parameter estimation, thermal analysis, and fault response measurement

The infrastructure allows Sheffield to operate storage assets directly on the live grid, where they respond to real market signals, deliver contracted power services, and experience genuine frequency deviations, voltage events, and operational disturbances. When controlled experiments are required, the same platform can replay historical grid and market signals, enabling repeatable full power testing under conditions that faithfully reflect commercial operation. This combination provides empirical data of a quality and realism rarely available outside utility-scale deployments, allowing researchers to analyse system behavior at millisecond timescales and gather data at a granularity rarely achievable in conventional laboratory environments.

According to Professor Gladwin, “the ability to test at scale, under real operational conditions, is what gives us insights that simulation alone cannot provide.”

![Image 12: Man in a suit stands in a lab with equipment and computer showing graphics.](blob:http://localhost/f0d94f4da0fd4b4dd7aef64dcf2a3e02)Dan Gladwin, Professor of Electrical and Control Systems Engineering, leads Sheffield’s research into grid-connected energy storage.The University of Sheffield

Setting the benchmark with grid scale demonstration
---------------------------------------------------

One of Sheffield’s earliest breakthroughs came with the installation of a 2 MW / 1 MWh lithium titanate demonstrator, a first-of-a-kind system installed at a time when the UK had no established standards for BESS connection, safety, or control. Professor Gladwin led the engineering, design, installation, and commissioning of the system, establishing one of the country’s first independent megawatt scale storage platforms.

The project provided deep insight into how high-power battery chemistries behave under grid stressors. Researchers observed sub-second response times and measured the system’s capability to deliver synthetic inertia-like behavior. As Gladwin reflects, “that project showed us just how fast and capable storage could be when properly integrated into the grid.”

But the demonstrator’s long-term value has been its continued operation. Over nearly a decade of research, it has served as a platform for:

*   Hybridization studies, including battery-flywheel control architectures
*   Response time optimization for new grid services
*   Operator training and market integration, exposing control rooms and traders to a live asset
*   Algorithm development, including dispatch controllers, forecasting tools, and prognostic and health management systems
*   Comparative benchmarking, such as evaluation of different lithium-ion chemistries, lead-acid systems, and second-life batteries

A recurring finding is that behavior observed on the live grid often differs significantly from what laboratory tests predict. Subtle electrical, thermal, and balance-of-plant interactions that barely register in controlled experiments can become important at megawatt-scale, especially when systems are exposed to rapid cycling, fluctuating set-points, or tightly coupled control actions. Variations in efficiency, cooling system response, and auxiliary power demand can also amplify these effects under real operating stress. As Professor Gladwin notes, “phenomena that never appear in a lab can dominate behavior at megawatt scale.”

These real-world insights feed directly into improved system design. By understanding how efficiency losses, thermal behavior, auxiliary systems, and control interactions emerge at scale, researchers can refine both the assumptions and architecture of future deployments. This closes the loop between application and design, ensuring that new storage systems can be engineered for the operational conditions they will genuinely encounter rather than idealized laboratory expectations.

Ensuring longevity with advanced diagnostics
--------------------------------------------

![Image 13: Battery testing unit with connected cables and a metal duct.](blob:http://localhost/14eb1510ba52b5f104db6b3d0d13a6ef)Sheffield’s Centre for Research into Electrical Energy Storage and Applications (CREESA) enables researchers to test storage technologies not just in simulation or controlled cycling rigs, but under full-scale, live grid conditions.The University of Sheffield

Ensuring the long-term reliability of storage requires understanding how systems age under the conditions they actually face. Sheffield’s research combines high-resolution laboratory testing with empirical data from full-scale grid-connected assets, building a comprehensive approach to diagnostics and prognostics. In Gladwin’s words, “A model is only as good as the data and conditions that shape it. To predict lifetime with confidence, we need laboratory measurements, full-scale testing, and validation under real-world operating conditions working together.”
A major focus is accurate state estimation during highly dynamic operation. Using advanced observers, Kalman filtering, and hybrid physics-ML approaches, the team has developed methods that deliver reliable SOC, SOH and SOP estimates during rapid power swings, irregular cycling, and noisy conditions where traditional methods break down.

Another key contribution is understanding cell-to-cell divergence in large strings. Sheffield’s data shows how imbalance accelerates near SOC extremes, how thermal gradients drive uneven ageing, and how current distribution causes long-term drift. These insights inform balancing strategies that improve usable capacity and safety.

Sheffield has also strengthened lifetime and degradation modeling by incorporating real grid behavior directly into the framework. By analyzing actual market signals, frequency deviations, and dispatch patterns, the team uncovers ageing mechanisms that do not appear during controlled laboratory cycling and would otherwise remain hidden.

These contributions fall into four core areas:

#### State Estimation and Parameter Identification

*   Robust SOC/SOH estimation
*   Online parameter identification for equivalent circuit models
*   Power capability prediction using transient excitation
*   Data selection strategies under noise and variability

#### Degradation and Lifetime Modelling

*   Degradation models built on real frequency and market data
*   Analysis of micro cycling and asymmetric duty cycles
*   Hybrid physics-ML forecasting models

#### Thermal and Imbalance Behavior

*   Characterizing thermal gradients in containerized systems
*   Understanding cell imbalance in large-scale systems
*   Mitigation strategies at the cell and module level
*   Coupled thermal-electrical behavior under fast cycling

#### Hybrid Systems and Multi-Technology Optimization

*   Battery-flywheel coordination strategies
*   Techno-economic modeling for hybrid assets
*   Dispatch optimization using evolutionary algorithms
*   Control schemes that extend lifetime and enhance service performance

Beyond grid-connected systems, Sheffield’s diagnostic methods have also proved valuable in off-grid environments. A key example is the collaboration with [MOPO](https://mopo.co/), a company deploying pay-per-swap lithium-ion battery packs in low-income communities across Sub-Saharan Africa. These batteries face deep cycling, variable user behavior, and sustained high temperatures, all without active cooling or controlled environments. The team’s techniques in cell characterization, parameter estimation, and in-situ health tracking have helped extend the usable life of MOPO’s battery packs. “By applying our know-how, we can make these battery-swap packs clean, safe, and significantly more affordable than petrol and diesel generators for the communities that rely on them,” says Professor Gladwin.

Beyond grid-connected systems, Sheffield’s diagnostic methods have also proved valuable in off-grid environments. A key example is the collaboration with MOPO, a company deploying pay-per-swap lithium-ion battery packs in low-income communities across Sub-Saharan Africa. MOPO

Collaboration and the global future
-----------------------------------

A defining strength of Sheffield’s approach is its close integration with industry, system operators, technology developers, and service providers. Over the past decade, its grid-connected testbed has enabled organizations to trial control algorithms, commission their first battery assets, test market participation strategies, and validate performance under real operational constraints.

These partnerships have produced practical engineering outcomes, including improved dispatch strategies, refined control architectures, validated installation and commissioning methods, and a clearer understanding of degradation under real-world market operation. According to Gladwin, “It is a two-way relationship, we bring the analytical and research tools, industry brings the operational context and scale.”

![Image 14: A crewed spacecraft with deployed parachutes gently descends back to Earth.](blob:http://localhost/58a11aff1ed059461dea84e1cff52165)One of Sheffield’s earliest breakthroughs came with the installation of a 2 MW / 1 MWh lithium titanate demonstrator. Professor Gladwin led the engineering, design, installation, and commissioning of the system, establishing one of UK’s first independent megawatt scale storage platforms.The University of Sheffield

This two-way exchange, combining academic insight with operational experience, ensures that Sheffield’s research remains directly relevant to modern power systems. It continues to shape best practice in lifetime modelling, hybrid system control, diagnostics, and operational optimization.

As electricity systems worldwide move toward net zero, the need for validated models, proven control algorithms, and empirical understanding will only grow. [Sheffield’s](https://ad.doubleclick.net/ddm/clk/629147697;436392438;c;gdpr=%24%7BGDPR%7D;gdpr_consent=%24%7BGDPR_CONSENT_755%7D) combination of full-scale infrastructure, long-term datasets, and collaborative research culture ensures it will remain at the forefront of developing storage technologies that perform reliably in the environments that matter most, the real world.

Keep Reading ↓Show less

[Telecommunications](https://spectrum.ieee.org/topic/telecommunications/)[Consumer Electronics](https://spectrum.ieee.org/topic/consumer-electronics/)[Whitepaper](https://spectrum.ieee.org/type/whitepaper/)

[Breaking Boundaries in Wireless Communication](https://content.knowledgehub.wiley.com/breaking-boundaries-in-wireless-communication-simulating-animated-on-body-rf-propagation/)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### Simulating Animated, On-Body RF Propagation

[Remcom](https://spectrum.ieee.org/u/remcom-)

[Remcom](https://www.remcom.com/) has been a leading provider of electromagnetic simulation software for 30 years. Our versatile tools and agile structure have made Remcom a trusted partner to EM professionals in diverse markets around the world.

5 hours ago

1 min read

[](javascript:;)

[](mailto:?subject=Breaking%20Boundaries%20in%20Wireless%20Communication&body=https://spectrum.ieee.org/breaking-boundaries-in-wireless-communication)

This paper discusses how RF propagation simulations empower engineers to test numerous real-world use cases in far less time, and at lower costs, than in situ testing alone. Learn how simulations provide a powerful visual aid and offer valuable insights to improve the performance and design of body-worn wireless devices.

[Download this free whitepaper now!](https://content.knowledgehub.wiley.com/breaking-boundaries-in-wireless-communication-simulating-animated-on-body-rf-propagation/)

[Computing](https://spectrum.ieee.org/topic/computing/)[News](https://spectrum.ieee.org/type/news/)

[Brainlike Computers Can Do Math, Too](https://spectrum.ieee.org/neuromorphic-math)
-----------------------------------------------------------------------------------

### Neuromorphic computer solves differential equations

[Katherine Bourzac](https://spectrum.ieee.org/u/katherine-bourzac)

[Katherine Bourzac](https://spectrum.ieee.org/u/katherine-bourzac) is a freelance journalist based in San Francisco, Calif. She writes about materials science, nanotechnology, energy, computing, and medicine—and about how all these fields overlap. Bourzac is a contributing editor at _Technology Review_ and a contributor at _Chemical & Engineering News_; her work can also be found in _Nature_ and _Scientific American_. She serves on the board of the Northern California chapter of the Society of Professional Journalists.

02 Feb 2026

3 min read

[](javascript:;) 2 

[![Image 15: Brad Theilman leaning over a crate of neuromorphic computers in a lab.](blob:http://localhost/c4804d1e082ee1810361cf79e35b4d22)](https://spectrum.ieee.org/neuromorphic-math)

Brad Theilman, a computational neuroscientist at Sandia National Laboratories, helped discover that nature-inspired, neuromorphic computers, like the one shown here, are better at solving complex math problems than previously thought.

 Craig Fritz 

[](mailto:?subject=Brainlike%20Computers%20Can%20Do%20Math%2C%20Too&body=https://spectrum.ieee.org/neuromorphic-math)

[sandia national laboratories](https://spectrum.ieee.org/tag/sandia-national-laboratories)[energy efficiency](https://spectrum.ieee.org/tag/energy-efficiency)[neuromorphic](https://spectrum.ieee.org/tag/neuromorphic)[computing](https://spectrum.ieee.org/tag/computing)[neuromorphic-computing](https://spectrum.ieee.org/tag/neuromorphic-computing)

Computer scientists often assume that the brain works by approximations, and therefore that computing hardware inspired by the brain won’t be as good at complex math as traditional hardware. Researchers at [Sandia National Laboratories](https://spectrum.ieee.org/tag/sandia-national-laboratories), in Albuquerque, are pushing back against this premise. In a paper published in [_Nature Machine Intelligence_](https://www.nature.com/articles/s42256-025-01143-2) last November, they show that neuromorphic hardware built by [Intel](https://spectrum.ieee.org/tag/intel) can solve differential equations using one of the most important methods in [scientific computing](https://spectrum.ieee.org/tag/scientific-computing): the finite element method.

Computing inspired by the brain
-------------------------------

[Neuromorphic computing](https://spectrum.ieee.org/tag/neuromorphic-computing) promises to be more energy efficient than conventional hardware, which uses densely packed electrical switches to add up signals. “We have made tremendous advances in AI, but people are building power plants” to make that possible, says [Sandia](https://spectrum.ieee.org/tag/sandia) computational neuroscientist [James B. (Brad) Aimone](https://www.sandia.gov/ccr/staff/james-bradley-aimone/). “Meanwhile, we’re able to have this conversation at 10 watts each,” he says, referring to the power consumption of the [human brain](https://spectrum.ieee.org/tag/human-brain).

The brain relies on relatively sparse [neurons](https://spectrum.ieee.org/tag/neurons) that communicate with pulses in spiking patterns. “Neurons receive weighted information and send a timed, all-or-nothing pulse that is transmitted to near neighbors,” Aimone says. “The dynamics of this determine the output.” There are various ways of implementing these spiking patterns in hardware. Some systems use analog elements; others, like Intel’s [Loihi 2](https://spectrum.ieee.org/neuromorphic-computing-with-lohi2), replicate these spiking neurons in digital circuits built with conventional manufacturing methods. Loihi 2 has over 1 billion of these digital neurons, a similar number to those found in the brains of small mammals and birds.

Typically, people working in [neuromorphic computing](https://spectrum.ieee.org/neuromorphic-computing-2671121824) focus on applications they assume are brainlike, such as processing real-time sensor data, says Aimone, whose background is in [neuroscience](https://spectrum.ieee.org/tag/neuroscience). He says this may be underestimating the capabilities of the brain, and that there’s no reason to assume this hardware isn’t suited to conventional [high-performance computing](https://spectrum.ieee.org/tag/high-performance-computing) tasks like the finite element method.

The finite element method, or FEM, is widely used to solve problems in fluid dynamics, mechanics, and electromagnetics—how materials fail, how [Wi-Fi](https://spectrum.ieee.org/tag/wi-fi) signals travel through a building. It’s a way to solve differential equations related to “any physical problem that’s distributed over time and space,” says [Bradley Theilman](https://www.sandia.gov/ccr/staff/brad-theilman/), a computational neuroscientist at Sandia.

The inspiring monkey brain
--------------------------

Theilman and Aimone assert that the brain routinely solves similar problems. Hitting a [baseball](https://spectrum.ieee.org/tag/baseball), for instance, is a complex physical problem that requires processing information that’s changing over time—following the arc of the ball and planning how to move the bat. “It’s a complex problem. The brain is controlling muscles in response to real-time information to make contact with the ball,” Theilman says.

Their research on the finite element method was inspired by a computational model of the motor cortex in a monkey. A table of numbers in this model, called a matrix, reminded Theilman of matrices used in the finite element method. The Sandia team translated the FEM to this motor cortex model, implemented it on Loihi 2, and showed that it can solve partial differential equations.

![Image 16: A crewed spacecraft with deployed parachutes gently descends back to Earth.](blob:http://localhost/58a11aff1ed059461dea84e1cff52165)Brad Theilman [center] and Felix Wang [in [deep blue](https://spectrum.ieee.org/tag/deep-blue) hoodie] unpack a neuromorphic computing core at Sandia National Laboratories.Craig Fritz

Thielman says their projections suggest that doing FEM on neuromorphic hardware may offer a slight energy advantage over traditional systems, but since they are using non-standardized research hardware it’s challenging to make a realistic comparison based on the results they’ve published so far. The team is currently working on adapting other larger problems to the neuromorphic hardware, and they expect there may be clearer energy advantages on larger scales. Standard computational methods have been optimized for traditional hardware, and vice versa, says Aimone. It will take time to show [energy efficiency](https://spectrum.ieee.org/tag/energy-efficiency) advantages.

[Steve Furber](https://spectrum.ieee.org/neuromorphic-computing-2671121824), a computer scientist emeritus at the University of Manchester, in England, says this builds on the Sandia team’s [previous work](https://www.nature.com/articles/s41928-021-00705-7) implementing another method for solving differential equations, the [Monte Carlo](https://spectrum.ieee.org/tag/monte-carlo) method, on neuromorphic hardware. He says Loihi 2 is well suited for solving these sorts of problems, and should be efficient at them. Furber’s team has used its Arm-based [SpiNNaker](https://spectrum.ieee.org/tag/spinnaker) hardware for modeling heat diffusion, which is a similar problem.

 Aimone says these advances show that neuromorphic computing has broader potential than computer scientists have appreciated. “It’s worth looking deeply at any kind of mathematical problem,” he says. “There’s no reason to assume you can’t do something in neuromorphic computing.”

From Your Site Articles

*   [Brain-Inspired Chips Good for More than AI, Study Says ›](https://spectrum.ieee.org/neuromorphic-computing-more-than-ai)
*   [Neuromorphic Camera Helps Drones Navigate Without GPS ›](https://spectrum.ieee.org/drone-gps-alternatives)
*   [How Neuromorphic Image Sensors Steal Tricks From the Human ... ›](https://spectrum.ieee.org/how-neuromorphic-image-sensors-steal-tricks-from-the-human-eye)
*   [IBM Debuts Brain-Inspired Chip For Speedy, Efficient AI - IEEE ... ›](https://spectrum.ieee.org/neuromorphic-computing-ibm-northpole)
*   [Neuromorphic Computing Is Ready for the Big Time - IEEE Spectrum ›](https://spectrum.ieee.org/neuromorphic-computing-2671121824)

Related Articles Around the Web

*   [Neuromorphic computing: the future of AI | LANL ›](https://www.lanl.gov/media/publications/1663/1269-neuromorphic-computing)
*   [Neuromorphic Computing and Engineering with AI | Intel® ›](https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html)

Keep Reading ↓Show less

Get the latest technology news in your inbox
--------------------------------------------

Subscribe to IEEE Spectrum’s newsletters by selecting from the list.

- [x] Tech Alert 

- [x] AI Alert 

- [x] Climate Tech Alert 

- [x] Career Alert 

- [x] Robotics News 

- [x] The Future Lane 

- [x] University Spotlight 

- [x] Product Spotlight 

Email input 

- [x]  I agree to the [IEEE Privacy Policy](https://www.ieee.org/security-privacy.html) 

 Please select one or more newsletters, enter a valid email address and accept Privacy Policy 

[Subscribe](http://spectrum.ieee.org/china-moon-mission-mengzhou-artemis)

[Close](javascript:;)

You have succesfully subscribed to the newsletters below:
---------------------------------------------------------

Thank your for your subscription.
---------------------------------

[Biomedical](https://spectrum.ieee.org/topic/biomedical/)[Magazine](https://spectrum.ieee.org/magazine/)[Feature](https://spectrum.ieee.org/type/feature/)[Consumer Electronics](https://spectrum.ieee.org/topic/consumer-electronics/)[February 2026](https://spectrum.ieee.org/magazine/2026/february/)

[These Hearing Aids Will Tune in to Your Brain](https://spectrum.ieee.org/hearing-aids-biosignals)
--------------------------------------------------------------------------------------------------

### Tracking brain waves and eye signals could cut through the noise

[Shruthi Raghavendra](https://spectrum.ieee.org/u/shruthi-raghavendra)

07 Jan 2026

10 min read

[](javascript:;) 11 

[](mailto:?subject=These%20Hearing%20Aids%20Will%20Tune%20in%20to%20Your%20Brain&body=https://spectrum.ieee.org/hearing-aids-biosignals)

[![Image 17: Side by side photos show a woman wearing specialized hearing aids, each with a cord coming down from it, and a close up of the device, showing electrodes embedded in it.](blob:http://localhost/537a32a8ea6e6d4dc366836300944535)](https://spectrum.ieee.org/hearing-aids-biosignals)

These custom-molded hearing aids, created by Aarhus University’s Center for Ear-EEG, use electrodes to measure tiny voltage changes on the surface of the skin inside the ear.

 Center for Ear-EEG/ECE/Aarhus University 

**Imagine you’re at a** bustling dinner party filled with laughter, music, and clinking silverware. You’re trying to follow a conversation across the table, but every word feels like it’s wrapped in noise. For most people, these types of party scenarios, where it’s difficult to filter out extraneous sounds and focus on a single source, are an occasional annoyance. For millions with [hearing loss](https://spectrum.ieee.org/tag/hearing-loss), they’re a daily challenge—and not just in busy settings.

Today’s [hearing aids](https://spectrum.ieee.org/tag/hearing-aids) aren’t great at determining which sounds to amplify and which to ignore, and this often leaves users overwhelmed and fatigued. Even the routine act of conversing with a loved one during a car ride can be mentally draining, simply because the hum of the engine and road noises are magnified to create loud and constant background static that blurs speech.

In recent years, modern hearing aids have made impressive strides. They can, for example, use a technology called adaptive [beamforming](https://spectrum.ieee.org/tag/beamforming) to focus their [microphones](https://spectrum.ieee.org/tag/microphones) in the direction of a talker. Noise-reduction settings also help decrease background cacophony, and some devices even use machine-learning-based analysis, trained on uploaded data, to detect certain environments—for example a car or a party—and deploy custom settings.

That’s why I was initially surprised to find out that today’s state-of-the-art hearing aids aren’t good enough. “It’s like my ears work but my brain is tired,” I remember one elderly man complaining, frustrated with the inadequacy of his cutting-edge noise-suppression hearing aids. At the time, I was a graduate student at the University of Texas at Dallas, surveying individuals with hearing loss. The man’s insight led me to a realization: Mental strain is an unaddressed frontier of hearing technology.

But what if hearing aids were more than just [amplifiers](https://spectrum.ieee.org/tag/amplifiers)? What if they were listeners too? I envision a new generation of intelligent hearing aids that not only boost sound but also read the wearer’s brain waves and other key physiological markers, enabling them to react accordingly to improve hearing and counter fatigue.

Until last spring, when I took time off to care for my child, I was a senior audio research scientist at [Harman International](https://www.harman.com/), in Los Angeles. My work combined cognitive [neuroscience](https://spectrum.ieee.org/tag/neuroscience), auditory [prosthetics](https://spectrum.ieee.org/tag/prosthetics), and the processing of biosignals, which are measurable physiological cues that reflect our mental and physical state. I’m passionate about developing [brain-computer interfaces](https://spectrum.ieee.org/tag/brain-computer-interfaces) ([BCIs](https://spectrum.ieee.org/tag/bci)) and adaptive signal-processing systems that make life easier for people with hearing loss. And I’m not alone. A number of researchers and companies are working to create smart hearing aids, and it’s likely they’ll come on the market within a decade.

Two technologies in particular are poised to revolutionize hearing aids, offering personalized, fatigue-free listening experiences: [electroencephalography](https://spectrum.ieee.org/tag/electroencephalography) (EEG), which tracks brain activity, and pupillometry, which uses eye measurements to gauge cognitive effort. These approaches might even be used to improve consumer audio devices, transforming the way we listen everywhere.

Aging Populations in a Noisy World
----------------------------------

More than [430 million people](https://www.who.int/news-room/fact-sheets/detail/deafness-and-hearing-loss) suffer from disabling hearing loss worldwide, including 34 million children, according to the [World Health Organization](https://spectrum.ieee.org/tag/world-health-organization). And the problem will likely get worse due to rising life expectancies and the fact that the world itself seems to be getting louder. By 2050, an estimated [2.5 billion people](https://pubmed.ncbi.nlm.nih.gov/33714390/) will suffer some degree of hearing loss and 700 million will require intervention. On top of that, [as many as 1.4 billion of today’s young people](https://gh.bmj.com/content/7/11/e010501)—nearly half of those aged 12 to 34—could be at risk of permanent hearing loss from listening to audio devices too loud and for too long.

Every year, close to a trillion dollars is lost globally due to unaddressed hearing loss, a trend that is also likely getting more pronounced. That doesn’t account for the significant emotional and physical toll on the hearing impaired, including isolation, loneliness, depression, shame, anxiety, sleep disturbances, and loss of balance.

![Image 18: A back view of a man's head shows a flexible pattern of lines with electrodes inside that go over his ear and extend toward the front of his face.](blob:http://localhost/11aa3ed2cec1086f597566102f550f7a)Flex-printed electrode arrays, such as these from the [Fraunhofer Institute](https://spectrum.ieee.org/tag/fraunhofer-institute) for Digital Media Technology, offer a comfortable option for collecting high-quality EEG signals. Leona Hofmann/Fraunhofer IDMT

And yet, despite widespread availability, hearing aid adoption remains low. According to a [2024 study](https://www.thelancet.com/journals/lanhl/article/PIIS2666-7568%2823%2900232-5/fulltext) published in [_\_The Lancet\__,](https://www.thelancet.com/) only about 13 percent of American adults with hearing loss regularly wear hearing aids. Key reasons for this deficiency include discomfort, stigma, cost—and, crucially, frustration with the poor performance of hearing aids in noisy environments.

Historically, hearing technology has come a long way. As early as the 13th century, people began using horns of cows and rams as “[ear trumpets](https://en.wikipedia.org/wiki/Ear_trumpet).” Commercial versions made of various materials, including brass and wood, came on the market in the early 19th century. (Beethoven, who famously began losing his hearing in his twenties, used variously shaped ear trumpets, some of which are now on display in a museum in Bonn, [Germany](https://spectrum.ieee.org/tag/germany).) But these contraptions were so bulky that users had to hold them with their hands or wear them within headbands. To avoid stigma, some even hid hearing aids inside furniture to mask their [disability](https://spectrum.ieee.org/tag/disability). In 1819, a [special acoustic chair](https://www.bbc.com/news/blogs-ouch-29896747) was designed for the king of [Portugal](https://spectrum.ieee.org/tag/portugal), featuring arms ornately carved to look like open lion mouths, which helped transmit sound to the king’s ear via speaking tubes.

Modern hearing aids came into being after the advent of electronics in the early 20th century. Early devices used [vacuum tubes](https://spectrum.ieee.org/tag/vacuum-tubes) and then [transistors](https://spectrum.ieee.org/tag/transistors) to amplify sound, shrinking over time from bulky body-worn boxes to discreet units that fit behind or inside the ear. At their core, today’s hearing aids still work on the same principle: A microphone picks up sound, a processor amplifies and shapes it to match the user’s hearing loss, and a tiny speaker delivers the adjusted sound into the ear canal.

Today’s best-in-class devices, like those from [Oticon](https://www.oticon.com/solutions/real-hearing-aids), [Phonak](https://www.phonak.com/en-us/hearing-devices/hearing-aids/audeo-lumity), and [Starkey](https://www.starkey.com/hearing-aids/genesis-artificial-intelligence-hearing-aids), have pioneered increasingly advanced technologies, including the aforementioned beamforming microphones, frequency lowering to better pick up high-pitched sounds and voices, and [machine learning](https://spectrum.ieee.org/tag/machine-learning) to recognize and adapt to specific environments. For example, the device may reduce amplification in a quiet room to avoid escalating background hums or else increase amplification in a noisy café to make speech more intelligible.

Advances in the AI technique of [deep learning](https://spectrum.ieee.org/tag/deep-learning), which relies on artificial [neural networks](https://spectrum.ieee.org/tag/neural-networks) to automatically recognize patterns, also hold enormous promise. Using context-aware [algorithms](https://spectrum.ieee.org/tag/algorithms), this technology can, for example, be used to help distinguish between speech and noise, predict and suppress unwanted clamor in real time, and attempt to clean up speech that is muffled or distorted.

The problem? As of right now, consumer systems respond only to external acoustic environments and not to the internal cognitive state of the listener—which means they act on imperfect and incomplete information. So, what if hearing aids were more empathetic? What if they could sense when the listener’s brain feels tired or overwhelmed and automatically use that feedback to deploy advanced features?

Using EEG to Augment Hearing Aids
---------------------------------

When it comes to creating intelligent hearing aids, there are two main challenges. The first is building convenient, power-efficient [wearable devices](https://spectrum.ieee.org/tag/wearable-devices) that accurately detect brain states. The second, perhaps more difficult step is decoding feedback from the brain and using that information to help hearing aids adapt in real time to the listener’s cognitive state and auditory experience.

Let’s start with [EEG](https://spectrum.ieee.org/tag/eeg). This century-old noninvasive technology uses electrodes placed on the scalp to measure the brain’s electrical activity through voltage fluctuations, which are recorded as “brain waves.”

![Image 19: A man with headphones sits in a lab in front of computers displaying information. Behind him through a doorway is seen another person sitting in front of a screen, wearing an EEG cap.](blob:http://localhost/ecff80bc7bf24894c51bbf97c2287e99)Brain-computer interfaces allow researchers to accurately determine a listener’s focus in multitalker environments. Here, professor Christopher Smalt works on an attention-decoding system at the [MIT](https://spectrum.ieee.org/tag/mit)[Lincoln Laboratory](https://spectrum.ieee.org/tag/lincoln-laboratory).MIT Lincoln Laboratory

Clinically, EEG has long been applied for diagnosing [epilepsy](https://spectrum.ieee.org/tag/epilepsy) and sleep disorders, monitoring brain injuries, assessing hearing ability in [infants](https://spectrum.ieee.org/tag/infants) and impaired individuals, and more. And while standard EEG requires conductive gel and bulky headsets, we now have versions that are far more portable and convenient. These breakthroughs have already allowed EEG to migrate from [hospitals](https://spectrum.ieee.org/tag/hospitals) into the consumer tech spaces, driving everything from neurofeedback headbands to the BCIs in gaming and [wellness apps](https://spectrum.ieee.org/tag/wellness-apps) that allow people to control devices with their minds.

The [cEEGrid project](https://uol.de/psychologie/abteilungen/ceegrid)at Oldenburg University, in Germany, positions lightweight adhesive electrodes around the ear to create a low-profile version. In [Denmark](https://spectrum.ieee.org/tag/denmark), [Aarhus University’s Center for Ear-EEG](https://ece.au.dk/en/research/research-centres/center-for-ear-eeg/) also has an ear-based EEG system designed for comfort and portability. While the signal-to-noise ratio is slightly lower compared to head-worn EEG, these ear-based systems have proven sufficiently accurate for gauging attention, listening effort, [hearing thresholds](https://ieeexplore.ieee.org/document/8006311), and [speech tracking](https://doi.org/10.1088/1741-2552/ae00f3) in real time.

For hearing aids, EEG technology can pick up brain-wave patterns that reveal how well a listener is following speech: When listeners are paying attention, their brain rhythms synchronize with the syllabic rhythms of discourse, essentially tracking the speaker’s cadence. By contrast, if the signal becomes weaker or less precise, it suggests the listener is struggling to comprehend and losing focus.

During my own [Ph.D. research](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.927872/full), I observed firsthand how real-time brain-wave patterns, picked up by EEG, can reflect the quality of a listener’s speech cognition. For example, when participants successfully homed in on a single talker in a crowded room, their neural rhythms aligned nearly perfectly with that speaker’s voice. It was as if there were a brain-based spotlight on that speaker! But when background fracas grew louder or the listener’s attention drifted, those patterns waned, revealing stress in keeping up.

Today, researchers at [Aarhus University](https://ece.au.dk/en/research/research-centres/center-for-ear-eeg/projects/investigation-of-auditory-responses-in-ear-eeg?utm_source=chatgpt.com), [Oldenburg University](https://uol.de/en/psychology/neurophysiology/forschung), and [MIT](https://www.ll.mit.edu/partner-us/available-technologies/end-end-deep-neural-network-auditory-attention-decoding?utm_source=chatgpt.com) are developing attention-decoding algorithms specifically for auditory applications. For example, Oldenburg’s cEEGrid technology has been used to [successfully identify](https://iopscience.iop.org/article/10.1088/1741-2552/aa66dd) which of two speakers a listener is trying to hear. In a [related study](https://iopscience.iop.org/article/10.1088/1741-2560/13/6/066004), researchers demonstrated that in-ear EEG can track the attended speech stream in multitalker environments.

All of this could prove transformational in creating neuroadaptive hearing aids. If a listener’s EEG reveals a drop in speech tracking, the hearing aid could infer increased listening difficulty, even if ambient noise levels have remained constant. For example, if a hearing-impaired car driver can’t focus on a conversation due to mental fatigue caused by background noise, the hearing aid could switch on beamforming to better augment the passenger’s voice, as well as machine-learning settings to deploy sound canceling that blocks the din of the road.

Of course, there are several hurdles to cross before commercialization becomes possible. For one thing, EEG-paired hearing aids will need to handle the fact that neural responses differ from person to person, which means they will likely need to be calibrated individually to capture each user’s unique brain-speech patterns.

Additionally, EEG signals are themselves notoriously “noisy,” especially in real-world environments. Luckily, we already have algorithms and processing tools for cleaning and organizing these signals so [computer models](https://spectrum.ieee.org/tag/computer-models) can search for key patterns that predict mental states, including attention drift and fatigue.

Commercial versions of EEG-paired hearing aids will also need to be small and energy-efficient when it comes to [signal processing](https://spectrum.ieee.org/tag/signal-processing) and real-time computation. And getting them to work reliably, despite head movement and daily activity, will be no small feat. Importantly, companies will need to resolve ethical and regulatory considerations, such as data ownership. To me, these challenges seem surmountable, especially with technology progressing at a rapid clip.

A Window to the Brain: Using Our Eyes to Hear
---------------------------------------------

Now let’s consider a second way of reading brain states: through the listener’s eyes.

When a person has trouble hearing and starts feeling overwhelmed, the body reacts. Heart-rate variability diminishes, indicating stress, and sweating increases. Researchers are investigating how these types of autonomic nervous-system responses can be measured and used to create smart hearing aids. For the purposes of this article, I will focus on a response that seems especially promising—namely, pupil size.

[Pupillometry](https://en.wikipedia.org/wiki/Pupillometry) is the measurement of pupil size and how it changes in response to stimuli. We all know that pupils expand or contract depending on light brightness. As it turns out, pupil size is also an accurate means of evaluating attention, arousal, mental strain—and, crucially, listening effort.

![Image 20: Three eye illustrations showing pupil size changes due to light and emotional stimuli.](blob:http://localhost/de4f5887bdeee93b6731477e339981e8)Pupil size is determined by both external stimuli, such as light, and internal stimuli, such as fatigue or excitement.Chris Philpot

In recent years, studies at [University College London](https://discovery.ucl.ac.uk/id/eprint/10173304/1/4856.full.pdf)and [Leiden University](https://pubmed.ncbi.nlm.nih.gov/29435963/), in [the Netherlands](https://spectrum.ieee.org/tag/the-netherlands),have demonstrated that pupil dilation is consistently greater in hearing-impaired individuals when processing speech in noisy conditions. Research has also shown pupillometry to be a sensitive, objective correlate of speech intelligibility and mental strain. It could therefore offer a feedback mechanism for user-aware hearing aids that dynamically adjust amplification strategies, directional focus, or [noise reduction](https://spectrum.ieee.org/tag/noise-reduction) based not just on the acoustic environment but on how hard the user is working to comprehend speech.

While more straightforward than EEG, pupillometry presents its own engineering challenges. Pupillometry requires a direct line of sight to the pupil, necessitating a stable, front-facing camera-to-eye configuration—which isn’t easy to achieve when a wearer is moving around in real-world settings. On top of that, most pupil-tracking systems require infrared illumination and high-resolution optical [cameras](https://spectrum.ieee.org/tag/cameras), which are too bulky and power intensive for the tiny housings of in-ear or behind-the-ear hearing aids. All this makes it unlikely that standalone hearing aids will include pupil-tracking hardware in the near future.

A more viable approach may be pairing hearing aids with [smart glasses](https://spectrum.ieee.org/tag/smart-glasses) or other [wearables](https://spectrum.ieee.org/tag/wearables) that contain the necessary eye-tracking hardware. Products from companies like [Tobii](https://www.tobii.com/)and [Pupil Labs](https://pupil-labs.com/) already offer real-time pupillometry via lightweight headgear for use in research, behavioral analysis, and [assistive technology](https://spectrum.ieee.org/tag/assistive-technology) for people with medical conditions that limit movement but leave eye control intact. Apple’s Vision Pro and other [augmented reality](https://spectrum.ieee.org/tag/augmented-reality) or [virtual reality](https://spectrum.ieee.org/tag/virtual-reality) platforms also include built-in eye-tracking sensors that could support pupillometry-driven adaptations for audio content.

![Image 21: A woman wears a pair of specialized glasses that have small cameras and infrared illuminators around edges of the glass for eye tracking, as well as a camera and microphone above the nose bridge.](blob:http://localhost/7389eee5a42165646a87ced51339464c)Smart glasses that measure pupil size, such as these made by Tobii, could help determine listening strain. Tobii

Once pupil data is acquired, the next step will be real-time interpretation. Here, again, is where machine learning can use large datasets to detect patterns signifying increased cognitive load or attentional shifts. For instance, if a listener’s pupils dilate unnaturally during a conversation, signifying strain, the hearing aid could automatically engage a more aggressive noise suppression mode or narrow its directional microphone beam. These types of systems can also learn from contextual features, such as time of day or prior environments, to continuously refine their response strategies.

While no commercial hearing aid currently integrates pupillometry, adjacent industries are moving quickly.[Emteq Labs](https://emteqlabs.com/) is developing “emotion-sensing” glasses that combine facial and [eye tracking](https://spectrum.ieee.org/tag/eye-tracking), along with pupil measurement, to do things like evaluate [mental health](https://spectrum.ieee.org/tag/mental-health) and capture consumer insights. Ethical controversies aside—just imagine what dystopian governments might do with emotion-reading eyewear!—such devices show that it’s feasible to embed biosignal monitoring in consumer-grade smart glasses.

A Future with Empathetic Hearing Aids
-------------------------------------

Back at the dinner party, it remains nearly impossible to participate in conversation. “Why even bother going out?” some ask. But that will soon change.

We’re at the cusp of a paradigm shift in auditory technology, from device-centered to user-centered innovation. In the next five years, we may see hybrid solutions where EEG-enabled [earbuds](https://spectrum.ieee.org/tag/earbuds) work in tandem with smart glasses. In 10 years, fully integrated biosignal-driven hearing aids could become the standard. And in 50? Perhaps audio systems will evolve into cognitive companions, devices that adjust, advise, and align with our mental state.

Personalizing hearing-assistance technology isn’t just about improving clarity; it’s also about easing mental fatigue, reducing social isolation, and empowering people to engage confidently with the world. Ultimately, it’s about restoring dignity, connection, and joy.

_This article appears in the February 2026 print issue._

From Your Site Articles

*   [Tech Gives Sound Directionality to Hearing Aids ›](https://spectrum.ieee.org/directional-hearing-aids)
*   [Deep Learning Reinvents the Hearing Aid ›](https://spectrum.ieee.org/deep-learning-reinvents-the-hearing-aid)
*   [Starkey’s AI Transforms Hearing Aids Into Smart Wearables ›](https://spectrum.ieee.org/starkeys-ai-transforms-hearing-aid-into-smart-wearables)

Related Articles Around the Web

*   [Hearing aids: How to choose the right one - Mayo Clinic ›](https://www.mayoclinic.org/diseases-conditions/hearing-loss/in-depth/hearing-aids/art-20044116)

Keep Reading ↓Show less

{"imageShortcodeIds":[]}

[AI](https://spectrum.ieee.org/topic/artificial-intelligence/)[Semiconductors](https://spectrum.ieee.org/topic/semiconductors/)[Sponsored Article](https://spectrum.ieee.org/type/sponsored/)

[From Bottleneck to Breakthrough: AI in Chip Verification](https://spectrum.ieee.org/calibre-vision-ai-chip-design)
-------------------------------------------------------------------------------------------------------------------

### How AI is transforming chip design with smarter verification methods

[Priyank Jain](https://spectrum.ieee.org/u/priyank-jain)

[Priyank Jain](https://www.linkedin.com/in/priyankjain05)leads product management for Calibre Interfaces at Siemens EDA, where he is at the forefront of innovation in semiconductor design and manufacturing and driving the AI revolution in chip debug. Most recently, he launched the groundbreaking Calibre Vision AI, transforming how engineers accelerate design closure. With over a decade of experience—including leadership roles at KLA in EUV Mask Inspection and Wafer Metrology—Priyank brings deep expertise in semiconductor technology. He holds a Master’s in System Modelling and Control from IIT Roorkee.

30 Oct 2025

8 min read

[](javascript:;) 11 

[![Image 22: Close-up of a blue circuit board featuring a large, central white microchip.](blob:http://localhost/5a6a057585f6137572947ff258527c9f)](https://spectrum.ieee.org/calibre-vision-ai-chip-design)

Using advanced machine learning algorithms, Vision AI analyzes every error to find groups with common failure causes. This means designers can attack the root cause once, fixing problems for hundreds of checks at a time instead of tediously resolving them one by one.

 Siemens 

[](mailto:?subject=From%20Bottleneck%20to%20Breakthrough%3A%20AI%20in%20Chip%20Verification&body=https://spectrum.ieee.org/calibre-vision-ai-chip-design)

_This is a sponsored article brought to you by [Siemens](https://eda.sw.siemens.com/en-US/)._

In the world of electronics, integrated circuits (IC) chips are the unseen powerhouse behind progress. Every leap—whether it’s smarter phones, more capable cars, or breakthroughs in [healthcare](https://spectrum.ieee.org/tag/healthcare) and science—relies on chips that are more complex, faster, and packed with more features than ever before. But creating these chips is not just a question of sheer engineering talent or ambition. The design process itself has reached staggering levels of complexity, and with it, the challenge to keep productivity and quality moving forward.

As we push against the boundaries of physics, chipmakers face more than just technical hurdles. The workforce challenges, tight timelines, and the requirements for building reliable chips are stricter than ever. Enormous effort goes into making sure chip layouts follow detailed constraints—such as maintaining minimum feature sizes for [transistors](https://spectrum.ieee.org/tag/transistors) and wires, keeping proper spacing between different layers like metal, polysilicon, and active areas, and ensuring vias overlap correctly to create solid electrical connections. These design rules multiply with every new technology generation. For every innovation, there’s pressure to deliver more with less. So, the question becomes: How do we help designers meet these demands, and how can technology help us handle the complexity without compromising on quality?

Shifting the paradigm: the rise of AI in [electronic design automation](https://spectrum.ieee.org/tag/electronic-design-automation)
-----------------------------------------------------------------------------------------------------------------------------------

A major wave of change is moving through the entire field of [electronic design](https://spectrum.ieee.org/tag/electronic-design) automation (EDA), the specialized area of software and tools that chipmakers use to design, analyze, and verify the complex integrated circuits inside today’s chips. [Artificial intelligence](https://spectrum.ieee.org/topic/artificial-intelligence/) is already touching many parts of the chip design flow—helping with placement and routing, predicting yield outcomes, tuning analog circuits, automating simulation, and even guiding early architecture planning. Rather than simply speeding up old steps, AI is opening doors to new ways of thinking and working.

[Machine learning](https://spectrum.ieee.org/tag/machine-learning) models can help predict defect hotspots or prioritize risky areas long before sending a chip to be manufactured.

Instead of brute-force computation or countless lines of custom code, AI uses advanced [algorithms](https://spectrum.ieee.org/tag/algorithms) to spot patterns, organize massive datasets, and highlight issues that might otherwise take weeks of manual work to uncover. For example, [generative AI](https://spectrum.ieee.org/tag/generative-ai) can help designers ask questions and get answers in natural language, streamlining routine tasks. Machine learning models can help predict defect hotspots or prioritize risky areas long before sending a chip to be manufactured.

This growing partnership between human expertise and [machine intelligence](https://spectrum.ieee.org/tag/machine-intelligence) is paving the way for what some call a “shift left” or concurrent build revolution—finding and fixing problems much earlier in the design process, before they grow into expensive setbacks. For chipmakers, this means higher quality and faster time to market. For designers, it means a chance to focus on innovation rather than chasing bugs.

![Image 23: Flow diagram: IC design rule checking (DRC), SoC integration, physical verification showing errors.](blob:http://localhost/78697e350b5b3972edf2617be15cc10f)Figure 1. Shift-left and concurrent build of IC chips performs multiple tasks simultaneously that use to be done sequentially.[Siemens](https://spectrum.ieee.org/tag/siemens)

The physical verification bottleneck: why design rule checking is harder than ever
----------------------------------------------------------------------------------

As chips grow more complex, the part of the design called physical verification becomes a critical bottleneck. Physical verification checks whether a chip layout meets the manufacturer’s strict rules and faithfully matches the original functional schematic. Its main goal is to ensure the design can be reliably manufactured into a working chip, free of physical defects that might cause failures later on.

Design rule checking (DRC) is the backbone of physical verification. DRC software scans every corner of a chip’s layout for violations—features that might cause defects, reduce yield, or simply make the design un-manufacturable. But today’s chips aren’t just bigger; they’re more intricate, woven from many layers of logic, memory, and analog components, sometimes stacked in three dimensions. The rules aren’t simple either. They may depend on the geometry, the context, the manufacturing process and even the interactions between distant layout features.

![Image 24: Man with wavy black hair in a black blazer and white shirt against a plain background.](blob:http://localhost/0cd7b496596cad3fea1c8d48b2c7e1f7)Priyank Jain leads product management for Calibre Interfaces at Siemens EDA.Siemens

Traditionally, DRC is performed late in the flow, when all components are assembled into the final chip layout. At this stage, it’s common to uncover millions of violations—and fixing these late-stage issues requires extensive effort, leading to costly delays.

To minimize this burden, there’s a growing focus on shifting DRC earlier in the flow—a strategy called “shift-left.” Instead of waiting until the entire design is complete, engineers try to identify and address DRC errors much sooner at block and cell levels. This concurrent design and verification approach allows the bulk of errors to be caught when fixes are faster and less disruptive.

However, running DRC earlier in the flow on a full chip when the blocks are not DRC clean produces results datasets of breathtaking scale—often tens of millions to billions of “errors,” warnings, or flags because the unfinished chip design is “dirty” compared to a chip that’s been through the full design process. Navigating these “dirty” results is a challenge all on its own. Designers must prioritize which issues to tackle, identify patterns that point to systematic problems, and decide what truly matters. In many cases, this work is slow and “manual,” depending on the ability of engineers to sort through data, filter what matters, and share findings across teams.

To cope, design teams have crafted ways to limit the flood of information. They might cap the number of errors per rule, or use informal shortcuts—passing [databases](https://spectrum.ieee.org/tag/databases) or screenshots by email to team members, sharing filters in chat messages, and relying on experts to know where to look. Yet this approach is not sustainable. It risks missing major, chip-wide issues that can cascade through the final product. It slows down response and makes collaboration labor-intensive.

With ongoing workforce challenges and the surging complexity of modern chips, the need for smarter, more automated DRC analysis becomes urgent. So what could a better solution look like—and how can AI help bridge the gap?

The rise of AI-powered DRC analysis
-----------------------------------

Recent breakthroughs in AI have changed the game for DRC analysis in ways that were unthinkable even a few years ago. Rather than scanning line by line or check by check, AI-powered systems can process billions of errors, cluster them into meaningful groups, and help designers find the root causes much faster. These tools use techniques from [computer vision](https://spectrum.ieee.org/tag/computer-vision), advanced machine learning, and [big data](https://spectrum.ieee.org/tag/big-data) analytics to turn what once seemed like an impossible pile of information into a roadmap for action.

AI’s ability to organize chaotic datasets—finding systematic problems hidden across multiple rules or regions—helps catch risks that basic filtering might miss. By grouping related errors and highlighting hot spots, designers can see the [big picture](https://spectrum.ieee.org/tag/big-picture) and focus their time where it counts. AI-based clustering algorithms reliably transform weeks of manual investigation into minutes of guided analysis.

AI-powered systems can process billions of errors, cluster them into meaningful groups, and help designers find the root causes much faster.

Another benefit: collaboration. By treating results as shared, living datasets—rather than static tables—modern tools let teams assign owners, annotate findings and pass exact analysis views between block and partition engineers, even across organizational boundaries. Dynamic bookmarks and shared UI states cut down on confusion and rework. Instead of “back and forth,” teams move forward together.

Many of these innovations tease at what’s possible when AI is built into the heart of the verification flow. Not only do they help designers analyze the results; they help everyone reason about the data, summarize findings and make better design decisions all the way to tape out.

A real-world breakthrough in DRC analysis and collaboration: Siemens’ Calibre Vision AI
---------------------------------------------------------------------------------------

One of the most striking examples of AI-powered DRC analysis comes from [Siemens](https://eda.sw.siemens.com/en-US/), whose [Calibre Vision AI platform](https://eda.sw.siemens.com/en-US/ic/calibre-design/interfaces/vision-ai/) is setting new standards for how full-chip verification happens. Building on years of experience in physical verification, Siemens realized that breaking bottlenecks required not only smarter algorithms but rethinking how teams work together and how data moves across the flow.

Vision AI is designed for speed and scalability. It uses a compact error database and a multi-threaded engine to load millions—or even billions—of errors in minutes, visualizing them so engineers see clusters and hot spots across the entire die. Instead of a wall of error codes or isolated rule violations, the tool presents a heat map of the layout, highlighting areas with the highest concentration of issues. By enabling or disabling layers (layout, markers, heat map) and adjusting layer opacity, users get a clear, customizable view of what’s happening—and where to look next.

Using advanced machine learning algorithms, Vision AI analyzes every error to find groups with common failure causes.

But the real magic is in AI-guided clustering. Using advanced machine learning algorithms, Vision AI analyzes every error to find groups with common failure causes. This means designers can attack the root cause once, fixing problems for hundreds of checks at a time instead of tediously resolving them one by one. In cases where legacy tools would force teams to slog through, for example, 3,400 checks with 600 million errors, Vision AI’s clustering can reduce that effort to investigating just 381 groups—turning mountains into molehills and speeding debug time by at least 2x.

![Image 25: Calibre Vision software, check groups, cells list, and die-view heatmap interface screenshot.](blob:http://localhost/0735c40f66f1d4fc0beca0976e056bcc)Figure 2. The Calibre Vision AI software automates and simplifies the chip-level DRC verification process.Siemens

Vision AI is also highly collaborative. Dynamic bookmarks capture the exact state of analysis, from layer filters to zoomed layout areas, along with annotations and owner assignments. Sharing a bookmark sends a living analysis—not just a static snapshot—to coworkers, so everyone is working from the same view. Teams can export results databases, distribute actionable groups to block owners, and seamlessly import findings into other Siemens EDA tools for further debug.

Empowering every designer: reducing the expertise gap
-----------------------------------------------------

A frequent pain point in chip verification is the need for deep expertise—knowing which errors matter, which patterns mean trouble, and how to interpret complex results. Calibre Vision AI helps level the playing field. Its AI-based algorithms consistently create the same clusters and debug paths that senior experts would identify, but does so in minutes. New users can quickly find systematic issues and perform like seasoned engineers, helping chip companies address workforce shortages and staff turnover.

Beyond clusters and bookmarks, Vision AI lets designers build custom signals by leveraging their own data. The platform secures customer models and data for exclusive use, making sure sensitive information stays within the company. And by integrating with Siemens’ EDA AI ecosystem, [Calibre Vision AI](https://eda.sw.siemens.com/en-US/ic/calibre-design/interfaces/vision-ai/) supports generative AI [chatbots](https://spectrum.ieee.org/tag/chatbots) and reasoning assistants. Designers can ask direct questions—about syntax, about a signal, about the flow—and get prompt—accurate answers, streamlining training and adoption.

Real results: speeding analysis and sharing insight
---------------------------------------------------

Customer feedback from leading IC companies shows the real-world value of AI for full-chip DRC analysis and debug. One company reported that Vision AI reduced their debug effort by at least half—a gain that makes the difference between tapeout and delay. Another noted the platform’s signals algorithm automatically creates the same check groups that experienced users would manually identify, saving not just time but energy.

Quantitative gains are dramatic. For example, Calibre Vision AI can load and visualize error files significantly faster than traditional debug flows. Figure 3 shows the difference in four different test cases: a results file that took 350 minutes with the traditional flow, took Calibre Vision AI only 31 minutes. In another test case (not shown), it took just five minutes to analyze and cluster 3.2 billion errors from more than 380 rule checks into 17 meaningful groups. Instead of getting lost in gigabytes of error data, designers now spend time solving real problems.

![Image 26: Bar graph comparing traditional flow vs. Vision AI flow times at various nanometer scales.](blob:http://localhost/fbdc3de23d73ae9f94a1c1b23f77a2db)Figure 3. Charting the results load time between the traditional DRC debug flow and the Calibre Vision AI flow.Siemens

Looking ahead: the future of AI in chip design
----------------------------------------------

Today’s chips demand more than incremental improvements in EDA software. As the need for speed, quality and collaboration continues to grow, the story of physical verification will be shaped by smarter, more adaptive technologies. With AI-powered DRC analysis, we see a clear path: a faster and more productive way to find systematic issues, intelligent debug, stronger collaboration and the chance for every designer to make an expert impact.

By combining the creativity of engineers with the speed and insight of AI, platforms like [Calibre Vision AI](https://eda.sw.siemens.com/en-US/ic/calibre-design/interfaces/vision-ai/) are driving a new productivity curve in full-chip analysis. With these tools, teams don’t just keep up with complexity—they turn it into a competitive advantage.

At Siemens, the future of chip verification is already taking shape—where intelligence works hand in hand with intuition, and new ideas find their way to silicon faster than ever before. As the industry continues to push boundaries and unlock the next generation of devices, AI will help chip design reach new heights.

_For more on Calibre Vision AI and how Siemens is shaping the future of chip design, visit [eda.sw.siemens.com](https://eda.sw.siemens.com/en-US/) and search for Calibre Vision AI._

Keep Reading ↓Show less

[AI](https://spectrum.ieee.org/topic/artificial-intelligence/)[Biomedical](https://spectrum.ieee.org/topic/biomedical/)[Whitepaper](https://spectrum.ieee.org/type/whitepaper/)

[How AI Accelerates PMUT Design for Biomedical Ultrasonic Applications](https://content.knowledgehub.wiley.com/quanscient-multiphysicsai-for-pmut-design/)
----------------------------------------------------------------------------------------------------------------------------------------------------------

### Neural networks trained on 10,000 FEM simulations deliver reliable performance forecasts.

[Quanscient](https://spectrum.ieee.org/u/quanscient-)

08 Jan 2026

1 min read

[](javascript:;) 2 

[](mailto:?subject=How%20AI%20Accelerates%20PMUT%20Design%20for%20Biomedical%20Ultrasonic%20Applications&body=https://spectrum.ieee.org/how-ai-accelerates-pmut-design-for-biomedical-ultrasonic-applications)

This whitepaper provides [MEMS](https://spectrum.ieee.org/tag/mems) engineers, biomedical device developers, and [multiphysics](https://spectrum.ieee.org/tag/multiphysics) simulation specialists with a practical AI-accelerated workflow for optimizing piezoelectric micromachined [ultrasonic](https://spectrum.ieee.org/tag/ultrasonic) transducers (PMUTs), enabling you to explore complex design trade-offs between sensitivity and bandwidth while achieving validated performance improvements in minutes instead of days using standard cloud infrastructure.

**What you will learn about:**

*   MultiphysicsAI combines cloud-based FEM simulation with neural surrogates to transform PMUT design from trial-and-error iteration into systematic inverse optimization
*   Training on 10,000 randomized geometries produces AI surrogates with 1% mean error and sub-millisecond inference for key performance indicators: transmit sensitivity, center frequency, fractional bandwidth, and electrical impedance
*   Pareto front optimization simultaneously increases fractional bandwidth from 65% to 100% and improves sensitivity by 2-3 dB while maintaining 12 MHz center frequency within ±0.2%

[Download this free whitepaper now!](https://content.knowledgehub.wiley.com/quanscient-multiphysicsai-for-pmut-design/)

Keep Reading ↓Show less

[AI](https://spectrum.ieee.org/topic/artificial-intelligence/)[Guest Article](https://spectrum.ieee.org/type/guest-article/)

[Don’t Regulate AI Models. Regulate AI Use](https://spectrum.ieee.org/ai-model-regulation)
------------------------------------------------------------------------------------------

### Rather than play catch-up, the U.S. can adapt foreign AI rules

[John deVadoss](https://spectrum.ieee.org/u/john-devadoss)

[John deVadoss](https://www.linkedin.com/in/johnkdevadoss/) is co-founder of NeuralFabric, since [acquired by Cisco](https://www.cisco.com/site/us/en/about/corporate-development/acquisitions/neuralfabric/index.html#tabs-9da71fbd27-item-1288c79d71-tab), where he pioneered domain-specific foundation models. He is co-founder of the [InterWork Alliance](https://www.gbbc.io/interwork-alliance), where he built the first Token Taxonomy Framework for Digital Assets. He [serves on the Board](https://www.gbbc.io/about/john-devadoss) of the Global Blockchain Business Council. He was a General Manager at Microsoft where he pioneered SOA, built .NET Architecture, the Application Platform, patterns & practices, Microsoft Digital and more. His Ph.D. work was in AI at the University of Massachusetts, Amherst.

02 Feb 2026

5 min read

[](javascript:;) 2 

[![Image 27: Silhouettes looking at screens sit behind a building shape with the scales of justice in a digital, grid-patterned setting.](blob:http://localhost/8cd62dc942140eef39eff751a6011c09)](https://spectrum.ieee.org/ai-model-regulation)

 Getty Images 

[](mailto:?subject=Don%E2%80%99t%20Regulate%20AI%20Models.%20Regulate%20AI%20Use&body=https://spectrum.ieee.org/ai-model-regulation)

[ai regulation](https://spectrum.ieee.org/tag/ai-regulation)[ai policy](https://spectrum.ieee.org/tag/ai-policy)[tech policy](https://spectrum.ieee.org/tag/tech-policy)[ai](https://spectrum.ieee.org/tag/ai)

At times, it ca n seem like efforts to regulate and rein in AI are [everything, everywhere, all at once](https://spectrum.ieee.org/ai-ethics-governance).

[China](https://spectrum.ieee.org/tag/china) issued the first [AI-specific regulations in 2021](https://carnegieendowment.org/research/2024/02/tracing-the-roots-of-chinas-ai-regulations?lang=en). The focus is squarely on providers and content governance, enforced through platform control and recordkeeping requirements.

In Europe, the [European Union AI Act](https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)dates to 2024, but the [European Commission](https://spectrum.ieee.org/tag/european-commission) is already proposing [updates and simplification](https://digital-strategy.ec.europa.eu/en/library/digital-omnibus-ai-regulation-proposal).

[India](https://spectrum.ieee.org/tag/india) charged its senior technical advisors with creating an AI governance system, which they [released](https://static.pib.gov.in/WriteReadData/specificdocs/documents/2025/nov/doc2025115685601.pdf) in November 2025.

In the [United States](https://spectrum.ieee.org/tag/united-states), the states are [legislat ing](https://www.ncsl.org/financial-services/artificial-intelligence-legislation-database) and enforc ing their own AI rules even as the federal government in 2025 [moved to prevent state action and loosen the reins](https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/).

This leads to a critical question for American engineers and policymakers alike: What can the U.S. actually enforce in a way that reduces real-world harm? My answer: Regulate AI use, not the underlying models.

Why model-centric regulation fails
----------------------------------

Proposals to license “frontier” training runs, restrict open weights, or require permission before publishing models, such as California’s [Transparency in Frontier Artificial Intelligence Act,](https://legiscan.com/CA/text/SB53/id/3270002)promise control but deliver theater. Model weights and code are digital artifacts; once released, by a lab, a leak, or a foreign competitor, they replicate at near-zero cost. You can’t unpublish weights, geofence research, or prevent distillation into smaller models. Trying to bottle up artifacts yields two bad outcomes: Compliant firms drown in paperwork, while reckless actors route around rules offshore, underground, or both.

In the United States, model-publication licensing also likely collides with speech law. Federal courts have treated software [source code](https://spectrum.ieee.org/tag/source-code) as protected expression, so any system that prevents the publication of [AI models](https://spectrum.ieee.org/tag/ai-models) would be vulnerable to legal challenges.

“Do nothing” is [not an option](https://spectrum.ieee.org/ai-regulation-worldwide) either. Without guardrails, we will keep seeing [deepfake scams](https://www.dhs.gov/sites/default/files/publications/increasing_threats_of_deepfake_identities_0.pdf), automated fraud, and mass-persuasion campaigns until a headline catastrophe triggers a blunt response optimized for [optics](https://spectrum.ieee.org/tag/optics), not outcomes.

A practical alternative: Regulate use, proportionate to risk
------------------------------------------------------------

A use-based regime classifies deployments by risk and scales obligations accordingly. Here is a workable template focused on keeping enforcement where systems actually touch people:

1.   **Baseline: General-purpose consumer interaction** (open-ended chat, creative writing, learning assistance, casual productivity). 

Regulatory adherence: clear AI disclosure at point of interaction, published acceptable-use policies, technical guardrails preventing escalation into higher-risk tiers, and a mechanism for users to flag problematic outputs.

1.   **Low-risk assistance** (drafting, summarization, basic productivity). 

Regulatory adherence _\_:\__ simple disclosure, baseline data hygiene. 

1.   **Moderate-risk decision support affecting individuals** (hiring triage, benefits screening, loan prequalification). 

Regulatory adherence _\_:\__ documented [risk assessment](https://spectrum.ieee.org/tag/risk-assessment), meaningful human oversight, and an “AI bill of materials” consisting of at least the model lineage, key evaluations, and mitigations. 

1.   **High-impact uses in safety-critical contexts** (clinical decision support, critical-infrastructure operations). 

Regulatory adherence _\_:\__ rigorous predeployment testing tied to the specific use, [continuous monitoring](https://spectrum.ieee.org/tag/continuous-monitoring), incident reporting, and, when warranted, authorization linked to validated performance.

1.   **Hazardous dual-use functions** (for example, tools to fabricate biometric voiceprints to defeat [authentication](https://spectrum.ieee.org/tag/authentication)). 

Regulatory adherence _\_:\__ confine to licensed facilities and verified operators; prohibit capabilities whose primary purpose is unlawful. 

Close the loop at real-world choke points
-----------------------------------------

AI-enabled systems become real when they’re connected to users, money, infrastructure, and institutions, and that’s where regulators should focus enforcement: at the points of distribution (app stores and enterprise marketplaces), capability access (cloud and AI platforms), monetization ([payment systems](https://spectrum.ieee.org/tag/payment-systems) and ad networks), and risk transfer (insurers and contract counterparties).

For high-risk uses, we need to require identity binding for operators, capability gating aligned to the risk tier, and tamper-evident logging for audits and postincident review, paired with privacy protections. We need to demand evidence for deployer claims, maintain incident-response plans, report material faults, and provide human fallback. When AI use leads to damage, firms should have to show their work and face [liability](https://spectrum.ieee.org/tag/liability) for harms.

This approach creates market dynamics that accelerate compliance. If crucial business operations such as procurement, access to cloud services, and insurance depend on proving that you’re following the rules, AI model developers will build to specifications buyers can check. That raises the safety floor for a ll industry players,[startups](https://spectrum.ieee.org/tag/startups) included, without handing an advantage to a few large, licensed incumbents.

The E.U. approach: How this aligns, where it differs
----------------------------------------------------

This framework aligns with the E.U. AI Act in two important ways. First, it centers risk at the point of impact: The act’s “high-risk” categories include employment, education, access to essential services, and critical infrastructure, with life-cycle obligations and complaint rights. It also recognizes special treatment for broadly capable systems (GPAI) without pretending publication control is a safety strategy. My proposal for the United States differs in three key ways:

First, the U.S. must design for constitutional durability. Courts have treated source code as protected speech, and a regime that requires permission to publish weights or train a class of models starts to resemble prior restraint. A use-based regime of rules governing what AI operators can do in sensitive settings, and under what conditions, fits more naturally within the U.S. First Amendment doctrine than speaker-based licensing schemes.

Second, the E.U. can rely on platforms adapting to the precautionary rules it writes for its unified single market. The U.S. should accept that models will exist globally, both open and closed, and focus on where AI becomes actionable: app stores, enterprise platforms, cloud providers, enterprise identity layers, payment rails, insurers, and regulated-sector gatekeepers ([hospitals](https://spectrum.ieee.org/tag/hospitals), utilities, [banks](https://spectrum.ieee.org/tag/banks)). Those are enforceable points where identity, logging, capability gating, and postincident accountability can be required without pretending we can “contain” software. They also span the many specialized U.S. agencies that may not be able to write higher-level rules broad enough to affect the whole AI ecosystem. Instead, the U.S. should regulate AI service choke points more explicitly than Europe does, to accommodate the different shape of its government and public administration. 

Third, the U.S. should add an explicit “dual-use hazard” tier. The E.U. AI Act is primarily a fundamental-rights and product-safety regime. The United States also has a national-security reality: Certain capabilities are dangerous because they scale harm (biosecurity, cyberoffense, mass fraud). A coherent U.S. framework should name that category and regulate it directly, rather than trying to fit it into generic “frontier model” licensing.

China’s approach: What to reuse, what to avoid
----------------------------------------------

China has built a layered regime for public-facing AI. The “deep synthesis” rules (effective 10 January 2023) require conspicuous labeling of synthetic media and place duties on providers and platforms. The **I**nterim Measures for [Generative AI](https://spectrum.ieee.org/tag/generative-ai) (effective 15 August 2023) add registration and governance obligations for services offered to the public. Enforcement leverages platform control and algorithm filing systems.

The United States should not copy China’s state-directed control of AI viewpoints or information management; it is incompatible with U.S. values and would not survive U.S. constitutional scrutiny. The licensing of model publication is brittle in practice and, in the United States, likely an unconstitutional form of [censorship](https://spectrum.ieee.org/tag/censorship).

But we can borrow two practical ideas from China. First, we should ensure trustworthy provenance and traceability for synthetic media. This involves mandatory labeling and provenance forensic tools. They give legitimate creators and platforms a reliable way to prove origin and integrity. When it is quick to check authenticity at scale, attackers lose the advantage of cheap copies or [deepfakes](https://spectrum.ieee.org/tag/deepfakes) and defenders regain time to detect, triage, and respond. Second, we should require operators to file their methods and risk controls with regulators for public-facing, high-risk services, like we do for other safety-critical projects. This should include due-process and transparency safeguards appropriate to liberal democracies along with clear responsibility for safety measures, data protection, and incident handling, especially for systems designed to manipulate emotions or build dependency, which already include gaming, role-playing, and associated applications.

A pragmatic approach
--------------------

We cannot meaningfully regulate the development of AI in a world where artifacts copy in near real time and research flows fluidly across borders. But we can keep unvetted systems out of hospitals, payment systems, and critical infrastructure by regulating uses, not models; enforcing at choke points; and applying obligations that scale with risk. 

Done right, this approach harmonizes with the E.U.’s outcome-oriented framework, channels U.S. federal and state innovation into a coherent baseline, and reuses China’s useful distribution-level controls while rejecting speech-restrictive licensing. We can write rules that protect people and that still promote [robust AI](https://spectrum.ieee.org/tag/robust-ai) innovation.

From Your Site Articles

*   [AI Everywhere, All at Once ›](https://spectrum.ieee.org/ai-ethics-governance)
*   [The Who, Where, and How of Regulating AI ›](https://spectrum.ieee.org/ai-regulation-worldwide)

Related Articles Around the Web

*   [Markus Anderljung on how to regulate cutting-edge AI models ›](https://80000hours.org/podcast/episodes/markus-anderljung-regulating-cutting-edge-ai/)
*   [Regulate AI Use, Not AI Development | Andreessen Horowitz ›](https://a16z.com/regulate-ai-use-not-ai-development/)

Keep Reading ↓Show less

---
*自动采集于 2026-02-03*
