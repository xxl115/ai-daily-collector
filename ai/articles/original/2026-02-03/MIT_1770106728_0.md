---
title: "MIT Technology Review"
url: "https://www.technologyreview.com"
source: "MIT Technology Review"
date: "2026-02-03"
---

# MIT Technology Review

**来源**: [MIT Technology Review](https://www.technologyreview.com)

## 原文内容

Title: MIT Technology Review

URL Source: http://www.technologyreview.com/

Markdown Content:
Magazine
--------

Our new issue!

January/February 2026

[### The Innovation issue](https://www.technologyreview.com/magazines/the-innovation-issue-26/)
It’s the 10 breakthrough technologies for 2026! That’s hyperscale data centers, designer babies, smaller nuclear power, space stations you can visit, and more. Plus, read about conjuring water from air, dissecting artificial intelligence, and a scientist who swears he’s going to do a human head transplant any day now.

Today's Newsletter

The latest from

The Algorithm: Our weekly AI email

Sign up to get The Algorithm weekly in your inbox.

### What we’ve been getting wrong about AI’s truth crisis

What would it take to convince you that the era of truth decay we were long warned about—where AI content dupes us, shapes our beliefs even when we catch the lie, and erodes societal trust in the process—is now here? A story I published last week pushed me over the edge. It also made me realize that the tools we were sold as a cure for this crisis are failing miserably.

On Thursday, I [reported](https://www.technologyreview.com/2026/01/29/1131938/dhs-is-using-google-and-adobe-ai-to-make-videos/) the first confirmation that the US Department of Homeland Security, which houses immigration agencies, is using AI video generators from Google and Adobe to make content that it shares with the public. The news comes as immigration agencies have flooded social media with content to support President Trump's mass deportation agenda—some of which appears to be made with AI (like a [video](https://x.com/DHSgov/status/2002424827798585786) about “Christmas after mass deportations”).

But I received two types of reactions from readers that may explain just as much about the epistemic crisis we’re in.

One was from people who weren’t surprised, because on January 22 the White House had posted a [digitally altered](https://www.theguardian.com/us-news/2026/jan/22/white-house-ice-protest-arrest-altered-image) photo of a woman arrested at an ICE protest, one that made her appear hysterical and in tears. Kaelan Dorr, the White House’s deputy communications director, did not respond to questions about whether the White House altered the photo but [wrote](https://x.com/Kaelan47/status/2014410500096856358?s=20), “The memes will continue.”

The second was from readers who saw no point in reporting that DHS was using AI to edit content shared with the public, because news outlets were apparently doing the same. They pointed to the fact that the news network MS Now (formerly MSNBC) shared an image of Alex Pretti that was AI-edited and appeared to make him look more handsome, a fact that led to many viral clips this week, including one from Joe Rogan’s podcast. Fight fire with fire, in other words? A spokesperson for MS Now [told Snopes](https://www.snopes.com/fact-check/alex-pretti-photo-ai-msnbc/) that the news outlet aired the image without knowing it was edited.

There is no reason to collapse these two cases of altered content into the same category, or to read them as evidence that truth no longer matters. One involved the US government sharing a clearly altered photo with the public and declining to answer whether it was intentionally manipulated; the other involved a news outlet airing a photo it [should have known](https://perma.cc/JB3J-57QN) was altered but taking some steps to disclose the mistake.

What these reactions reveal instead is a flaw in how we were collectively preparing for this moment. Warnings about the AI truth crisis revolved around a core thesis: that not being able to tell what is real will destroy us, so we need tools to independently verify the truth. My two grim takeaways are that these tools are failing, and that while vetting the truth remains essential, it is no longer capable on its own of producing the societal trust we were promised.

For example, there was plenty of hype in 2024 about the Content Authenticity Initiative, cofounded by Adobe and adopted by major tech companies, which would attach labels to content disclosing when it was made, by whom, and whether AI was involved. But Adobe applies automatic labels only when the content is wholly AI-generated. Otherwise the labels are opt-in on the part of the creator.

And platforms like X, where the altered arrest photo was posted, can strip content of such labels anyway (a note that the photo was altered was [added](https://x.com/WhiteHouse/status/2014365986388951194/photo/1) by users). Platforms can also simply not choose to show the label; indeed, when Adobe launched the initiative, it [noted](https://blog.adobe.com/en/publish/2024/09/18/authenticity-age-ai-growing-content-credentials-momentum-across-social-media-platforms-ai-companies-rising-consumer-awareness) that the Pentagon's website for sharing official images, DVIDS, would display the labels to prove authenticity, but a review of the website today shows no such labels.

Noticing how much traction the White House’s photo got even after it was shown to be AI-altered, I was struck by the findings of a very relevant new [paper](https://www.nature.com/articles/s44271-025-00381-9) published in the journal _Communications Psychology_. In the study, participants watched a deepfake “confession” to a crime, and the researchers found that even when they were told explicitly that the evidence was fake, participants relied on it when judging an individual’s guilt.In other words, even when people learn that the content they’re looking at is entirely fake, they remain emotionally swayed by it.

“Transparency helps, but it isn’t enough on its own,” the disinformation expert Christopher Nehring [wrote](https://www.linkedin.com/posts/christopher-n-423b06257_influence-of-deepfakes-activity-7422200374591361043-Fp5C?utm_source=share&utm_medium=member_desktop&rcm=ACoAADzDcuoBK3BxlTOFUM8kwVIgx8nHdh4DhwY) recently about the study’s findings. “We have to develop a new masterplan of what to do about deepfakes.”

AI tools to generate and edit content are getting more advanced, easier to operate, and cheaper to run—all reasons why the US government is increasingly paying to use them. We were well warned of this, but we responded by preparing for a world in which the main danger was confusion. What we’re entering instead is a world in which influence survives exposure, doubt is easily weaponized, and establishing the truth does not serve as a reset button. And the defenders of truth are already trailing way behind.

_Update: This story was updated on February 2 with details about how Adobe applies its content authenticity labels._

What would it take to convince you that the era of truth decay we were long warned about—where AI content dupes us, shapes our beliefs even when we catch the lie, and erodes societal trust in the process—is now here? A story I published last week pushed me over the edge. It also made me realize that the tools we were sold as a cure for this crisis are failing miserably.

On Thursday, I [reported](https://www.technologyreview.com/2026/01/29/1131938/dhs-is-using-google-and-adobe-ai-to-make-videos/) the first confirmation that the US Department of Homeland Security, which houses immigration agencies, is using AI video generators from Google and Adobe to make content that it shares with the public. The news comes as immigration agencies have flooded social media with content to support President Trump's mass deportation agenda—some of which appears to be made with AI (like a [video](https://x.com/DHSgov/status/2002424827798585786) about “Christmas after mass deportations”).

But I received two types of reactions from readers that may explain just as much about the epistemic crisis we’re in.

One was from people who weren’t surprised, because on January 22 the White House had posted a [digitally altered](https://www.theguardian.com/us-news/2026/jan/22/white-house-ice-protest-arrest-altered-image) photo of a woman arrested at an ICE protest, one that made her appear hysterical and in tears. Kaelan Dorr, the White House’s deputy communications director, did not respond to questions about whether the White House altered the photo but [wrote](https://x.com/Kaelan47/status/2014410500096856358?s=20), “The memes will continue.”

The second was from readers who saw no point in reporting that DHS was using AI to edit content shared with the public, because news outlets were apparently doing the same. They pointed to the fact that the news network MS Now (formerly MSNBC) shared an image of Alex Pretti that was AI-edited and appeared to make him look more handsome, a fact that led to many viral clips this week, including one from Joe Rogan’s podcast. Fight fire with fire, in other words? A spokesperson for MS Now [told Snopes](https://www.snopes.com/fact-check/alex-pretti-photo-ai-msnbc/) that the news outlet aired the image without knowing it was edited.

There is no reason to collapse these two cases of altered content into the same category, or to read them as evidence that truth no longer matters. One involved the US government sharing a clearly altered photo with the public and declining to answer whether it was intentionally manipulated; the other involved a news outlet airing a photo it [should have known](https://perma.cc/JB3J-57QN) was altered but taking some steps to disclose the mistake.

What these reactions reveal instead is a flaw in how we were collectively preparing for this moment. Warnings about the AI truth crisis revolved around a core thesis: that not being able to tell what is real will destroy us, so we need tools to independently verify the truth. My two grim takeaways are that these tools are failing, and that while vetting the truth remains essential, it is no longer capable on its own of producing the societal trust we were promised.

For example, there was plenty of hype in 2024 about the Content Authenticity Initiative, cofounded by Adobe and adopted by major tech companies, which would attach labels to content disclosing when it was made, by whom, and whether AI was involved. But Adobe applies automatic labels only when the content is wholly AI-generated. Otherwise the labels are opt-in on the part of the creator.

And platforms like X, where the altered arrest photo was posted, can strip content of such labels anyway (a note that the photo was altered was [added](https://x.com/WhiteHouse/status/2014365986388951194/photo/1) by users). Platforms can also simply not choose to show the label; indeed, when Adobe launched the initiative, it [noted](https://blog.adobe.com/en/publish/2024/09/18/authenticity-age-ai-growing-content-credentials-momentum-across-social-media-platforms-ai-companies-rising-consumer-awareness) that the Pentagon's website for sharing official images, DVIDS, would display the labels to prove authenticity, but a review of the website today shows no such labels.

Noticing how much traction the White House’s photo got even after it was shown to be AI-altered, I was struck by the findings of a very relevant new [paper](https://www.nature.com/articles/s44271-025-00381-9) published in the journal _Communications Psychology_. In the study, participants watched a deepfake “confession” to a crime, and the researchers found that even when they were told explicitly that the evidence was fake, participants relied on it when judging an individual’s guilt.In other words, even when people learn that the content they’re looking at is entirely fake, they remain emotionally swayed by it.

“Transparency helps, but it isn’t enough on its own,” the disinformation expert Christopher Nehring [wrote](https://www.linkedin.com/posts/christopher-n-423b06257_influence-of-deepfakes-activity-7422200374591361043-Fp5C?utm_source=share&utm_medium=member_desktop&rcm=ACoAADzDcuoBK3BxlTOFUM8kwVIgx8nHdh4DhwY) recently about the study’s findings. “We have to develop a new masterplan of what to do about deepfakes.”

AI tools to generate and edit content are getting more advanced, easier to operate, and cheaper to run—all reasons why the US government is increasingly paying to use them. We were well warned of this, but we responded by preparing for a world in which the main danger was confusion. What we’re entering instead is a world in which influence survives exposure, doubt is easily weaponized, and establishing the truth does not serve as a reset button. And the defenders of truth are already trailing way behind.

_Update: This story was updated on February 2 with details about how Adobe applies its content authenticity labels._

[### What even is the AI bubble?](https://www.technologyreview.com/2025/12/15/1129183/what-even-is-the-ai-bubble/)
Everyone in tech agrees we’re in a bubble. They just can’t agree on what it looks like — or what happens when it pops.

1 / 10

2 / 10

3 / 10

4 / 10

5 / 10

6 / 10

7 / 10

8 / 10

[### What is vibe coding, exactly?](https://www.technologyreview.com/2025/04/16/1115135/what-is-vibe-coding-exactly/)
While letting AI take the wheel and write the code for your website may seem like a good idea, it’s not without its limitations.

9 / 10

10 / 10

1 / 10

2 / 10

3 / 10

4 / 10

5 / 10

6 / 10

7 / 10

8 / 10

9 / 10

10 / 10

1 / 10

[### Powering up (and saving) the planet](https://www.technologyreview.com/2026/01/06/1128655/powering-up-and-saving-the-planet/)
As the Institute’s first VP for energy and climate, Evelyn Wang ’00 is marshaling MIT’s expertise to meet the greatest challenge of our age.

2 / 10

3 / 10

[### Dennis Whyte’s fusion quest](https://www.technologyreview.com/2026/01/06/1128665/dennis-whytes-fusion-quest/)
When the US Department of Energy announced that it would stop funding the tokamak at MIT’s Plasma Science and Fusion Center, Dennis Whyte considered giving up on fusion research. But as this excerpt from the new book “Hope Dies Last” recounts, Whyte instead had a brainstorm—and challenged his students to bring the idea to life.

4 / 10

[### Starstruck](https://www.technologyreview.com/2026/01/06/1128661/starstruck/)
Aomawa Shields ’97 was equally enticed by the prospect of studying stars and the dream of becoming one herself. Today, she draws from her exploration of acting and astronomy to search for life on other planets.

5 / 10

6 / 10

7 / 10

[### How the Longfellow Bridge came to be](https://www.technologyreview.com/2026/01/06/1129019/how-the-longfellow-bridge-came-to-be/)
At the turn of the 20th century, a commission set out to replace the old West Boston Bridge with “one of the finest and most beautiful structures in the world”—and hired two MIT alumni to make it happen.

8 / 10

9 / 10

10 / 10

January/February 2026

[### MIT Alumni News](https://www.technologyreview.com/magazines/powering-up-and-saving-the-planet/)
Read the whole issue of MIT Alumni News, the alumni magazine of Massachusetts Institute of Technology.

1 / 10

2 / 10

[### What’s next for carbon removal?](https://www.technologyreview.com/2025/10/24/1126478/whats-next-for-carbon-removal/)
Companies have still drawn down only enough CO2 to cancel out a few hours of US emissions. Here’s what it will take to really scale up the sector.

3 / 10

4 / 10

[### What’s next for AI and math](https://www.technologyreview.com/2025/06/04/1117753/whats-next-for-ai-and-math/)
The last year has seen rapid progress in the ability of large language models to tackle math at high school level and beyond. Is AI closing in on human mathematicians?

5 / 10

6 / 10

[### What’s next for our privacy?](https://www.technologyreview.com/2025/01/07/1109301/privacy-protection-data-brokers-personal-information/)
The US still has no federal privacy law. But recent enforcement actions against data brokers may offer some new protections for Americans’ personal information.

7 / 10

8 / 10

9 / 10

[### What’s next for drones](https://www.technologyreview.com/2024/08/16/1096517/whats-next-for-drones/)
Police drones, rapid deliveries of blood, tech-friendly regulations, and autonomous weapons are all signs that drone technology is changing quickly.

10 / 10

[### What’s next in chips](https://www.technologyreview.com/2024/05/13/1092319/whats-next-in-chips/)
How Big Tech, startups, AI devices, and trade wars will transform the way chips are made and the technologies they power.

Sponsored

---
*自动采集于 2026-02-03*
