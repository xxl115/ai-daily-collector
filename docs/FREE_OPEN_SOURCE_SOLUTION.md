# å…è´¹å¼€æºå†…å®¹å¤„ç†æ–¹æ¡ˆ

## æ–¹æ¡ˆæ¦‚è¿°

å®Œå…¨å…è´¹çš„å¼€æºæ–¹æ¡ˆï¼Œæ— éœ€è°ƒç”¨ä»˜è´¹ APIï¼š
- âœ… å†…å®¹æŠ“å–ï¼šå¼€æºåº“
- âœ… å†…å®¹æ¸…æ´—ï¼šå¼€æºå·¥å…·
- âœ… æ–‡æœ¬æ‘˜è¦ï¼šæœ¬åœ° LLM (Ollama) æˆ–å¼€æºç®—æ³•
- âœ… æ™ºèƒ½åˆ†ç±»ï¼šè§„åˆ™ + å…³é”®è¯ + å…è´¹ Embedding
- âœ… å­˜å‚¨ï¼šD1 å…è´¹é¢åº¦

**é¢„è®¡æˆæœ¬ï¼š0 å…ƒ**

---

## 1. å†…å®¹æŠ“å–ï¼ˆå…è´¹ï¼‰

### æŠ€æœ¯é€‰å‹

| å·¥å…· | ç”¨é€” | ä¼˜ç‚¹ |
|------|------|------|
| `newspaper3k` | æ–°é—»æ–‡ç« æå– | ä¸“é—¨é’ˆå¯¹æ–°é—»ä¼˜åŒ–ï¼Œè‡ªåŠ¨æå–æ ‡é¢˜ã€ä½œè€…ã€æ­£æ–‡ã€å›¾ç‰‡ |
| `trafilatura` | é€šç”¨ç½‘é¡µæ­£æ–‡æå– | å‡†ç¡®ç‡é«˜ï¼Œé€Ÿåº¦å¿«ï¼Œæ”¯æŒå¤šè¯­è¨€ |
| `readability-lxml` | ç±»ä¼¼æµè§ˆå™¨é˜…è¯»æ¨¡å¼ | æå–ä¸»è¦å†…å®¹ï¼Œå»é™¤å¹¿å‘Š |

### æ¨èæ–¹æ¡ˆï¼šnewspaper3k + trafilatura ç»„åˆ

```python
# ingestor/content_fetcher.py

import requests
from newspaper import Article
import trafilatura
from typing import Optional
import time

class FreeContentFetcher:
    """å…è´¹å†…å®¹æŠ“å–å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
    
    def fetch_content(self, url: str) -> Optional[dict]:
        """
        æŠ“å–ç½‘é¡µå†…å®¹ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®
        
        Returns:
            {
                'title': str,
                'content': str,
                'summary': str,
                'authors': list,
                'publish_date': str,
                'top_image': str
            }
        """
        try:
            # æ–¹æ¡ˆ1: ä½¿ç”¨ newspaper3kï¼ˆé€‚åˆæ–°é—»ç½‘ç«™ï¼‰
            article = Article(url, language='zh')
            article.download()
            article.parse()
            article.nlp()  # è‡ªåŠ¨æå–å…³é”®è¯å’Œæ‘˜è¦
            
            if article.text and len(article.text) > 200:
                return {
                    'title': article.title or '',
                    'content': article.text,
                    'summary': article.summary or article.text[:300],
                    'authors': article.authors or [],
                    'publish_date': str(article.publish_date) if article.publish_date else '',
                    'top_image': article.top_image or '',
                    'keywords': article.keywords or []
                }
        except Exception as e:
            print(f"newspaper3k failed: {e}")
        
        # æ–¹æ¡ˆ2: ä½¿ç”¨ trafilaturaï¼ˆé€šç”¨ç½‘é¡µï¼‰
        try:
            downloaded = trafilatura.fetch_url(url)
            if downloaded:
                content = trafilatura.extract(downloaded, 
                                              include_comments=False,
                                              include_tables=False,
                                              deduplicate=True,
                                              target_language="zh")
                if content and len(content) > 200:
                    return {
                        'title': '',  # éœ€è¦å¦å¤–æå–
                        'content': content,
                        'summary': content[:300],
                        'authors': [],
                        'publish_date': '',
                        'top_image': '',
                        'keywords': []
                    }
        except Exception as e:
            print(f"trafilatura failed: {e}")
        
        return None
    
    def batch_fetch(self, urls: list, delay: float = 1.0) -> list:
        """æ‰¹é‡æŠ“å–ï¼Œå¸¦å»¶è¿Ÿé˜²æ­¢è¢«å°"""
        results = []
        for url in urls:
            result = self.fetch_content(url)
            if result:
                results.append(result)
            time.sleep(delay)  # ç¤¼è²Œçˆ¬å–
        return results
```

### å®‰è£…ä¾èµ–

```bash
pip install newspaper3k trafilatura readability-lxml
python -m nltk.downloader punkt  # newspaper3k éœ€è¦
```

---

## 2. æ–‡æœ¬æ‘˜è¦ï¼ˆå…è´¹ï¼‰

### æ–¹æ¡ˆ A: åŸºäº TextRank çš„æŠ½å–å¼æ‘˜è¦ï¼ˆå®Œå…¨å…è´¹ï¼‰

ä¸éœ€è¦ AIï¼Œç”¨ç®—æ³•ä»æ–‡ç« ä¸­æå–å…³é”®å¥å­ã€‚

```python
# ingestor/summarizer.py

import re
from collections import Counter
import math
from typing import List

class TextRankSummarizer:
    """åŸºäº TextRank çš„å…è´¹æ‘˜è¦ç”Ÿæˆ"""
    
    def __init__(self):
        self.stopwords = set(['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'])
    
    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å¥"""
        # ä¸­æ–‡åˆ†å¥
        sentences = re.split('[ã€‚ï¼ï¼Ÿ\n]+', text)
        return [s.strip() for s in sentences if len(s.strip()) > 10]
    
    def _split_words(self, text: str) -> List[str]:
        """åˆ†è¯ - ç®€å•å®ç°"""
        # ä½¿ç”¨ jieba æˆ–ç®€å•å­—ç¬¦åˆ†å‰²
        import jieba
        words = jieba.cut(text)
        return [w for w in words if len(w) > 1 and w not in self.stopwords]
    
    def _sentence_similarity(self, sent1: str, sent2: str) -> float:
        """è®¡ç®—å¥å­ç›¸ä¼¼åº¦"""
        words1 = set(self._split_words(sent1))
        words2 = set(self._split_words(sent2))
        
        if not words1 or not words2:
            return 0
        
        intersection = words1 & words2
        return len(intersection) / (math.log(len(words1)) + math.log(len(words2)) + 1)
    
    def summarize(self, text: str, num_sentences: int = 3) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        sentences = self._split_sentences(text)
        
        if len(sentences) <= num_sentences:
            return 'ã€‚'.join(sentences) + 'ã€‚'
        
        # æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ
        n = len(sentences)
        sim_matrix = [[0 for _ in range(n)] for _ in range(n)]
        
        for i in range(n):
            for j in range(n):
                if i != j:
                    sim_matrix[i][j] = self._sentence_similarity(sentences[i], sentences[j])
        
        # è®¡ç®—å¥å­å¾—åˆ†ï¼ˆç®€å•ç‰ˆ PageRankï¼‰
        scores = [1.0] * n
        damping = 0.85
        iterations = 30
        
        for _ in range(iterations):
            new_scores = [0.0] * n
            for i in range(n):
                for j in range(n):
                    if i != j and sim_matrix[j][i] > 0:
                        new_scores[i] += sim_matrix[j][i] * scores[j]
                new_scores[i] = (1 - damping) + damping * new_scores[i]
            scores = new_scores
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å¥å­
        ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)
        selected_indices = sorted([idx for idx, _ in ranked[:num_sentences]])
        
        summary = 'ã€‚'.join([sentences[i] for i in selected_indices])
        return summary + 'ã€‚'

# ä½¿ç”¨ç¤ºä¾‹
summarizer = TextRankSummarizer()
summary = summarizer.summarize(long_article_text, num_sentences=2)
```

### å®‰è£…ä¾èµ–

```bash
pip install jieba
```

### æ–¹æ¡ˆ B: æœ¬åœ° LLM (Ollama)

å¦‚æœæœ‰æœåŠ¡å™¨èµ„æºï¼Œå¯ä»¥æœ¬åœ°è¿è¡Œå°æ¨¡å‹ã€‚

```bash
# å®‰è£… Ollama
ollama pull llama2-chinese:7b  # ä¸­æ–‡æ¨¡å‹
ollama pull qwen:7b  # é˜¿é‡Œé€šä¹‰åƒé—®
```

```python
import requests

class LocalLLMProcessor:
    """æœ¬åœ° Ollama LLM"""
    
    def __init__(self, model: str = "qwen:7b", host: str = "http://localhost:11434"):
        self.model = model
        self.host = host
    
    def summarize(self, text: str) -> str:
        """æœ¬åœ°æ¨¡å‹ç”Ÿæˆæ‘˜è¦"""
        prompt = f"è¯·ç”¨50å­—æ¦‚æ‹¬ä»¥ä¸‹å†…å®¹ï¼š\n{text[:1000]}"
        
        response = requests.post(f"{self.host}/api/generate", json={
            "model": self.model,
            "prompt": prompt,
            "stream": False
        })
        
        return response.json().get('response', '')
    
    def classify(self, title: str, content: str) -> dict:
        """æœ¬åœ°æ¨¡å‹åˆ†ç±»"""
        prompt = f"åˆ†ç±»ä»¥ä¸‹æ–‡ç« ï¼ˆhot/deep/new/breakingï¼‰ï¼Œåªè¾“å‡ºåˆ†ç±»è¯ï¼š\næ ‡é¢˜ï¼š{title}\nå†…å®¹ï¼š{content[:500]}"
        
        response = requests.post(f"{self.host}/api/generate", json={
            "model": self.model,
            "prompt": prompt,
            "stream": False
        })
        
        category = response.json().get('response', '').strip().lower()
        return {"category": category, "tags": []}
```

---

## 3. æ™ºèƒ½åˆ†ç±»ï¼ˆå…è´¹ï¼‰

### æ–¹æ¡ˆ A: è§„åˆ™ + å…³é”®è¯ï¼ˆå®Œå…¨å…è´¹ï¼‰

```python
# ingestor/classifier.py

import re
from typing import List, Dict

class RuleClassifier:
    """åŸºäºè§„åˆ™çš„å…è´¹åˆ†ç±»å™¨"""
    
    # åˆ†ç±»è§„åˆ™
    CATEGORIES = {
        'breaking': {
            'keywords': ['çªå‘', 'ç´§æ€¥', 'åˆšåˆš', 'é‡ç£…', 'éœ‡æƒŠ', 'ç´§æ€¥å‘å¸ƒ', 'å¿«è®¯', 'Breaking'],
            'weight': 10
        },
        'hot': {
            'keywords': ['çƒ­é—¨', 'çƒ­è®®', 'ç«äº†', 'çˆ†ç«', 'åˆ·å±', 'çƒ­æœ', ' trending', 'viral'],
            'weight': 8
        },
        'new': {
            'keywords': ['å‘å¸ƒ', 'æ–°å“', 'æ¨å‡º', 'ä¸Šçº¿', 'é—®ä¸–', 'äº®ç›¸', 'å®˜å®£', 'ç™»åœº', 'å‘å¸ƒ'],
            'weight': 6
        },
        'deep': {
            'keywords': ['ç ”ç©¶', 'è®ºæ–‡', 'æ·±åº¦', 'åˆ†æ', 'è§£è¯»', 'ç»¼è¿°', 'æŠ€æœ¯ç»†èŠ‚', 'åŸç†', 'æ–¹æ³•è®º'],
            'weight': 5
        }
    }
    
    # æ ‡ç­¾è§„åˆ™
    TAGS = {
        'AIç»˜ç”»': ['midjourney', 'dalle', 'stable diffusion', 'ç»˜ç”»', 'ç”Ÿå›¾', 'å›¾åƒç”Ÿæˆ', 'ai art', 'ç”Ÿæˆå›¾ç‰‡'],
        'LLM': ['gpt', 'llama', 'claude', 'å¤§æ¨¡å‹', 'è¯­è¨€æ¨¡å‹', 'chatgpt', 'bert', 'transformer'],
        'äº§å“å‘å¸ƒ': ['å‘å¸ƒ', 'æ–°å“', 'æ¨å‡º', 'ä¸Šçº¿', 'v1.', 'v2.', 'ç‰ˆæœ¬æ›´æ–°', 'æ­£å¼ç‰ˆ'],
        'ç ”ç©¶': ['è®ºæ–‡', 'arxiv', 'research', 'ç ”ç©¶', 'novel', 'method', 'approach', 'å®éªŒ'],
        'å·¥å…·': ['å·¥å…·', 'plugin', 'æ’ä»¶', 'æ‰©å±•', 'cursor', 'ide', 'vscode', 'å¼€æº'],
        'å®‰å…¨': ['å®‰å…¨', 'é£é™©', 'æ¼æ´', 'éšç§', 'æ”»å‡»', 'é˜²æŠ¤', 'hack', 'security'],
        'å•†ä¸š': ['èèµ„', 'æ”¶è´­', 'è´¢æŠ¥', 'å¸‚åœº', 'å•†ä¸š', 'æŠ•èµ„', 'ä¼°å€¼', 'ipo', 'startup'],
        'ä¼¦ç†': ['ä¼¦ç†', 'ç›‘ç®¡', 'æ”¿ç­–', 'æ³•å¾‹', 'ç‰ˆæƒ', 'aiæ³•æ¡ˆ', 'æ²»ç†'],
        'ç¡¬ä»¶': ['èŠ¯ç‰‡', 'gpu', 'tpu', 'nvidia', 'apple silicon', 'æ¨ç†èŠ¯ç‰‡']
    }
    
    def classify(self, title: str, content: str) -> Dict:
        """åˆ†ç±»æ–‡ç« """
        text = (title + ' ' + content).lower()
        
        # è®¡ç®—æ¯ä¸ªåˆ†ç±»çš„å¾—åˆ†
        scores = {}
        for cat, config in self.CATEGORIES.items():
            score = 0
            for keyword in config['keywords']:
                count = len(re.findall(keyword.lower(), text))
                score += count * config['weight']
            scores[cat] = score
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„åˆ†ç±»
        if max(scores.values()) > 0:
            category = max(scores, key=scores.get)
        else:
            category = 'new'  # é»˜è®¤åˆ†ç±»
        
        # æå–æ ‡ç­¾
        tags = []
        for tag, keywords in self.TAGS.items():
            if any(kw.lower() in text for kw in keywords):
                tags.append(tag)
        
        return {
            'category': category,
            'tags': tags[:3]  # æœ€å¤š3ä¸ªæ ‡ç­¾
        }
```

### æ–¹æ¡ˆ B: å…è´¹ Embedding + ç›¸ä¼¼åº¦ï¼ˆéœ€è¦ä¸€ç‚¹è®¡ç®—èµ„æºï¼‰

ä½¿ç”¨å¼€æºçš„ sentence-transformers ç”Ÿæˆ embeddingï¼Œç„¶åä¸é¢„å®šä¹‰çš„åˆ†ç±»æ¨¡æ¿æ¯”è¾ƒç›¸ä¼¼åº¦ã€‚

```python
# éœ€è¦å®‰è£…ï¼špip install sentence-transformers

from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class EmbeddingClassifier:
    """åŸºäºå…è´¹ Embedding çš„åˆ†ç±»"""
    
    def __init__(self):
        # ä¸‹è½½å…è´¹çš„å¼€æºæ¨¡å‹ï¼ˆé¦–æ¬¡ä¸‹è½½çº¦ 100MBï¼‰
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # é¢„å®šä¹‰åˆ†ç±»æ¨¡æ¿
        self.templates = {
            'breaking': ['çªå‘æ–°é—»', 'ç´§æ€¥æ¶ˆæ¯', 'å¿«è®¯', 'é‡å¤§äº‹ä»¶'],
            'hot': ['çƒ­é—¨è¯é¢˜', ' trending', 'çƒ­æœ', 'å¤§å®¶éƒ½åœ¨è®¨è®º'],
            'new': ['æ–°äº§å“å‘å¸ƒ', 'æ–°å“ä¸Šå¸‚', 'æ–°ç‰ˆæœ¬', 'æ­£å¼ä¸Šçº¿'],
            'deep': ['æ·±åº¦ç ”ç©¶æŠ¥å‘Š', 'æŠ€æœ¯åˆ†æ', 'åŸç†è§£æ', 'ç»¼è¿°']
        }
        
        # é¢„è®¡ç®—æ¨¡æ¿ embedding
        self.template_embeddings = {}
        for cat, texts in self.templates.items():
            embeddings = self.model.encode(texts)
            self.template_embeddings[cat] = np.mean(embeddings, axis=0)
    
    def classify(self, title: str, content: str) -> str:
        """åŸºäºç›¸ä¼¼åº¦åˆ†ç±»"""
        text = title + ' ' + content[:500]
        text_embedding = self.model.encode([text])
        
        # è®¡ç®—ä¸æ¯ä¸ªåˆ†ç±»çš„ç›¸ä¼¼åº¦
        similarities = {}
        for cat, template_emb in self.template_embeddings.items():
            sim = cosine_similarity(text_embedding, [template_emb])[0][0]
            similarities[cat] = sim
        
        return max(similarities, key=similarities.get)
```

---

## 4. å®Œæ•´å¤„ç†æµç¨‹

```python
# ingestor/article_processor.py

from content_fetcher import FreeContentFetcher
from summarizer import TextRankSummarizer
from classifier import RuleClassifier

class FreeArticleProcessor:
    """å…è´¹æ–‡ç« å¤„ç†å™¨"""
    
    def __init__(self):
        self.fetcher = FreeContentFetcher()
        self.summarizer = TextRankSummarizer()
        self.classifier = RuleClassifier()
    
    async def process(self, url: str, title: str, rss_description: str = '') -> dict:
        """
        å¤„ç†å•ç¯‡æ–‡ç« 
        
        Returns:
            {
                'title': str,
                'url': str,
                'content': str,
                'summary': str,
                'category': str,
                'tags': list,
                'source': str
            }
        """
        # 1. æŠ“å–å®Œæ•´å†…å®¹
        fetched = self.fetcher.fetch_content(url)
        
        if fetched and fetched['content']:
            content = fetched['content']
            # ä½¿ç”¨æŠ“å–åˆ°çš„æ ‡é¢˜ï¼ˆé€šå¸¸æ›´å®Œæ•´ï¼‰
            final_title = fetched['title'] or title
        else:
            # æŠ“å–å¤±è´¥ï¼Œä½¿ç”¨ RSS çš„ description
            content = rss_description
            final_title = title
        
        # 2. ç”Ÿæˆæ‘˜è¦
        if len(content) > 300:
            summary = self.summarizer.summarize(content, num_sentences=2)
        else:
            summary = content[:200]
        
        # 3. åˆ†ç±»
        classification = self.classifier.classify(final_title, content)
        
        return {
            'title': final_title,
            'url': url,
            'content': content[:2000],  # é™åˆ¶é•¿åº¦
            'summary': summary,
            'category': classification['category'],
            'tags': classification['tags'],
            'source': self._detect_source(url)
        }
    
    def _detect_source(self, url: str) -> str:
        """ä» URL æ£€æµ‹æ¥æº"""
        domains = {
            '36kr.com': '36æ°ª',
            'arxiv.org': 'ArXiv',
            'news.ycombinator.com': 'Hacker News',
            'techcrunch.com': 'TechCrunch',
            'v2ex.com': 'V2EX',
            'mit.edu': 'MIT Technology Review',
            'venturebeat.com': 'VentureBeat',
            'jiqizhixin.com': 'æœºå™¨ä¹‹å¿ƒ',
            'taime.com': 'é’›åª’ä½“',
            'leiphone.com': 'é›·å³°ç½‘'
        }
        
        for domain, name in domains.items():
            if domain in url:
                return name
        
        return 'å…¶ä»–'
```

---

## 5. æˆæœ¬å¯¹æ¯”

| æ–¹æ¡ˆ | æœˆæˆæœ¬ | å‡†ç¡®ç‡ | é€Ÿåº¦ | æ¨èåœºæ™¯ |
|------|--------|--------|------|----------|
| **æœ¬æ–¹æ¡ˆ** | **0å…ƒ** | ä¸­ | å¿« | ä¸ªäºº/å°é¡¹ç›® |
| OpenAI API | ~$20-50 | é«˜ | å¿« | å•†ä¸šé¡¹ç›® |
| æ™ºè°± API | ~ï¿¥50-200 | é«˜ | å¿« | ä¸­æ–‡å†…å®¹ |

---

## 6. å®æ–½æ­¥éª¤

### ä»Šå¤©å°±èƒ½å®Œæˆï¼ˆ2å°æ—¶ï¼‰

1. **å®‰è£…ä¾èµ–**
   ```bash
   pip install newspaper3k trafilatura jieba
   python -m nltk.downloader punkt
   ```

2. **å¤åˆ¶ä»£ç **
   - åˆ›å»º `ingestor/content_fetcher.py`
   - åˆ›å»º `ingestor/summarizer.py`
   - åˆ›å»º `ingestor/classifier.py`
   - åˆ›å»º `ingestor/article_processor.py`

3. **æµ‹è¯•è¿è¡Œ**
   ```python
   processor = FreeArticleProcessor()
   result = processor.process(
       url='https://36kr.com/p/123456.html',
       title='åŸæ ‡é¢˜',
       rss_description='RSSæè¿°'
   )
   print(result)
   ```

### æœ¬å‘¨ä¼˜åŒ–

1. **æ·»åŠ æ›´å¤šæ•°æ®æºè§„åˆ™**
2. **ä¼˜åŒ–åˆ†ç±»å…³é”®è¯**
3. **å¤„ç†ç‰¹æ®Šæƒ…å†µï¼ˆç™»å½•å¢™ã€åçˆ¬ï¼‰**
4. **æ‰¹é‡å¤„ç†é˜Ÿåˆ—**

---

## 7. æ³¨æ„äº‹é¡¹

### çˆ¬è™«ç¤¼ä»ª
- âš ï¸ æ·»åŠ å»¶è¿Ÿï¼ˆ1-2ç§’ï¼‰
- âš ï¸ éµå®ˆ robots.txt
- âš ï¸ ä¸è¦å¹¶å‘å¤ªé«˜
- âš ï¸ å¤±è´¥æ—¶ä¼˜é›…é™çº§ï¼ˆç”¨ RSS å†…å®¹ï¼‰

### å‡†ç¡®æ€§ä¼˜åŒ–
- ğŸ“Š æ ¹æ®å®é™…æ•°æ®è°ƒæ•´å…³é”®è¯
- ğŸ“Š æ”¶é›†ç”¨æˆ·åé¦ˆæ”¹è¿›åˆ†ç±»
- ğŸ“Š å®šæœŸæ›´æ–°è§„åˆ™

---

## ä¸‹ä¸€æ­¥

æ‚¨å¸Œæœ›æˆ‘ï¼š
1. **ç«‹å³å¤åˆ¶ä»£ç åˆ°é¡¹ç›®**ï¼ˆä»Šå¤©å°±èƒ½ç”¨ï¼‰
2. **å…ˆåšç®€å•çš„ rule-based åˆ†ç±»**ï¼ˆ30åˆ†é’Ÿï¼‰
3. **ç­‰æ˜å¤©å†å®Œæ•´å®æ–½**

**æ¨è**: å…ˆåšç®€å•çš„ rule-based åˆ†ç±»ï¼ˆæ–¹æ¡ˆAï¼‰ï¼Œè®©æ–‡ç« æœ‰åˆ†ç±»æ ‡ç­¾ï¼Œå†…å®¹å¯ä»¥å…ˆç”¨ RSS çš„ descriptionï¼Œåç»­å†æŠ“å–å®Œæ•´å†…å®¹ã€‚

è¯·å‘Šè¯‰æˆ‘æ‚¨çš„é€‰æ‹©ï¼
