# å…è´¹å¼€æºå†…å®¹å¤„ç†æ–¹æ¡ˆï¼ˆä¼˜åŒ–ç‰ˆï¼‰

## å‘ç°çš„æ–°å·¥å…·

ç»è¿‡æ·±å…¥è°ƒç ”ï¼Œå‘ç°äº†å¤šä¸ªæ›´å¥½çš„å¼€æºæ›¿ä»£æ–¹æ¡ˆã€‚

---

## 1. å†…å®¹æå–ï¼ˆæ›´å¥½çš„æ–¹æ¡ˆï¼‰

### æ–¹æ¡ˆ A: newspaper4k â­ æ¨è

**newspaper3k çš„æ´»è·ƒ forkï¼ŒæŒç»­ç»´æŠ¤**

```python
pip install newspaper4k
```

```python
from newspaper import Article

url = 'https://36kr.com/p/123456.html'
article = Article(url, language='zh')
article.download()
article.parse()

print(article.title)      # æ ‡é¢˜
print(article.text)       # æ­£æ–‡ï¼ˆå®Œæ•´ï¼‰
print(article.authors)    # ä½œè€…
print(article.publish_date)  # å‘å¸ƒæ—¶é—´
print(article.top_image)  # å°é¢å›¾
print(article.keywords)   # å…³é”®è¯ï¼ˆNLPæå–ï¼‰
print(article.summary)    # æ‘˜è¦ï¼ˆè‡ªåŠ¨æå–ï¼‰
```

**ä¼˜ç‚¹**ï¼š
- âœ… newspaper3k çš„ç»§ä»»è€…ï¼ŒæŒç»­æ›´æ–°
- âœ… å†…ç½® NLPï¼Œè‡ªåŠ¨æå–å…³é”®è¯å’Œæ‘˜è¦
- âœ… æ”¯æŒä¸­æ–‡
- âœ… è‡ªåŠ¨è¯†åˆ«å‘å¸ƒæ—¶é—´ã€ä½œè€…
- âœ… èƒ½æå–é¡¶éƒ¨å›¾ç‰‡

### æ–¹æ¡ˆ B: news-fetch â­ æ›´ç®€å•

**å¼€ç®±å³ç”¨ï¼Œå†…ç½® NLP**

```python
pip install news-fetch
```

```python
from newsfetch import NewsFetch

news = NewsFetch('https://36kr.com/p/123456.html')
print(news.title)
print(news.content)
print(news.summary)  # è‡ªåŠ¨ç”Ÿæˆæ‘˜è¦
print(news.keywords)  # è‡ªåŠ¨æå–å…³é”®è¯
print(news.authors)
```

**ä¼˜ç‚¹**ï¼š
- âœ… ä¸€è¡Œä»£ç æå®š
- âœ… è‡ªå¸¦æ‘˜è¦å’Œå…³é”®è¯
- âœ… åŸºäº newspaper3k + NLP å¢å¼º

### æ–¹æ¡ˆ C: fundus â­ æ›´é€‚åˆä¸­æ–‡

**ä¸“ä¸ºæ–°é—»åª’ä½“è®¾è®¡**

```python
pip install fundus
```

```python
from fundus import PublisherCollection, Crawler

# æ”¯æŒ 100+ æ–°é—»æº
crawler = Crawler(PublisherCollection.cn)  # ä¸­æ–‡æ–°é—»æº

for article in crawler.crawl(
    max_articles=10,
    only_complete=True  # åªè¿”å›å®Œæ•´çš„æ–‡ç« 
):
    print(article.title)
    print(article.body)  # æ­£æ–‡
    print(article.summary)
    print(article.authors)
    print(article.publishing_date)
```

**æ”¯æŒçš„ ä¸­æ–‡æ–°é—»æº**ï¼š
- ä¸­å›½æ—¥æŠ¥ (China Daily)
- ç¯çƒæ—¶æŠ¥ (Global Times)
- äººæ°‘æ—¥æŠ¥ (People's Daily)
- æ–°åç½‘ (Xinhua)
- ...ç­‰ 100+ å›½é™…æ–°é—»æº

---

## 2. æ–‡æœ¬æ‘˜è¦ï¼ˆæ›´å¥½çš„æ–¹æ¡ˆï¼‰

### æ–¹æ¡ˆ A: HanLP â­ ä¸­æ–‡æ•ˆæœæœ€å¥½

**å¼€æºä¸­æ–‡ NLP ç¥å™¨ï¼Œæ”¯æŒæŠ½å–å¼æ‘˜è¦**

```python
pip install hanlp
```

```python
import hanlp

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)

# æŠ½å–å¼æ‘˜è¦
text = """è¿™é‡Œæ˜¯å¾ˆé•¿çš„æ–°é—»å†…å®¹..."""
summary = HanLP(text, tasks='extractive_summarization')

print(summary['extractive_summarization'])  # å…³é”®å¥å­
```

**ä¼˜ç‚¹**ï¼š
- âœ… ä¸­æ–‡æ•ˆæœé¡¶çº§
- âœ… å®Œå…¨å…è´¹ï¼Œæœ¬åœ°è¿è¡Œ
- âœ… æ”¯æŒå¤šç§ NLP ä»»åŠ¡

### æ–¹æ¡ˆ B: UniLM_summarization â­ ç”Ÿæˆå¼æ‘˜è¦

**åŸºäºä¸­æ–‡ BERT çš„ç”Ÿæˆå¼æ‘˜è¦**

```bash
git clone https://github.com/chenlian98/UniLM_summarization.git
pip install -r requirements.txt
```

```python
from summarization import UniLMSummarizer

model = UniLMSummarizer(model_path='unilm-base-chinese')
text = "è¿™é‡Œæ˜¯å¾ˆé•¿çš„æ–°é—»å†…å®¹..."
summary = model.generate(text, max_length=100)
print(summary)
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç”Ÿæˆå¼æ‘˜è¦ï¼Œæ›´è‡ªç„¶
- âœ… åŸºäºä¸­æ–‡ BERTï¼Œç†è§£ä¸­æ–‡æ›´å¥½
- âœ… å®Œå…¨å¼€æºå…è´¹

### æ–¹æ¡ˆ C: æœ¬åœ°å°æ¨¡å‹ï¼ˆCPUå¯è·‘ï¼‰

**ChatGLM-6B-Int4ï¼ˆé‡åŒ–ç‰ˆï¼‰**

```bash
# ä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 6GBï¼‰
git clone https://huggingface.co/THUDM/chatglm-6b-int4

# æˆ–ç”¨æ›´å°æ¨¡å‹
git clone https://huggingface.co/THUDM/chatglm2-6b-int4
```

```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True).half().cuda()

# ç”Ÿæˆæ‘˜è¦
prompt = "è¯·ç”¨50å­—æ¦‚æ‹¬ä»¥ä¸‹å†…å®¹ï¼š\n" + article_text[:2000]
response, history = model.chat(tokenizer, prompt, history=[])
print(response)
```

**ç¡¬ä»¶è¦æ±‚**ï¼š
- Int4 é‡åŒ–ç‰ˆï¼š6GB æ˜¾å­˜ï¼ˆæˆ– CPU + 16GB å†…å­˜ï¼‰
- Int8 é‡åŒ–ç‰ˆï¼š8GB æ˜¾å­˜

---

## 3. æ™ºèƒ½åˆ†ç±»ï¼ˆæ›´å¥½çš„æ–¹æ¡ˆï¼‰

### æ–¹æ¡ˆ A: bert-base-chinese + å¾®è°ƒï¼ˆæ¨èï¼‰

**ä½¿ç”¨å¼€æº BERT åšæ–‡æœ¬åˆ†ç±»**

```python
# ä½¿ç”¨ transformers åº“çš„é›¶æ ·æœ¬åˆ†ç±»
from transformers import pipeline

# é›¶æ ·æœ¬åˆ†ç±»ï¼ˆæ— éœ€è®­ç»ƒï¼‰
classifier = pipeline(
    "zero-shot-classification",
    model="MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",  # å¤šè¯­è¨€æ¨¡å‹
    device=-1  # CPU
)

text = "OpenAIå‘å¸ƒGPT-5æ–°æ¨¡å‹..."
labels = ["çƒ­é—¨", "æ·±åº¦", "æ–°å“", "çªå‘"]

result = classifier(text, labels)
print(result['labels'][0])  # æœ€å¯èƒ½çš„åˆ†ç±»
print(result['scores'][0])  # ç½®ä¿¡åº¦
```

**ä¼˜ç‚¹**ï¼š
- âœ… é›¶æ ·æœ¬ï¼Œæ— éœ€è®­ç»ƒæ•°æ®
- âœ… å¤šè¯­è¨€æ”¯æŒ
- âœ… å®Œå…¨å…è´¹

### æ–¹æ¡ˆ B: ä½¿ç”¨ç°æœ‰çš„å¼€æºåˆ†ç±»æ¨¡å‹

```python
# ä½¿ç”¨é˜¿é‡Œå¼€æºçš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# ä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 400MBï¼‰
model_name = "uer/roberta-base-finetuned-jd-binary-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# åˆ†ç±»
text = "OpenAIå‘å¸ƒæ–°æ¨¡å‹..."
inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
outputs = model(**inputs)
predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```

### æ–¹æ¡ˆ C: ç®€å•ä½†æœ‰æ•ˆçš„å…³é”®è¯åŒ¹é…

```python
# æ›´å¿«æ›´ç®€å•ï¼Œä¸éœ€è¦ä¸‹è½½æ¨¡å‹
import jieba
import jieba.analyse

def classify_by_keywords(text):
    """åŸºäº TF-IDF æå–å…³é”®è¯ï¼ŒåŒ¹é…åˆ†ç±»"""
    
    # æå–å…³é”®è¯
    keywords = jieba.analyse.extract_tags(text, topK=10, withWeight=True)
    
    # åˆ†ç±»è¯å…¸
    categories = {
        'breaking': {'çªå‘': 10, 'ç´§æ€¥': 10, 'é‡ç£…': 8, 'åˆšåˆš': 8},
        'hot': {'çƒ­é—¨': 8, 'çƒ­è®®': 8, 'ç«äº†': 7, 'çˆ†æ¬¾': 7},
        'new': {'å‘å¸ƒ': 6, 'æ–°å“': 6, 'æ¨å‡º': 6, 'ä¸Šçº¿': 6},
        'deep': {'ç ”ç©¶': 5, 'æ·±åº¦': 5, 'åˆ†æ': 5, 'è§£è¯»': 5}
    }
    
    # è®¡ç®—åˆ†ç±»å¾—åˆ†
    scores = {}
    for cat, words in categories.items():
        score = 0
        for word, weight in keywords:
            if word in words:
                score += words[word] * weight
        scores[cat] = score
    
    return max(scores, key=scores.get) if max(scores.values()) > 0 else 'new'
```

---

## 4. æ¨èæœ€ä½³ç»„åˆ

### ğŸ† æ¨èæ–¹æ¡ˆ Aï¼šå…¨éƒ¨å…è´¹ï¼Œæ•ˆæœæœ€å¥½

| åŠŸèƒ½ | å·¥å…· | ä¼˜ç‚¹ |
|------|------|------|
| **å†…å®¹æå–** | `newspaper4k` | æŒç»­ç»´æŠ¤ï¼Œè‡ªå¸¦ NLP |
| **æ‘˜è¦ç”Ÿæˆ** | `HanLP` | ä¸­æ–‡æ•ˆæœæœ€ä½³ |
| **æ™ºèƒ½åˆ†ç±»** | `bert-base-chinese` | å‡†ç¡®ç‡é«˜ |

**æˆæœ¬ï¼š0 å…ƒ**

### ğŸš€ æ¨èæ–¹æ¡ˆ Bï¼šæœ€ç®€å•å¿«é€Ÿ

| åŠŸèƒ½ | å·¥å…· | ä¼˜ç‚¹ |
|------|------|------|
| **å†…å®¹æå–** | `news-fetch` | ä¸€è¡Œä»£ç  |
| **æ‘˜è¦ç”Ÿæˆ** | å†…ç½®æ‘˜è¦ | news-fetch è‡ªå¸¦ |
| **æ™ºèƒ½åˆ†ç±»** | è§„åˆ™åŒ¹é… | æ— éœ€æ¨¡å‹ |

**æˆæœ¬ï¼š0 å…ƒ**

### ğŸ¯ æ¨èæ–¹æ¡ˆ Cï¼šå¹³è¡¡æ•ˆæœä¸èµ„æº

| åŠŸèƒ½ | å·¥å…· | ä¼˜ç‚¹ |
|------|------|------|
| **å†…å®¹æå–** | `newspaper4k` | åŠŸèƒ½å…¨é¢ |
| **æ‘˜è¦ç”Ÿæˆ** | `TextRank` (gensim) | å¿«é€Ÿï¼Œæ— éœ€æ¨¡å‹ |
| **æ™ºèƒ½åˆ†ç±»** | å…³é”®è¯ + TF-IDF | ç®€å•æœ‰æ•ˆ |

**æˆæœ¬ï¼š0 å…ƒ**

---

## 5. å®Œæ•´ä»£ç ç¤ºä¾‹ï¼ˆæ¨èæ–¹æ¡ˆ Cï¼‰

```python
# ingestor/enhanced_processor.py

import requests
from newspaper import Article
from gensim.summarization import summarize as gensim_summarize
import jieba
import jieba.analyse
from typing import Dict, List

class EnhancedArticleProcessor:
    """ä¼˜åŒ–çš„æ–‡ç« å¤„ç†å™¨ - å®Œå…¨å…è´¹"""
    
    def __init__(self):
        # åŠ è½½ jieba è¯å…¸ï¼ˆå¯é€‰ï¼Œæå‡å‡†ç¡®ç‡ï¼‰
        # jieba.load_userdict('custom_dict.txt')
        pass
    
    def fetch_content(self, url: str) -> Dict:
        """æŠ“å–æ–‡ç« å†…å®¹"""
        try:
            article = Article(url, language='zh')
            article.download()
            article.parse()
            
            # ä½¿ç”¨å†…ç½® NLP
            article.nlp()
            
            return {
                'title': article.title,
                'content': article.text,
                'authors': article.authors,
                'publish_date': article.publish_date,
                'keywords': article.keywords,  # NLPæå–çš„å…³é”®è¯
                'top_image': article.top_image,
                'success': True
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def generate_summary(self, text: str, word_count: int = 100) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        if len(text) < 200:
            return text[:200]
        
        try:
            # ä½¿ç”¨ gensim çš„ TextRankï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
            summary = gensim_summarize(text, word_count=word_count)
            return summary if summary else text[:200]
        except:
            # é™çº§ï¼šå–å‰ N ä¸ªå¥å­
            sentences = text.split('ã€‚')[:3]
            return 'ã€‚'.join(sentences) + 'ã€‚'
    
    def classify(self, title: str, content: str) -> Dict:
        """æ™ºèƒ½åˆ†ç±»"""
        text = title + ' ' + content[:1000]
        
        # æå–å…³é”®è¯
        keywords = jieba.analyse.extract_tags(text, topK=10, withWeight=True)
        
        # åˆ†ç±»è§„åˆ™ï¼ˆå¸¦æƒé‡ï¼‰
        rules = {
            'breaking': {
                'keywords': ['çªå‘', 'ç´§æ€¥', 'åˆšåˆš', 'é‡ç£…', 'éœ‡æƒŠ'],
                'weight': 10
            },
            'hot': {
                'keywords': ['çƒ­é—¨', 'çƒ­è®®', 'ç«äº†', 'çˆ†ç«', 'çƒ­æœ'],
                'weight': 8
            },
            'new': {
                'keywords': ['å‘å¸ƒ', 'æ–°å“', 'æ¨å‡º', 'ä¸Šçº¿', 'é—®ä¸–'],
                'weight': 6
            },
            'deep': {
                'keywords': ['ç ”ç©¶', 'æ·±åº¦', 'åˆ†æ', 'è§£è¯»', 'ç»¼è¿°'],
                'weight': 5
            }
        }
        
        # è®¡ç®—å¾—åˆ†
        scores = {}
        for cat, config in rules.items():
            score = 0
            for kw, weight in keywords:
                if kw in config['keywords']:
                    score += config['weight'] * weight
            scores[cat] = score
        
        # é€‰æ‹©æœ€ä½³åˆ†ç±»
        best_category = max(scores, key=scores.get) if max(scores.values()) > 0 else 'new'
        
        # æå–æ ‡ç­¾
        tag_keywords = ['AIç»˜ç”»', 'LLM', 'ChatGPT', 'Midjourney', 'äº§å“å‘å¸ƒ', 
                       'ç ”ç©¶', 'å·¥å…·', 'å®‰å…¨', 'å•†ä¸š', 'ç¡¬ä»¶']
        tags = [kw for kw, _ in keywords if any(t in kw for t in tag_keywords)]
        
        return {
            'category': best_category,
            'tags': tags[:3],
            'keywords': [kw for kw, _ in keywords[:5]]
        }
    
    def process(self, url: str, rss_description: str = '') -> Dict:
        """å¤„ç†å•ç¯‡æ–‡ç« """
        # 1. æŠ“å–å†…å®¹
        fetched = self.fetch_content(url)
        
        if not fetched['success']:
            # æŠ“å–å¤±è´¥ï¼Œä½¿ç”¨ RSS å†…å®¹
            content = rss_description
            title = ''
        else:
            content = fetched['content']
            title = fetched['title']
        
        if not content:
            return None
        
        # 2. ç”Ÿæˆæ‘˜è¦
        summary = self.generate_summary(content, word_count=100)
        
        # 3. åˆ†ç±»
        classification = self.classify(title, content)
        
        return {
            'title': title,
            'url': url,
            'content': content[:2000],  # é™åˆ¶é•¿åº¦
            'summary': summary,
            'category': classification['category'],
            'tags': classification['tags'],
            'keywords': classification['keywords'],
            'authors': fetched.get('authors', []),
            'publish_date': fetched.get('publish_date', ''),
            'top_image': fetched.get('top_image', ''),
            'source': self._detect_source(url)
        }
    
    def _detect_source(self, url: str) -> str:
        """æ£€æµ‹æ¥æº"""
        domains = {
            '36kr.com': '36æ°ª',
            'arxiv.org': 'ArXiv',
            'news.ycombinator.com': 'Hacker News',
            'techcrunch.com': 'TechCrunch',
            'v2ex.com': 'V2EX',
            'mit.edu': 'MIT Technology Review',
            'venturebeat.com': 'VentureBeat',
            'jiqizhixin.com': 'æœºå™¨ä¹‹å¿ƒ',
            'taime.com': 'é’›åª’ä½“',
            'leiphone.com': 'é›·å³°ç½‘'
        }
        
        for domain, name in domains.items():
            if domain in url:
                return name
        return 'å…¶ä»–'
```

### å®‰è£…ä¾èµ–

```bash
pip install newspaper4k gensim jieba
python -m nltk.downloader punkt  # newspaper4k éœ€è¦
```

### ä½¿ç”¨ç¤ºä¾‹

```python
processor = EnhancedArticleProcessor()
result = processor.process(
    url='https://36kr.com/p/123456.html',
    rss_description='è¿™æ˜¯ RSS çš„æè¿°'
)

print(f"æ ‡é¢˜: {result['title']}")
print(f"åˆ†ç±»: {result['category']}")
print(f"æ ‡ç­¾: {result['tags']}")
print(f"æ‘˜è¦: {result['summary'][:100]}...")
```

---

## 6. æ€§èƒ½å¯¹æ¯”

| æ–¹æ¡ˆ | å‡†ç¡®ç‡ | é€Ÿåº¦ | èµ„æºå ç”¨ | æ˜“ç”¨æ€§ |
|------|--------|------|----------|--------|
| newspaper4k + HanLP | â­â­â­â­â­ | â­â­â­ | â­â­ | â­â­â­ |
| news-fetch | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| æœ¬æ–¹æ¡ˆ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |

---

## 7. ä¸‹ä¸€æ­¥å»ºè®®

### ä»Šå¤©å°±èƒ½åšï¼ˆ1å°æ—¶ï¼‰

1. å®‰è£…ä¾èµ–å¹¶æµ‹è¯•å•ä¸ª URL
2. è°ƒæ•´åˆ†ç±»å…³é”®è¯è§„åˆ™
3. é›†æˆåˆ°ç°æœ‰çš„ ingestor

### æœ¬å‘¨ä¼˜åŒ–

1. æ‰¹é‡å¤„ç†é˜Ÿåˆ—ï¼ˆé˜²æ­¢è¢«å°ï¼‰
2. å¤±è´¥é‡è¯•æœºåˆ¶
3. æ ¹æ®å®é™…æ•ˆæœè°ƒæ•´åˆ†ç±»è§„åˆ™
4. æ·»åŠ æ›´å¤šæ•°æ®æº

### åæœŸå‡çº§ï¼ˆå¯é€‰ï¼‰

1. é›†æˆæœ¬åœ° ChatGLMï¼ˆå¦‚æœæœ‰ GPUï¼‰
2. ä½¿ç”¨ HanLP æ›¿ä»£ gensim æ‘˜è¦
3. è®­ç»ƒè‡ªå·±çš„åˆ†ç±»æ¨¡å‹

---

**æ¨èï¼šå…ˆä½¿ç”¨ã€Œæ–¹æ¡ˆ Cã€å¿«é€Ÿä¸Šçº¿ï¼ŒéªŒè¯æ•ˆæœåå†å†³å®šæ˜¯å¦å‡çº§ï¼**
